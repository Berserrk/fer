{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/firaterman/Documents/fer/berserk3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/Users/firaterman/Documents/fer/berserk3/lib/python3.12/site-packages/gliner/model.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_file, map_location=torch.device(map_location))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wladimir Putin => Person\n",
      "Weihnachten => Date\n",
      "Joe Biden => Person\n",
      "Dr. Smith => Person\n",
      "John Smith => Person\n",
      "Hillary Clinton => Person\n",
      "Wladimir Putin => Person\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "# Initialize GLiNER with the base model\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\")\n",
    "# Sample text for entity prediction\n",
    "text = \"\"\"\n",
    "Der russische Präsident Wladimir Putin hat heute keine neuen Richtlinien verkündet. Die letzten bekannten Äußerungen Putins stammen von einer Fragerunde vor Weihnachten, bei der er über verschiedene Themen wie Syrien, den Krieg gegen die Ukraine und die niedrige Geburtenrate in Russland sprach4\n",
    ". Bezüglich Joe Biden, Dr. Smith, John Smith und Hillary Clinton liegen keine aktuellen Informationen aus den gegebenen Suchergebnissen vor. Die Suchergebnisse konzentrieren sich hauptsächlich auf Wladimir Putin und enthalten keine Informationen über aktuelle Aktivitäten dieser amerikanischen Politiker oder Personen.\n",
    "\"\"\"\n",
    "\n",
    "# Labels for entity prediction\n",
    "# Most GLiNER models should work best when entity types are in lower case or title case\n",
    "labels = [\"Person\", \"Award\", \"Date\", \"Competitions\", \"Teams\"]\n",
    "\n",
    "# Perform entity prediction\n",
    "entities = model.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deberta SSL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie was fantastic! I absolutely loved it.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([[0.4585, 0.5415]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Text: This was the worst experience I've ever had.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([[0.4596, 0.5404]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Batch Processing Results:\n",
      "Text: This movie was fantastic! I absolutely loved it.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([0.4585, 0.5415], grad_fn=<UnbindBackward0>)\n",
      "\n",
      "Text: This was the worst experience I've ever had.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([0.4596, 0.5404], grad_fn=<UnbindBackward0>)\n",
      "\n",
      "Text: The product is okay, but the delivery was delayed.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([0.4566, 0.5434], grad_fn=<UnbindBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    num_labels=2,  # Binary classification\n",
    "    ignore_mismatched_sizes=True  # Handle dimension mismatches gracefully\n",
    ")\n",
    "\n",
    "# Define a function to classify a single text\n",
    "def classify_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted class (0 or 1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    # Map class to sentiment\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    \n",
    "    return sentiment, probabilities\n",
    "\n",
    "# Test the classifier with individual texts\n",
    "text_1 = \"This movie was fantastic! I absolutely loved it.\"\n",
    "text_2 = \"This was the worst experience I've ever had.\"\n",
    "\n",
    "# Get predictions for individual texts\n",
    "sentiment_1, probabilities_1 = classify_text(text_1)\n",
    "sentiment_2, probabilities_2 = classify_text(text_2)\n",
    "\n",
    "# Print results for individual texts\n",
    "print(f\"Text: {text_1}\\nSentiment: {sentiment_1}\\nProbabilities: {probabilities_1}\")\n",
    "print()\n",
    "print(f\"Text: {text_2}\\nSentiment: {sentiment_2}\\nProbabilities: {probabilities_2}\")\n",
    "\n",
    "# Batch processing for multiple texts\n",
    "texts = [\n",
    "    \"This movie was fantastic! I absolutely loved it.\",\n",
    "    \"This was the worst experience I've ever had.\",\n",
    "    \"The product is okay, but the delivery was delayed.\"\n",
    "]\n",
    "\n",
    "# Tokenize the texts in a batch\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get predictions for the batch\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "predicted_classes = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "# Map class indices to sentiment\n",
    "sentiments = [\"Positive\" if cls == 1 else \"Negative\" for cls in predicted_classes]\n",
    "\n",
    "# Print results for batch processing\n",
    "print(\"\\nBatch Processing Results:\")\n",
    "for text, sentiment, probs in zip(texts, sentiments, probabilities):\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\\nProbabilities: {probs}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deberta offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(f\"Cleared Hugging Face cache: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie was fantastic! I absolutely loved it.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([[0.4917, 0.5083]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Text: This was the worst experience I've ever had.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([[0.4917, 0.5083]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Batch Processing Results:\n",
      "Text: This movie was fantastic! I absolutely loved it.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([0.4917, 0.5083], grad_fn=<UnbindBackward0>)\n",
      "\n",
      "Text: This was the worst experience I've ever had.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([0.4917, 0.5083], grad_fn=<UnbindBackward0>)\n",
      "\n",
      "Text: The product is okay, but the delivery was delayed.\n",
      "Sentiment: Positive\n",
      "Probabilities: tensor([0.4903, 0.5097], grad_fn=<UnbindBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load tokenizer and model from local files\n",
    "local_model_path = \"/Users/firaterman/Documents/fer/deberta-v3-small\"  # Path to your downloaded model and tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    local_model_path,\n",
    "    num_labels=2,  # Binary classification\n",
    "    ignore_mismatched_sizes=True  # Handle mismatches if any\n",
    ")\n",
    "\n",
    "# Define a function to classify a single text\n",
    "def classify_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted class (0 or 1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    # Map class to sentiment\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    \n",
    "    return sentiment, probabilities\n",
    "\n",
    "# Test the classifier with individual texts\n",
    "text_1 = \"This movie was fantastic! I absolutely loved it.\"\n",
    "text_2 = \"This was the worst experience I've ever had.\"\n",
    "\n",
    "# Get predictions for individual texts\n",
    "sentiment_1, probabilities_1 = classify_text(text_1)\n",
    "sentiment_2, probabilities_2 = classify_text(text_2)\n",
    "\n",
    "# Print results for individual texts\n",
    "print(f\"Text: {text_1}\\nSentiment: {sentiment_1}\\nProbabilities: {probabilities_1}\")\n",
    "print()\n",
    "print(f\"Text: {text_2}\\nSentiment: {sentiment_2}\\nProbabilities: {probabilities_2}\")\n",
    "\n",
    "# Batch processing for multiple texts\n",
    "texts = [\n",
    "    \"This movie was fantastic! I absolutely loved it.\",\n",
    "    \"This was the worst experience I've ever had.\",\n",
    "    \"The product is okay, but the delivery was delayed.\"\n",
    "]\n",
    "\n",
    "# Tokenize the texts in a batch\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get predictions for the batch\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "predicted_classes = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "# Map class indices to sentiment\n",
    "sentiments = [\"Positive\" if cls == 1 else \"Negative\" for cls in predicted_classes]\n",
    "\n",
    "# Print results for batch processing\n",
    "print(\"\\nBatch Processing Results:\")\n",
    "for text, sentiment, probs in zip(texts, sentiments, probabilities):\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\\nProbabilities: {probs}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gliner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file in ['gitattributes', 'gliner_config.json', 'README.md', '.gitattributes', 'pytorch_model.bin', '.git']\n"
     ]
    }
   ],
   "source": [
    "local_gliner_path = \"/Users/firaterman/Documents/fer/gliner_small-v2.1\" \n",
    "print(\"file in\", os.listdir(local_gliner_path))\n",
    "\n",
    "local_gliner_path = \"/Users/firaterman/Documents/fer/gliner_small-v2.1\" \n",
    "print(\"file in\", os.listdir(local_gliner_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_94559/3279719854.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: City, Probabilities: tensor([[0.3172, 0.3323, 0.3505]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths to local DeBERTa and Gliner files\n",
    "deberta_path = \"/Users/firaterman/Documents/fer/deberta-v3-small\"\n",
    "gliner_weights_path = \"/Users/firaterman/Documents/fer/gliner_small-v2.1/pytorch_model.bin\"\n",
    "\n",
    "# Load tokenizer and configuration offline\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_path)\n",
    "config = AutoConfig.from_pretrained(deberta_path)\n",
    "\n",
    "# Update configuration for Gliner-specific task\n",
    "config.num_labels = 3  # Adjust based on Gliner task requirements\n",
    "\n",
    "# Load Gliner weights into DeBERTa model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=None,  # Not using a pretrained name\n",
    "    config=config,\n",
    "    state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "# Define an inference function\n",
    "def gliner_predict(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Compute probabilities and predicted class\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    # Map predicted class to labels\n",
    "    labels = [\"Location\", \"Person\", \"City\"]  # Update based on Gliner task\n",
    "    prediction = labels[predicted_class]\n",
    "    return prediction, probabilities\n",
    "\n",
    "# Example prediction\n",
    "example_text = \"Jean is present in Paris\"\n",
    "prediction, probabilities = gliner_predict(example_text)\n",
    "print(f\"Prediction: {prediction}, Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_94559/4273152854.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: [CLS], Label: I-PER\n",
      "Entity: ▁John, Label: B-DATE\n",
      "Entity: ▁Doe, Label: I-LOC\n",
      "Entity: ▁visited, Label: B-DATE\n",
      "Entity: ▁New, Label: B-DATE\n",
      "Entity: ▁York, Label: B-DATE\n",
      "Entity: ▁on, Label: B-DATE\n",
      "Entity: ▁January, Label: B-LOC\n",
      "Entity: ▁15, Label: B-DATE\n",
      "Entity: ,, Label: B-LOC\n",
      "Entity: ▁2023, Label: B-LOC\n",
      "Entity: ., Label: B-DATE\n",
      "Entity: [SEP], Label: I-PER\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Paths to local DeBERTa and Gliner weights\n",
    "deberta_path = \"/Users/firaterman/Documents/fer/deberta-v3-small\"\n",
    "gliner_weights_path = \"/Users/firaterman/Documents/fer/gliner_small-v2.1/pytorch_model.bin\"\n",
    "\n",
    "# Load tokenizer and configuration offline\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_path)\n",
    "config = AutoConfig.from_pretrained(deberta_path)\n",
    "\n",
    "# Update configuration for token classification\n",
    "config.num_labels = 7  # Number of NER labels (O, B-PER, I-PER, B-LOC, I-LOC, B-DATE, I-DATE)\n",
    "config.id2label = {0: \"O\", 1: \"B-PER\", 2: \"I-PER\", 3: \"B-LOC\", 4: \"I-LOC\", 5: \"B-DATE\", 6: \"I-DATE\"}\n",
    "config.label2id = {label: id for id, label in config.id2label.items()}\n",
    "\n",
    "# Load Gliner weights into DeBERTa model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=None,\n",
    "    config=config,\n",
    "    state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "# Function for NER predictions\n",
    "def gliner_ner_predict(text):\n",
    "    # Tokenize the input\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    \n",
    "    # Perform inference\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to predicted labels\n",
    "    predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    \n",
    "    # Decode tokens and align predictions\n",
    "    tokens_decoded = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    entities = []\n",
    "    for token, label_id in zip(tokens_decoded, predictions):\n",
    "        label = config.id2label[label_id]\n",
    "        if label != \"O\":  # Skip non-entity tokens\n",
    "            entities.append((token, label))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Example prediction\n",
    "example_text = \"John Doe visited New York on January 15, 2023.\"\n",
    "entities = gliner_ner_predict(example_text)\n",
    "\n",
    "# Print results\n",
    "for entity, label in entities:\n",
    "    print(f\"Entity: {entity}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_94559/2365459257.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Location\n",
      "Probabilities: tensor([[0.3950, 0.2735, 0.3314]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths to local DeBERTa and GLiNER model files\n",
    "deberta_path = \"/Users/firaterman/Documents/fer/deberta-v3-small\"  # Path to DeBERTa files\n",
    "gliner_weights_path = \"/Users/firaterman/Documents/fer/gliner_small-v2.1/pytorch_model.bin\"  # Path to GLiNER weights\n",
    "\n",
    "# Load tokenizer and configuration offline\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_path)\n",
    "config = AutoConfig.from_pretrained(deberta_path)\n",
    "\n",
    "# Update configuration for GLiNER-specific task\n",
    "config.num_labels = 3  # Number of labels for GLiNER task (update based on your labels)\n",
    "\n",
    "# Load GLiNER weights into DeBERTa model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=None,  # No internet usage\n",
    "    config=config,\n",
    "    state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "# Define a prediction function for GLiNER with DeBERTa\n",
    "def gliner_predict(text, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict entities in text using the GLiNER model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text for prediction.\n",
    "        labels (list): List of possible labels.\n",
    "        threshold (float): Minimum probability for classification.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Predicted label and probabilities.\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Compute probabilities for each label\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the most probable class and its label\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    prediction = labels[predicted_class]\n",
    "    \n",
    "    return prediction, probabilities\n",
    "\n",
    "# Define possible labels for the GLiNER task\n",
    "labels = [\"Location\", \"Person\", \"City\"]  # Update based on GLiNER's specific task requirements\n",
    "\n",
    "# Example usage\n",
    "example_text = \"Cristiano Ronaldo was born in Madeira\"\n",
    "prediction, probabilities = gliner_predict(example_text, labels)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_94559/4116677029.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano => Location\n",
      " do => Location\n",
      "s => Location\n",
      "iro => Location\n",
      "born => Location\n",
      " 5 => Location\n",
      " 1985 => Location\n",
      " is => Location\n",
      " a => Location\n",
      " professional => Location\n",
      " footballer => Location\n",
      " plays => Location\n",
      " for => Location\n",
      " Al => Location\n",
      " Nass => Location\n",
      ". => Location\n",
      " => Location\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths to local DeBERTa and GLiNER files\n",
    "deberta_path = \"/Users/firaterman/Documents/fer/deberta-v3-small\"  # Path to DeBERTa files\n",
    "gliner_weights_path = \"/Users/firaterman/Documents/fer/gliner_small-v2.1/pytorch_model.bin\"  # Path to GLiNER weights\n",
    "\n",
    "# Load tokenizer and configuration offline\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_path)\n",
    "config = AutoConfig.from_pretrained(deberta_path)\n",
    "\n",
    "# Update configuration for GLiNER-specific task\n",
    "config.num_labels = 2  # Number of entity labels (update as needed for your task)\n",
    "\n",
    "# Load GLiNER weights into DeBERTa model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=None,\n",
    "    config=config,\n",
    "    state_dict=torch.load(gliner_weights_path, map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "# Define labels for the GLiNER task\n",
    "labels = [\"Person\", \"Location\"]  # Update as per GLiNER's label set\n",
    "\n",
    "\n",
    "def predict_entities(text, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict named entities in the text using GLiNER.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to process.\n",
    "        labels (list): List of entity labels (first label is usually \"O\" for non-entities).\n",
    "        threshold (float): Minimum probability threshold for classification.\n",
    "\n",
    "    Returns:\n",
    "        list: List of entities with their text and labels.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0].tolist()\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Extract predictions for each token\n",
    "    predictions = torch.argmax(probabilities, dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Map predictions back to original text\n",
    "    entities = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        if pred != 0:  # Ignore \"O\" (non-entity tokens)\n",
    "            entity_label = labels[pred]\n",
    "            start, end = offset_mapping[i]\n",
    "            entity_text = text[start:end]\n",
    "            entities.append({\"text\": entity_text, \"label\": entity_label})\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Example text\n",
    "example_text = \"\"\"\n",
    "Cristiano Ronaldo dos Santos Aveiro (born 5 February 1985) is a Portuguese professional footballer who plays for Al Nassr.\n",
    "\"\"\"\n",
    "\n",
    "# Perform entity prediction\n",
    "entities = predict_entities(example_text, labels)\n",
    "\n",
    "# Display results\n",
    "for entity in entities:\n",
    "    print(f\"{entity['text']} => {entity['label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_94559/1704549477.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama => Person\n",
      "USA => Country\n",
      "France => Country\n",
      "Trump => Person\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from gliner import GLiNER\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def load_local_gliner(gliner_path, deberta_path):\n",
    "    gliner_path = Path(gliner_path)\n",
    "    deberta_path = Path(deberta_path)\n",
    "\n",
    "    # Load GLiNER config\n",
    "    with open(gliner_path / \"gliner_config.json\", 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    # Convert dictionary to namespace\n",
    "    config = SimpleNamespace(**config_dict)\n",
    "    \n",
    "    # Point to local DeBERTa path\n",
    "    config.model_name = str(deberta_path)\n",
    "\n",
    "    # Initialize GLiNER model with config namespace\n",
    "    model = GLiNER(config)\n",
    "    \n",
    "    # Load GLiNER weights\n",
    "    state_dict = torch.load(\n",
    "        gliner_path / \"pytorch_model.bin\",\n",
    "        map_location=torch.device(\"cpu\")\n",
    "    )\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Paths to local models\n",
    "deberta_path = \"/Users/firaterman/Documents/fer/deberta-v3-small\"\n",
    "gliner_path = \"/Users/firaterman/Documents/fer/gliner_small-v2.1\"\n",
    "\n",
    "# Load model\n",
    "model = load_local_gliner(gliner_path, deberta_path)\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Obama is the president of the USA and he will visit France with Trump\n",
    "\"\"\"\n",
    "\n",
    "# Define labels\n",
    "labels = [\"Person\", \"Date\", \"Country\", \"Location\"]\n",
    "\n",
    "# Predict entities\n",
    "entities = model.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Display results\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_28424/4103473976.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama => Person\n",
      "president => Person\n",
      "USA => Location\n",
      "France => Location\n",
      "Trump => Person\n",
      "employees => Person\n",
      "DMC => Person\n",
      "Harry Potter => Person\n",
      "man => Person\n",
      "Jean => Person\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from gliner import GLiNER\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def load_local_gliner(gliner_path, deberta_path):\n",
    "    gliner_path = Path(gliner_path)\n",
    "    deberta_path = Path(deberta_path)\n",
    "\n",
    "    # Load GLiNER config\n",
    "    with open(gliner_path / \"gliner_config.json\", 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    # Convert dictionary to namespace\n",
    "    config = SimpleNamespace(**config_dict)\n",
    "    \n",
    "    # Point to local DeBERTa path\n",
    "    config.model_name = str(deberta_path)\n",
    "\n",
    "    # Initialize GLiNER model with config namespace\n",
    "    model = GLiNER(config)\n",
    "    \n",
    "    # Load GLiNER weights\n",
    "    state_dict = torch.load(\n",
    "        gliner_path / \"pytorch_model.bin\",\n",
    "        map_location=torch.device(\"cpu\")\n",
    "    )\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Paths to local models\n",
    "deberta_path = \"/Users/firaterman/Documents/fer/mdberta_v3_base\"\n",
    "gliner_path = \"/Users/firaterman/Documents/fer/gliner_multi_v2.1\"\n",
    "\n",
    "# Load model\n",
    "model = load_local_gliner(gliner_path, deberta_path)\n",
    "\n",
    "# Sample text\n",
    "text = \"Obama is the president of the USA and he will visit France with Trump. The employees were there for him. DMC is specialized in making cars. There is also Harry Potter present there. Microsoft is also doing some updates. The man is sick. Jean is absent. The firm is generating money\"\n",
    "\n",
    "# Define labels\n",
    "labels = [\"Person\", \"Date\", \"Country\", \"Location\"]\n",
    "\n",
    "# Predict entities\n",
    "entities = model.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Display results\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_28424/3943298557.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'end': 5, 'text': 'Obama', 'label': 'Person', 'score': 0.9598107933998108}, {'start': 13, 'end': 22, 'text': 'president', 'label': 'Person', 'score': 0.23888185620307922}, {'start': 30, 'end': 33, 'text': 'USA', 'label': 'organization', 'score': 0.1632390171289444}, {'start': 64, 'end': 69, 'text': 'Trump', 'label': 'Person', 'score': 0.9008338451385498}, {'start': 105, 'end': 108, 'text': 'DMC', 'label': 'organization', 'score': 0.6567029356956482}, {'start': 154, 'end': 166, 'text': 'Harry Potter', 'label': 'Person', 'score': 0.6932865381240845}, {'start': 182, 'end': 191, 'text': 'Microsoft', 'label': 'organization', 'score': 0.6205271482467651}, {'start': 224, 'end': 227, 'text': 'man', 'label': 'Person', 'score': 0.1237051859498024}, {'start': 237, 'end': 241, 'text': 'Jean', 'label': 'Person', 'score': 0.8641250729560852}, {'start': 257, 'end': 261, 'text': 'firm', 'label': 'organization', 'score': 0.11162350326776505}]\n",
      "Obama => Person\n",
      "Trump => Person\n",
      "DMC => organization\n",
      "Harry Potter => Person\n",
      "Microsoft => organization\n",
      "Jean => Person\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from gliner import GLiNER\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def load_local_gliner(gliner_path, deberta_path):\n",
    "   gliner_path = Path(gliner_path)\n",
    "   deberta_path = Path(deberta_path)\n",
    "   \n",
    "   with open(gliner_path / \"gliner_config.json\", 'r') as f:\n",
    "       config_dict = json.load(f)\n",
    "   config = SimpleNamespace(**config_dict)\n",
    "   \n",
    "   config.model_name = str(deberta_path)\n",
    "\n",
    "   model = GLiNER(config)\n",
    "   state_dict = torch.load(\n",
    "       gliner_path / \"pytorch_model.bin\",\n",
    "       map_location=torch.device(\"cpu\")\n",
    "   )\n",
    "   model.load_state_dict(state_dict, strict=True)\n",
    "   model.to(\"cpu\")\n",
    "   return model\n",
    "\n",
    "# Paths\n",
    "deberta_path = \"/Users/firaterman/Documents/fer/mdberta_v3_base\"\n",
    "gliner_path = \"/Users/firaterman/Documents/fer/gliner_multi_v2.1\"\n",
    "\n",
    "# Load and predict\n",
    "model = load_local_gliner(gliner_path, deberta_path)\n",
    "\n",
    "text = \"Obama is the president of the USA and he will visit France with Trump. The employees were there for him. DMC is specialized in making cars. There is also Harry Potter present there. Microsoft is also doing some updates. The man is sick. Jean is absent. The firm is generating money\"\n",
    "# text= \"The firm offers accountancy, taxation, financial consultancy and insolvency services to foreign and local clients\"\n",
    "\n",
    "# Define labels\n",
    "labels = [\"Person\", \"organization\", \"company\"]\n",
    "entities = model.predict_entities(text, labels, threshold=0.1)\n",
    "\n",
    "print(entities)\n",
    "for entity in entities:\n",
    "   if entity[\"score\"]>0.5:\n",
    "      print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
