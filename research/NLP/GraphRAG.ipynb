{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1-2: Entity Extraction\n",
    "\n",
    "- Uses spaCy to find people, organizations, money, locations\n",
    "- Adds custom regex for case numbers\n",
    "- Creates nodes in the knowledge graph\n",
    "\n",
    "# Step 3: Relationship Discovery\n",
    "\n",
    "- Finds entities mentioned in the same sentence\n",
    "- Infers relationship types (executive, legal action, employment), multiple strategies possible:\n",
    "    - option1 : keyword rules with Spacy (if sentence contains ceo, executive ... then \"EXECUTIVE\")\n",
    "    - option2: Spacy dependency Parsing (rule-based, smarter) to detect grammatical relationship between entities.\n",
    "    - option3: Pretrained relation extraction models : HF transformers Bert-for-relation-extraction\n",
    "    - option4: Use directly an LLM with a prompt to extract relationship from text \n",
    "- Adds edges between connected entities\n",
    "\n",
    "Step 4: Visualization\n",
    "\n",
    "Shows the knowledge graph with colored nodes\n",
    "Different colors for different entity types\n",
    "Displays entity names and connections\n",
    "\n",
    "Step 5-6: GraphRAG Querying\n",
    "\n",
    "Simple keyword matching to find relevant entities\n",
    "Expands to include connected entities (multi-hop)\n",
    "Shows relationships and supporting evidence\n",
    "\n",
    "Step 7: Traditional vs GraphRAG\n",
    "\n",
    "Demonstrates how traditional RAG returns isolated chunks\n",
    "Shows how GraphRAG reveals relationships and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load spaCy (if this fails, run: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am john and you are marcos. Mark is absent.\n",
      "(john, marcos, Mark)\n",
      "tokenize I\n",
      "tokenize am\n",
      "tokenize john\n",
      "tokenize and\n",
      "tokenize you\n",
      "tokenize are\n",
      "tokenize marcos\n",
      "tokenize .\n",
      "tokenize Mark\n",
      "tokenize is\n",
      "tokenize absent\n",
      "tokenize .\n",
      "POS tagging I PRON\n",
      "POS tagging am AUX\n",
      "POS tagging john PROPN\n",
      "POS tagging and CCONJ\n",
      "POS tagging you PRON\n",
      "POS tagging are AUX\n",
      "POS tagging marcos PROPN\n",
      "POS tagging . PUNCT\n",
      "POS tagging Mark PROPN\n",
      "POS tagging is AUX\n",
      "POS tagging absent ADJ\n",
      "POS tagging . PUNCT\n",
      "NER john PERSON\n",
      "NER marcos PERSON\n",
      "NER Mark PERSON\n",
      "dependency parsing I nsubj am\n",
      "dependency parsing am ROOT am\n",
      "dependency parsing john attr am\n",
      "dependency parsing and cc am\n",
      "dependency parsing you nsubj are\n",
      "dependency parsing are conj am\n",
      "dependency parsing marcos attr are\n",
      "dependency parsing . punct are\n",
      "dependency parsing Mark nsubj is\n",
      "dependency parsing is ROOT is\n",
      "dependency parsing absent acomp is\n",
      "dependency parsing . punct is\n",
      "lemmatization I I\n",
      "lemmatization am be\n",
      "lemmatization john john\n",
      "lemmatization and and\n",
      "lemmatization you you\n",
      "lemmatization are be\n",
      "lemmatization marcos marcos\n",
      "lemmatization . .\n",
      "lemmatization Mark Mark\n",
      "lemmatization is be\n",
      "lemmatization absent absent\n",
      "lemmatization . .\n",
      "stop_word_detection I\n",
      "stop_word_detection am\n",
      "stop_word_detection and\n",
      "stop_word_detection you\n",
      "stop_word_detection are\n",
      "stop_word_detection is\n",
      "text similarity 0.8712082441268073\n",
      "sentence segmentation I am john and you are marcos.\n",
      "sentence segmentation Mark is absent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/83m5p3m94s7gmtrc3_97lxvc0000gn/T/ipykernel_14406/533509402.py:34: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(\"text similarity\",doc1.similarity(doc2))\n"
     ]
    }
   ],
   "source": [
    "# how spacy detect entity via this language model\n",
    "doc = nlp(\"I am john and you are marcos. Mark is absent.\")\n",
    "print(doc)\n",
    "print(doc.ents)\n",
    "\n",
    "# spacy can tokenize\n",
    "for token in doc:\n",
    "    print(\"tokenize\",token.text)\n",
    "\n",
    "# Spacy can do POS tagging\n",
    "for token in doc:\n",
    "    print(\"POS tagging\",token.text, token.pos_)\n",
    "\n",
    "# Spacy can do NER \n",
    "for ent in doc.ents:\n",
    "    print(\"NER\",ent.text, ent.label_)\n",
    "\n",
    "# Spacy can do dependency parsing: understand the grammatical relationships between words\n",
    "for token in doc:\n",
    "    print(\"dependency parsing\",token.text, token.dep_, token.head.text)\n",
    "\n",
    "#Lemmatization: get the base form of words\n",
    "for token in doc:\n",
    "    print(\"lemmatization\",token.text, token.lemma_)\n",
    "\n",
    "# Stop word detection\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        print(\"stop_word_detection\",token.text)\n",
    "\n",
    "# Text similarity\n",
    "doc1 = nlp(\"Apple is a tech company\")\n",
    "doc2 = nlp(\"Google is a technology firm\")\n",
    "print(\"text similarity\",doc1.similarity(doc2))\n",
    "\n",
    "# Sentence segmentation\n",
    "for sent in doc.sents:\n",
    "    print(\"sentence segmentation\",sent.text)\n",
    "\n",
    "# Rule-Based matching \n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence segmentation I am john and you are marcos.\n",
      "sentence segmentation Mark is absent.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\"sentence segmentation\",sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n",
      "AUX\n",
      "PROPN\n",
      "CCONJ\n",
      "PRON\n",
      "AUX\n",
      "PROPN\n",
      "PUNCT\n",
      "PROPN\n",
      "AUX\n",
      "ADJ\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "for i in doc: \n",
    "    print(i.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample Documents Loaded\n",
      "Total documents: 3\n",
      "\n",
      "üîç Extracting Entities...\n",
      "doc1: Found 16 entities\n",
      "doc2: Found 13 entities\n",
      "doc3: Found 8 entities\n",
      "\n",
      "üìã Sample Entities:\n",
      "1. SEC (ORG) from doc1\n",
      "2. PharmaCorp Inc. (ORG) from doc1\n",
      "3. Case No (ORG) from doc1\n",
      "4. the Securities and Exchange Commission (ORG) from doc1\n",
      "5. PharmaCorp Inc. (ORG) from doc1\n",
      "6. John Smith (PERSON) from doc1\n",
      "7. Smith (PERSON) from doc1\n",
      "8. CFO (ORG) from doc1\n",
      "9. Sarah Johnson (PERSON) from doc1\n",
      "10. PharmaCorp (ORG) from doc1\n"
     ]
    }
   ],
   "source": [
    "# Sample legal documents\n",
    "documents = {\n",
    "    \"doc1\": \"\"\"\n",
    "    In SEC v. PharmaCorp Inc., Case No. 2023-156, the Securities and Exchange Commission \n",
    "    filed charges against PharmaCorp Inc. and its CEO John Smith for securities fraud. \n",
    "    The investigation revealed that Smith, along with CFO Sarah Johnson, manipulated drug \n",
    "    trial results. PharmaCorp had previously faced FDA warnings. The fraud cost investors \n",
    "    $50 million. Similar patterns were observed in SEC Case No. 2022-089 involving BioTech Inc.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"doc2\": \"\"\"\n",
    "    United States District Court ruled in favor of the SEC in Case No. 2023-156. \n",
    "    Judge Robert Wilson found that PharmaCorp's executives violated securities law. \n",
    "    John Smith had previously worked at BioMed Industries, which faced similar SEC violations. \n",
    "    Sarah Johnson joined PharmaCorp from the same company. The court ordered $75 million in penalties.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"doc3\": \"\"\"\n",
    "    BioTech Inc. was investigated in Case No. 2022-089 for similar fraud patterns. \n",
    "    CEO Michael Brown used identical methodologies to deceive investors. \n",
    "    The timeline shows FDA warnings preceded SEC investigations by 3 months in both cases.\n",
    "    Law firm Wilson & Associates represented both PharmaCorp and BioTech Inc.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"üìÑ Sample Documents Loaded\")\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Extract Entities from Documents\n",
    "# =============================================================================\n",
    "\n",
    "# Create empty graph and entity storage\n",
    "graph = nx.Graph()\n",
    "all_entities = []\n",
    "\n",
    "def extract_entities(text, doc_id):\n",
    "    \"\"\"Extract entities using spaCy\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON', 'ORG', 'MONEY', 'GPE']:  # Focus on key types\n",
    "            entities.append({\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'doc_id': doc_id,\n",
    "                'sentence': ent.sent.text\n",
    "            })\n",
    "    \n",
    "    # Also extract case numbers with regex\n",
    "    case_pattern = r'Case No\\. (\\d{4}-\\d{3})'\n",
    "    for match in re.finditer(case_pattern, text):\n",
    "        entities.append({\n",
    "            'text': match.group(1),\n",
    "            'label': 'CASE_NUMBER',\n",
    "            'doc_id': doc_id,\n",
    "            'sentence': text[max(0, match.start()-50):match.end()+50]\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities from all documents\n",
    "print(\"\\nüîç Extracting Entities...\")\n",
    "for doc_id, text in documents.items():\n",
    "    entities = extract_entities(text, doc_id)\n",
    "    all_entities.extend(entities)\n",
    "    print(f\"{doc_id}: Found {len(entities)} entities\")\n",
    "\n",
    "# Show some entities\n",
    "print(\"\\nüìã Sample Entities:\")\n",
    "for i, entity in enumerate(all_entities[:10]):\n",
    "    print(f\"{i+1}. {entity['text']} ({entity['label']}) from {entity['doc_id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Finding Relationships...\n",
      "sent_entities [('SEC', 'ORG'), ('PharmaCorp Inc.', 'ORG'), ('Case No', 'ORG'), ('the Securities and Exchange Commission', 'ORG'), ('PharmaCorp Inc.', 'ORG'), ('John Smith', 'PERSON'), ('2023-156', 'CASE_NUMBER')]\n",
      "sent_entities [('Smith', 'PERSON'), ('CFO', 'ORG'), ('Sarah Johnson', 'PERSON')]\n",
      "sent_entities [('PharmaCorp', 'ORG'), ('FDA', 'ORG')]\n",
      "sent_entities [('$50 million', 'MONEY')]\n",
      "sent_entities [('SEC', 'ORG'), ('BioTech Inc.', 'ORG'), ('2022-089', 'CASE_NUMBER')]\n",
      "doc1: Found 28 relationships\n",
      "sent_entities [('United States', 'GPE'), ('District Court', 'ORG'), ('SEC', 'ORG'), ('Case No', 'ORG'), ('2023-156', 'CASE_NUMBER')]\n",
      "sent_entities [('Robert Wilson', 'PERSON'), ('PharmaCorp', 'ORG')]\n",
      "sent_entities [('John Smith', 'PERSON'), ('BioMed Industries', 'ORG'), ('SEC', 'ORG')]\n",
      "sent_entities [('Sarah Johnson', 'PERSON'), ('PharmaCorp', 'ORG')]\n",
      "sent_entities [('$75 million', 'MONEY')]\n",
      "doc2: Found 15 relationships\n",
      "sent_entities [('BioTech Inc.', 'ORG'), ('2022-089', 'CASE_NUMBER')]\n",
      "sent_entities [('Michael Brown', 'PERSON')]\n",
      "sent_entities [('FDA', 'ORG'), ('SEC', 'ORG')]\n",
      "sent_entities [('Wilson & Associates', 'ORG'), ('PharmaCorp', 'ORG'), ('BioTech Inc.', 'ORG')]\n",
      "doc3: Found 5 relationships\n",
      "Added 0 relationships to graph\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Create Relationships\n",
    "# =============================================================================\n",
    "\n",
    "def find_relationships(text, doc_id):\n",
    "    \"\"\"Find relationships between entities in sentences\"\"\"\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        # Get entities in this sentence\n",
    "        sent_entities = []\n",
    "        for ent in sent.ents:\n",
    "            if ent.label_ in ['PERSON', 'ORG', 'MONEY', 'GPE']:\n",
    "                sent_entities.append((ent.text, ent.label_))\n",
    "        \n",
    "        # Also check for legal case numbers\n",
    "        case_matches = re.findall(r'Case No\\. (\\d{4}-\\d{3})', sent.text)\n",
    "        for case in case_matches:\n",
    "            sent_entities.append((case, 'CASE_NUMBER'))\n",
    "        \n",
    "        print(\"sent_entities\", sent_entities)\n",
    "        # Create relationships between entities in same sentence\n",
    "        for i, (ent1, type1) in enumerate(sent_entities):\n",
    "            for ent2, type2 in sent_entities[i+1:]:\n",
    "                # Determine relationship type based on sentence content\n",
    "                rel_type = \"MENTIONED_WITH\"\n",
    "                sent_lower = sent.text.lower()\n",
    "                \n",
    "                if any(word in sent_lower for word in ['ceo', 'president', 'officer']):\n",
    "                    rel_type = \"EXECUTIVE_OF\"\n",
    "                elif any(word in sent_lower for word in ['sued', 'charges', 'violation']):\n",
    "                    rel_type = \"LEGAL_ACTION\"\n",
    "                elif any(word in sent_lower for word in ['worked', 'joined', 'from']):\n",
    "                    rel_type = \"EMPLOYMENT\"\n",
    "                \n",
    "                relationships.append({\n",
    "                    'source': f\"{ent1}_{type1}\",\n",
    "                    'target': f\"{ent2}_{type2}\",\n",
    "                    'type': rel_type,\n",
    "                    'doc_id': doc_id,\n",
    "                    'context': sent.text\n",
    "                })\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# Find relationships in all documents\n",
    "print(\"\\nüîó Finding Relationships...\")\n",
    "all_relationships = []\n",
    "for doc_id, text in documents.items():\n",
    "    relationships = find_relationships(text, doc_id)\n",
    "    all_relationships.extend(relationships)\n",
    "    print(f\"{doc_id}: Found {len(relationships)} relationships\")\n",
    "\n",
    "# Add relationships to graph\n",
    "for rel in all_relationships:\n",
    "    if (graph.has_node(rel['source']) and graph.has_node(rel['target'])):\n",
    "        if not graph.has_edge(rel['source'], rel['target']):\n",
    "            graph.add_edge(rel['source'], rel['target'], \n",
    "                          relation_type=rel['type'],\n",
    "                          docs=[rel['doc_id']],\n",
    "                          contexts=[rel['context']])\n",
    "        else:\n",
    "            # Update existing edge\n",
    "            if rel['doc_id'] not in graph.edges[rel['source'], rel['target']]['docs']:\n",
    "                graph.edges[rel['source'], rel['target']]['docs'].append(rel['doc_id'])\n",
    "                graph.edges[rel['source'], rel['target']]['contexts'].append(rel['context'])\n",
    "\n",
    "print(f\"Added {len(graph.edges)} relationships to graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeView([])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Visualizing Knowledge Graph...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Visualize the Graph\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüé® Visualizing Knowledge Graph...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create layout\n",
    "pos = nx.spring_layout(graph, k=2, iterations=50)\n",
    "\n",
    "# Color nodes by type\n",
    "node_colors = []\n",
    "color_map = {\n",
    "    'PERSON': 'lightblue', \n",
    "    'ORG': 'lightgreen', \n",
    "    'CASE_NUMBER': 'orange',\n",
    "    'MONEY': 'gold',\n",
    "    'GPE': 'pink'\n",
    "}\n",
    "\n",
    "for node in graph.nodes():\n",
    "    node_type = graph.nodes[node]['type']\n",
    "    node_colors.append(color_map.get(node_type, 'gray'))\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw_networkx_nodes(graph, pos, node_color=node_colors, \n",
    "                      node_size=1000, alpha=0.8)\n",
    "nx.draw_networkx_edges(graph, pos, alpha=0.5, width=2)\n",
    "\n",
    "# Add labels\n",
    "labels = {node: graph.nodes[node]['name'][:15] for node in graph.nodes()}\n",
    "nx.draw_networkx_labels(graph, pos, labels, font_size=8)\n",
    "\n",
    "plt.title(\"Legal Document Knowledge Graph\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 5: Simple GraphRAG Query Function\n",
    "# =============================================================================\n",
    "\n",
    "def query_graph(query_text, max_hops=2):\n",
    "    \"\"\"Simple GraphRAG query\"\"\"\n",
    "    print(f\"\\nüîç Query: '{query_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Find relevant nodes (simple keyword matching)\n",
    "    relevant_nodes = []\n",
    "    query_words = query_text.lower().split()\n",
    "    \n",
    "    for node in graph.nodes():\n",
    "        node_name = graph.nodes[node]['name'].lower()\n",
    "        if any(word in node_name for word in query_words):\n",
    "            relevant_nodes.append(node)\n",
    "    \n",
    "    if not relevant_nodes:\n",
    "        print(\"‚ùå No relevant entities found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìç Found {len(relevant_nodes)} relevant entities:\")\n",
    "    for node in relevant_nodes:\n",
    "        print(f\"  ‚Ä¢ {graph.nodes[node]['name']} ({graph.nodes[node]['type']})\")\n",
    "    \n",
    "    # Expand to include connected nodes\n",
    "    expanded_nodes = set(relevant_nodes)\n",
    "    for node in relevant_nodes:\n",
    "        neighbors = list(graph.neighbors(node))\n",
    "        expanded_nodes.update(neighbors[:5])  # Limit to prevent huge results\n",
    "    \n",
    "    # Create subgraph\n",
    "    subgraph = graph.subgraph(expanded_nodes)\n",
    "    \n",
    "    print(f\"\\nüï∏Ô∏è Expanded network: {len(subgraph.nodes)} entities, {len(subgraph.edges)} connections\")\n",
    "    \n",
    "    # Analyze relationships\n",
    "    print(\"\\nüîó Key Relationships:\")\n",
    "    for edge in subgraph.edges(data=True):\n",
    "        source_name = subgraph.nodes[edge[0]]['name']\n",
    "        target_name = subgraph.nodes[edge[1]]['name']\n",
    "        rel_type = edge[2].get('relation_type', 'CONNECTED')\n",
    "        print(f\"  ‚Ä¢ {source_name} --[{rel_type}]--> {target_name}\")\n",
    "    \n",
    "    # Show contexts\n",
    "    print(f\"\\nüìù Supporting Evidence:\")\n",
    "    contexts_shown = 0\n",
    "    for edge in subgraph.edges(data=True):\n",
    "        if 'contexts' in edge[2] and contexts_shown < 3:\n",
    "            print(f\"  ‚Ä¢ {edge[2]['contexts'][0][:100]}...\")\n",
    "            contexts_shown += 1\n",
    "    \n",
    "    return subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ TESTING GRAPHRAG QUERIES\n",
      "============================================================\n",
      "\n",
      "üîç Query: 'PharmaCorp executives fraud'\n",
      "--------------------------------------------------\n",
      "‚ùå No relevant entities found\n",
      "\n",
      "üîç Query: 'John Smith connections'\n",
      "--------------------------------------------------\n",
      "‚ùå No relevant entities found\n",
      "\n",
      "üîç Query: 'case numbers SEC investigations'\n",
      "--------------------------------------------------\n",
      "‚ùå No relevant entities found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 6: Test GraphRAG Queries\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TESTING GRAPHRAG QUERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test Query 1\n",
    "subgraph1 = query_graph(\"PharmaCorp executives fraud\")\n",
    "\n",
    "# Test Query 2  \n",
    "subgraph2 = query_graph(\"John Smith connections\")\n",
    "\n",
    "# Test Query 3\n",
    "subgraph3 = query_graph(\"case numbers SEC investigations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üÜö TRADITIONAL RAG vs GRAPHRAG COMPARISON\n",
      "============================================================\n",
      "\n",
      "üì∞ Traditional RAG for: 'PharmaCorp executives'\n",
      "Would return these isolated chunks:\n",
      "  1. PharmaCorp Inc...\n",
      "  2. 2023-156, the Securities and Exchange Commission \n",
      "    filed charges against PharmaCorp Inc...\n",
      "  3. PharmaCorp had previously faced FDA warnings...\n",
      "‚ùå Issues: Isolated facts, no relationships, missing context\n",
      "\n",
      "üîç Query: 'PharmaCorp executives'\n",
      "--------------------------------------------------\n",
      "‚ùå No relevant entities found\n",
      "\n",
      "‚úÖ GraphRAG Benefits:\n",
      "  ‚Ä¢ Shows entity relationships and connections\n",
      "  ‚Ä¢ Reveals patterns across multiple documents\n",
      "  ‚Ä¢ Provides network context and supporting evidence\n",
      "  ‚Ä¢ Enables multi-hop reasoning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 7: Compare with Traditional RAG\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üÜö TRADITIONAL RAG vs GRAPHRAG COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def traditional_rag_simulation(query):\n",
    "    \"\"\"Simulate what traditional RAG would return\"\"\"\n",
    "    print(f\"\\nüì∞ Traditional RAG for: '{query}'\")\n",
    "    print(\"Would return these isolated chunks:\")\n",
    "    \n",
    "    # Simple keyword search in documents\n",
    "    query_words = query.lower().split()\n",
    "    chunks = []\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        sentences = text.split('.')\n",
    "        for sentence in sentences:\n",
    "            if any(word in sentence.lower() for word in query_words):\n",
    "                chunks.append(sentence.strip()[:100] + \"...\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[:3], 1):\n",
    "        print(f\"  {i}. {chunk}\")\n",
    "    \n",
    "    print(\"‚ùå Issues: Isolated facts, no relationships, missing context\")\n",
    "\n",
    "# Compare both approaches\n",
    "test_query = \"PharmaCorp executives\"\n",
    "\n",
    "traditional_rag_simulation(test_query)\n",
    "query_graph(test_query)\n",
    "\n",
    "print(f\"\\n‚úÖ GraphRAG Benefits:\")\n",
    "print(\"  ‚Ä¢ Shows entity relationships and connections\")\n",
    "print(\"  ‚Ä¢ Reveals patterns across multiple documents\") \n",
    "print(\"  ‚Ä¢ Provides network context and supporting evidence\")\n",
    "print(\"  ‚Ä¢ Enables multi-hop reasoning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä GRAPH STATISTICS\n",
      "------------------------------\n",
      "Total entities: 0\n",
      "Total relationships: 0\n",
      "Graph density: 0.000\n",
      "Connected components: 0\n",
      "\n",
      "Most connected entities:\n",
      "\n",
      "üéØ Try your own queries by calling: query_graph('your question here')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 8: Graph Statistics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìä GRAPH STATISTICS\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total entities: {len(graph.nodes)}\")\n",
    "print(f\"Total relationships: {len(graph.edges)}\")\n",
    "print(f\"Graph density: {nx.density(graph):.3f}\")\n",
    "print(f\"Connected components: {nx.number_connected_components(graph)}\")\n",
    "\n",
    "# Most connected entities\n",
    "degree_centrality = nx.degree_centrality(graph)\n",
    "top_central = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(f\"\\nMost connected entities:\")\n",
    "for node, centrality in top_central:\n",
    "    name = graph.nodes[node]['name']\n",
    "    node_type = graph.nodes[node]['type']\n",
    "    print(f\"  ‚Ä¢ {name} ({node_type}): {centrality:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Try your own queries by calling: query_graph('your question here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GraphRAG for Legal Documents - Notebook Version\n",
    "# Install these first: \n",
    "# pip install networkx matplotlib spacy transformers torch\n",
    "# pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_web_sm-3.4.1.tar.gz\n",
    "\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "print(\"üîß Loading NLP Models...\")\n",
    "\n",
    "# Option 1: spaCy with legal entities (most popular for NER)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ spaCy model loaded\")\n",
    "except:\n",
    "    print(\"‚ùå Please install: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "# Option 2: Hugging Face Transformers (state-of-the-art)\n",
    "print(\"üì¶ Loading Hugging Face legal NER model...\")\n",
    "try:\n",
    "    # Using a legal-specific BERT model for Named Entity Recognition\n",
    "    legal_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "    \n",
    "    # For NER, we'll use a general NER model that works well with legal text\n",
    "    ner_pipeline = pipeline(\n",
    "        \"ner\", \n",
    "        model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "        tokenizer=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "        aggregation_strategy=\"simple\"\n",
    "    )\n",
    "    print(\"‚úÖ Hugging Face BERT NER model loaded\")\n",
    "    USE_TRANSFORMERS = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Transformers not available: {e}\")\n",
    "    print(\"Using spaCy fallback\")\n",
    "    USE_TRANSFORMERS = False\n",
    "\n",
    "# Sample legal documents\n",
    "documents = {\n",
    "    \"doc1\": \"\"\"\n",
    "    In SEC v. PharmaCorp Inc., Case No. 2023-156, the Securities and Exchange Commission \n",
    "    filed charges against PharmaCorp Inc. and its CEO John Smith for securities fraud. \n",
    "    The investigation revealed that Smith, along with CFO Sarah Johnson, manipulated drug \n",
    "    trial results. PharmaCorp had previously faced FDA warnings. The fraud cost investors \n",
    "    $50 million. Similar patterns were observed in SEC Case No. 2022-089 involving BioTech Inc.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"doc2\": \"\"\"\n",
    "    United States District Court ruled in favor of the SEC in Case No. 2023-156. \n",
    "    Judge Robert Wilson found that PharmaCorp's executives violated securities law. \n",
    "    John Smith had previously worked at BioMed Industries, which faced similar SEC violations. \n",
    "    Sarah Johnson joined PharmaCorp from the same company. The court ordered $75 million in penalties.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"doc3\": \"\"\"\n",
    "    BioTech Inc. was investigated in Case No. 2022-089 for similar fraud patterns. \n",
    "    CEO Michael Brown used identical methodologies to deceive investors. \n",
    "    The timeline shows FDA warnings preceded SEC investigations by 3 months in both cases.\n",
    "    Law firm Wilson & Associates represented both PharmaCorp and BioTech Inc.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"üìÑ Sample Documents Loaded\")\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Extract Entities from Documents\n",
    "# =============================================================================\n",
    "\n",
    "# Create empty graph and entity storage\n",
    "graph = nx.Graph()\n",
    "all_entities = []\n",
    "\n",
    "def extract_entities_advanced(text, doc_id):\n",
    "    \"\"\"Extract entities using multiple methods\"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    # Method 1: Hugging Face Transformers (BERT-based NER)\n",
    "    if USE_TRANSFORMERS:\n",
    "        print(f\"  ü§ñ Using BERT for {doc_id}...\")\n",
    "        try:\n",
    "            ner_results = ner_pipeline(text)\n",
    "            for entity in ner_results:\n",
    "                if entity['score'] > 0.8:  # High confidence only\n",
    "                    entities.append({\n",
    "                        'text': entity['word'].replace('##', ''),  # Clean subword tokens\n",
    "                        'label': entity['entity_group'],\n",
    "                        'confidence': entity['score'],\n",
    "                        'doc_id': doc_id,\n",
    "                        'method': 'BERT',\n",
    "                        'sentence': text[max(0, entity['start']-50):entity['end']+50]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è BERT failed: {e}\")\n",
    "    \n",
    "    # Method 2: spaCy NER (fallback and additional entities)\n",
    "    print(f\"  üîç Using spaCy for {doc_id}...\")\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        # Focus on legal-relevant entity types\n",
    "        if ent.label_ in ['PERSON', 'ORG', 'MONEY', 'GPE', 'DATE', 'CARDINAL']:\n",
    "            entities.append({\n",
    "                'text': ent.text.strip(),\n",
    "                'label': ent.label_,\n",
    "                'confidence': 0.85,  # spaCy doesn't provide confidence, so we estimate\n",
    "                'doc_id': doc_id,\n",
    "                'method': 'spaCy',\n",
    "                'sentence': ent.sent.text\n",
    "            })\n",
    "    \n",
    "    # Method 3: Legal-specific regex patterns (domain expertise)\n",
    "    print(f\"  ‚öñÔ∏è Using legal patterns for {doc_id}...\")\n",
    "    legal_patterns = {\n",
    "        'CASE_NUMBER': r'Case No\\.\\s*(\\d{4}-\\d{3})',\n",
    "        'SEC_CASE': r'SEC\\s+(?:v\\.|Case)\\s+([A-Za-z\\s&]+(?:Inc\\.|Corp\\.|LLC)?)',\n",
    "        'COURT': r'((?:United States\\s+)?District Court(?:\\s+for\\s+[\\w\\s]+)?)',\n",
    "        'JUDGE': r'Judge\\s+([A-Z][a-z]+\\s+[A-Z][a-z]+)',\n",
    "        'LAW_FIRM': r'([A-Z][a-z]+(?:\\s+&\\s+[A-Z][a-z]+)*\\s+(?:LLP|LLC|Associates))',\n",
    "        'REGULATION': r'(Rule\\s+\\d+[a-z]?(?:-\\d+)?)',\n",
    "        'PENALTY': r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\\s+(?:million|billion)?'\n",
    "    }\n",
    "    \n",
    "    for pattern_name, pattern in legal_patterns.items():\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            entity_text = match.group(1) if match.groups() else match.group(0)\n",
    "            entities.append({\n",
    "                'text': entity_text.strip(),\n",
    "                'label': pattern_name,\n",
    "                'confidence': 0.95,  # High confidence for regex matches\n",
    "                'doc_id': doc_id,\n",
    "                'method': 'Legal_Regex',\n",
    "                'sentence': text[max(0, match.start()-100):match.end()+100]\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates and clean\n",
    "    seen = set()\n",
    "    clean_entities = []\n",
    "    for entity in entities:\n",
    "        # Create a key for deduplication\n",
    "        key = (entity['text'].lower(), entity['label'])\n",
    "        if key not in seen and len(entity['text']) > 1:\n",
    "            seen.add(key)\n",
    "            clean_entities.append(entity)\n",
    "    \n",
    "    return clean_entities\n",
    "\n",
    "def extract_entities(text, doc_id):\n",
    "    \"\"\"Original simple function - keeping for compatibility\"\"\"\n",
    "    return extract_entities_advanced(text, doc_id)\n",
    "\n",
    "# Extract entities from all documents\n",
    "print(\"\\nüîç Extracting Entities with Advanced NLP...\")\n",
    "for doc_id, text in documents.items():\n",
    "    print(f\"\\nüìÑ Processing {doc_id}:\")\n",
    "    entities = extract_entities(text, doc_id)\n",
    "    all_entities.extend(entities)\n",
    "    \n",
    "    # Show breakdown by method\n",
    "    method_counts = defaultdict(int)\n",
    "    for entity in entities:\n",
    "        method_counts[entity['method']] += 1\n",
    "    \n",
    "    print(f\"  ‚úÖ Found {len(entities)} entities:\")\n",
    "    for method, count in method_counts.items():\n",
    "        print(f\"    ‚Ä¢ {method}: {count}\")\n",
    "\n",
    "# Show some entities with confidence scores\n",
    "print(f\"\\nüìã Sample Entities (Total: {len(all_entities)}):\")\n",
    "for i, entity in enumerate(all_entities[:15]):\n",
    "    confidence = entity.get('confidence', 0.0)\n",
    "    method = entity.get('method', 'Unknown')\n",
    "    print(f\"{i+1:2d}. {entity['text']:20} ({entity['label']:12}) \"\n",
    "          f\"Conf: {confidence:.2f} [{method}] from {entity['doc_id']}\")\n",
    "\n",
    "# Show entity type distribution\n",
    "print(f\"\\nüìä Entity Distribution:\")\n",
    "type_counts = defaultdict(int)\n",
    "method_counts = defaultdict(int)\n",
    "for entity in all_entities:\n",
    "    type_counts[entity['label']] += 1\n",
    "    method_counts[entity.get('method', 'Unknown')] += 1\n",
    "\n",
    "print(\"By Type:\")\n",
    "for ent_type, count in sorted(type_counts.items()):\n",
    "    print(f\"  ‚Ä¢ {ent_type}: {count}\")\n",
    "\n",
    "print(\"By Extraction Method:\")\n",
    "for method, count in sorted(method_counts.items()):\n",
    "    print(f\"  ‚Ä¢ {method}: {count}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Add Entities to Graph\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüï∏Ô∏è Building Knowledge Graph...\")\n",
    "\n",
    "# Add entities as nodes with enhanced information\n",
    "entity_counts = defaultdict(int)\n",
    "confidence_scores = defaultdict(list)\n",
    "\n",
    "for entity in all_entities:\n",
    "    # Create unique node ID\n",
    "    node_id = f\"{entity['text']}_{entity['label']}\"\n",
    "    \n",
    "    if not graph.has_node(node_id):\n",
    "        graph.add_node(node_id, \n",
    "                      name=entity['text'], \n",
    "                      type=entity['label'],\n",
    "                      docs=[entity['doc_id']],\n",
    "                      methods=[entity.get('method', 'Unknown')],\n",
    "                      confidences=[entity.get('confidence', 0.0)],\n",
    "                      count=1)\n",
    "    else:\n",
    "        # Update existing node\n",
    "        graph.nodes[node_id]['count'] += 1\n",
    "        if entity['doc_id'] not in graph.nodes[node_id]['docs']:\n",
    "            graph.nodes[node_id]['docs'].append(entity['doc_id'])\n",
    "        if entity.get('method') not in graph.nodes[node_id]['methods']:\n",
    "            graph.nodes[node_id]['methods'].append(entity.get('method', 'Unknown'))\n",
    "        graph.nodes[node_id]['confidences'].append(entity.get('confidence', 0.0))\n",
    "    \n",
    "    entity_counts[entity['label']] += 1\n",
    "    confidence_scores[entity['label']].append(entity.get('confidence', 0.0))\n",
    "\n",
    "print(f\"‚úÖ Added {len(graph.nodes)} unique entities to graph\")\n",
    "print(\"\\nEntity types with average confidence:\")\n",
    "for ent_type, count in entity_counts.items():\n",
    "    avg_conf = sum(confidence_scores[ent_type]) / len(confidence_scores[ent_type])\n",
    "    print(f\"  ‚Ä¢ {ent_type}: {count} entities (avg confidence: {avg_conf:.2f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Create Relationships\n",
    "# =============================================================================\n",
    "\n",
    "def find_relationships(text, doc_id):\n",
    "    \"\"\"Find relationships between entities in sentences\"\"\"\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        # Get entities in this sentence\n",
    "        sent_entities = []\n",
    "        for ent in sent.ents:\n",
    "            if ent.label_ in ['PERSON', 'ORG', 'MONEY', 'GPE']:\n",
    "                sent_entities.append((ent.text, ent.label_))\n",
    "        \n",
    "        # Also check for case numbers\n",
    "        case_matches = re.findall(r'Case No\\. (\\d{4}-\\d{3})', sent.text)\n",
    "        for case in case_matches:\n",
    "            sent_entities.append((case, 'CASE_NUMBER'))\n",
    "        \n",
    "        # Create relationships between entities in same sentence\n",
    "        for i, (ent1, type1) in enumerate(sent_entities):\n",
    "            for ent2, type2 in sent_entities[i+1:]:\n",
    "                # Determine relationship type based on sentence content\n",
    "                rel_type = \"MENTIONED_WITH\"\n",
    "                sent_lower = sent.text.lower()\n",
    "                \n",
    "                if any(word in sent_lower for word in ['ceo', 'president', 'officer']):\n",
    "                    rel_type = \"EXECUTIVE_OF\"\n",
    "                elif any(word in sent_lower for word in ['sued', 'charges', 'violation']):\n",
    "                    rel_type = \"LEGAL_ACTION\"\n",
    "                elif any(word in sent_lower for word in ['worked', 'joined', 'from']):\n",
    "                    rel_type = \"EMPLOYMENT\"\n",
    "                \n",
    "                relationships.append({\n",
    "                    'source': f\"{ent1}_{type1}\",\n",
    "                    'target': f\"{ent2}_{type2}\",\n",
    "                    'type': rel_type,\n",
    "                    'doc_id': doc_id,\n",
    "                    'context': sent.text\n",
    "                })\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# Find relationships in all documents\n",
    "print(\"\\nüîó Finding Relationships...\")\n",
    "all_relationships = []\n",
    "for doc_id, text in documents.items():\n",
    "    relationships = find_relationships(text, doc_id)\n",
    "    all_relationships.extend(relationships)\n",
    "    print(f\"{doc_id}: Found {len(relationships)} relationships\")\n",
    "\n",
    "# Add relationships to graph\n",
    "for rel in all_relationships:\n",
    "    if (graph.has_node(rel['source']) and graph.has_node(rel['target'])):\n",
    "        if not graph.has_edge(rel['source'], rel['target']):\n",
    "            graph.add_edge(rel['source'], rel['target'], \n",
    "                          relation_type=rel['type'],\n",
    "                          docs=[rel['doc_id']],\n",
    "                          contexts=[rel['context']])\n",
    "        else:\n",
    "            # Update existing edge\n",
    "            if rel['doc_id'] not in graph.edges[rel['source'], rel['target']]['docs']:\n",
    "                graph.edges[rel['source'], rel['target']]['docs'].append(rel['doc_id'])\n",
    "                graph.edges[rel['source'], rel['target']]['contexts'].append(rel['context'])\n",
    "\n",
    "print(f\"Added {len(graph.edges)} relationships to graph\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Visualize the Graph\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüé® Visualizing Knowledge Graph...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create layout\n",
    "pos = nx.spring_layout(graph, k=2, iterations=50)\n",
    "\n",
    "# Color nodes by type with more colors\n",
    "node_colors = []\n",
    "color_map = {\n",
    "    'PERSON': 'lightblue', \n",
    "    'ORG': 'lightgreen', \n",
    "    'CASE_NUMBER': 'orange',\n",
    "    'SEC_CASE': 'red',\n",
    "    'MONEY': 'gold',\n",
    "    'GPE': 'pink',\n",
    "    'DATE': 'lightcoral',\n",
    "    'COURT': 'purple',\n",
    "    'JUDGE': 'darkblue',\n",
    "    'LAW_FIRM': 'brown',\n",
    "    'REGULATION': 'darkgreen',\n",
    "    'PENALTY': 'darkgoldenrod'\n",
    "}\n",
    "\n",
    "for node in graph.nodes():\n",
    "    node_type = graph.nodes[node]['type']\n",
    "    node_colors.append(color_map.get(node_type, 'gray'))\n",
    "\n",
    "# Create node sizes based on confidence and connections\n",
    "node_sizes = []\n",
    "for node in graph.nodes():\n",
    "    base_size = 500\n",
    "    # Size based on average confidence\n",
    "    confidences = graph.nodes[node].get('confidences', [0.5])\n",
    "    avg_confidence = sum(confidences) / len(confidences)\n",
    "    confidence_bonus = avg_confidence * 500\n",
    "    # Size based on connections\n",
    "    connection_bonus = graph.degree(node) * 100\n",
    "    total_size = base_size + confidence_bonus + connection_bonus\n",
    "    node_sizes.append(min(total_size, 1500))  # Cap the size\n",
    "\n",
    "# Draw the enhanced graph\n",
    "plt.figure(figsize=(15, 10))\n",
    "nx.draw_networkx_nodes(graph, pos, node_color=node_colors, \n",
    "                      node_size=node_sizes, alpha=0.8)\n",
    "nx.draw_networkx_edges(graph, pos, alpha=0.5, width=2)\n",
    "\n",
    "# Add labels with confidence info\n",
    "labels = {}\n",
    "for node in graph.nodes():\n",
    "    name = graph.nodes[node]['name'][:15]\n",
    "    confidences = graph.nodes[node].get('confidences', [0])\n",
    "    avg_conf = sum(confidences) / len(confidences)\n",
    "    labels[node] = f\"{name}\\n({avg_conf:.2f})\"\n",
    "\n",
    "nx.draw_networkx_labels(graph, pos, labels, font_size=7)\n",
    "\n",
    "plt.title(\"Enhanced Legal Document Knowledge Graph\\n(Node size = confidence + connections)\", fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = []\n",
    "for ent_type, color in color_map.items():\n",
    "    if any(graph.nodes[node]['type'] == ent_type for node in graph.nodes()):\n",
    "        legend_elements.append(plt.scatter([], [], c=color, s=100, label=ent_type))\n",
    "\n",
    "if legend_elements:\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Graph Enhanced with:\")\n",
    "print(f\"  ‚Ä¢ Node colors represent entity types\")\n",
    "print(f\"  ‚Ä¢ Node sizes represent confidence + connections\")\n",
    "print(f\"  ‚Ä¢ Numbers in parentheses show average confidence scores\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: Simple GraphRAG Query Function\n",
    "# =============================================================================\n",
    "\n",
    "def query_graph(query_text, max_hops=2):\n",
    "    \"\"\"Simple GraphRAG query\"\"\"\n",
    "    print(f\"\\nüîç Query: '{query_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Find relevant nodes (simple keyword matching)\n",
    "    relevant_nodes = []\n",
    "    query_words = query_text.lower().split()\n",
    "    \n",
    "    for node in graph.nodes():\n",
    "        node_name = graph.nodes[node]['name'].lower()\n",
    "        if any(word in node_name for word in query_words):\n",
    "            relevant_nodes.append(node)\n",
    "    \n",
    "    if not relevant_nodes:\n",
    "        print(\"‚ùå No relevant entities found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìç Found {len(relevant_nodes)} relevant entities:\")\n",
    "    for node in relevant_nodes:\n",
    "        print(f\"  ‚Ä¢ {graph.nodes[node]['name']} ({graph.nodes[node]['type']})\")\n",
    "    \n",
    "    # Expand to include connected nodes\n",
    "    expanded_nodes = set(relevant_nodes)\n",
    "    for node in relevant_nodes:\n",
    "        neighbors = list(graph.neighbors(node))\n",
    "        expanded_nodes.update(neighbors[:5])  # Limit to prevent huge results\n",
    "    \n",
    "    # Create subgraph\n",
    "    subgraph = graph.subgraph(expanded_nodes)\n",
    "    \n",
    "    print(f\"\\nüï∏Ô∏è Expanded network: {len(subgraph.nodes)} entities, {len(subgraph.edges)} connections\")\n",
    "    \n",
    "    # Analyze relationships\n",
    "    print(\"\\nüîó Key Relationships:\")\n",
    "    for edge in subgraph.edges(data=True):\n",
    "        source_name = subgraph.nodes[edge[0]]['name']\n",
    "        target_name = subgraph.nodes[edge[1]]['name']\n",
    "        rel_type = edge[2].get('relation_type', 'CONNECTED')\n",
    "        print(f\"  ‚Ä¢ {source_name} --[{rel_type}]--> {target_name}\")\n",
    "    \n",
    "    # Show contexts\n",
    "    print(f\"\\nüìù Supporting Evidence:\")\n",
    "    contexts_shown = 0\n",
    "    for edge in subgraph.edges(data=True):\n",
    "        if 'contexts' in edge[2] and contexts_shown < 3:\n",
    "            print(f\"  ‚Ä¢ {edge[2]['contexts'][0][:100]}...\")\n",
    "            contexts_shown += 1\n",
    "    \n",
    "    return subgraph\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: Test GraphRAG Queries\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TESTING GRAPHRAG QUERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test Query 1\n",
    "subgraph1 = query_graph(\"PharmaCorp executives fraud\")\n",
    "\n",
    "# Test Query 2  \n",
    "subgraph2 = query_graph(\"John Smith connections\")\n",
    "\n",
    "# Test Query 3\n",
    "subgraph3 = query_graph(\"case numbers SEC investigations\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: Compare with Traditional RAG\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üÜö TRADITIONAL RAG vs GRAPHRAG COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def traditional_rag_simulation(query):\n",
    "    \"\"\"Simulate what traditional RAG would return\"\"\"\n",
    "    print(f\"\\nüì∞ Traditional RAG for: '{query}'\")\n",
    "    print(\"Would return these isolated chunks:\")\n",
    "    \n",
    "    # Simple keyword search in documents\n",
    "    query_words = query.lower().split()\n",
    "    chunks = []\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        sentences = text.split('.')\n",
    "        for sentence in sentences:\n",
    "            if any(word in sentence.lower() for word in query_words):\n",
    "                chunks.append(sentence.strip()[:100] + \"...\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[:3], 1):\n",
    "        print(f\"  {i}. {chunk}\")\n",
    "    \n",
    "    print(\"‚ùå Issues: Isolated facts, no relationships, missing context\")\n",
    "\n",
    "# Compare both approaches\n",
    "test_query = \"PharmaCorp executives\"\n",
    "\n",
    "traditional_rag_simulation(test_query)\n",
    "query_graph(test_query)\n",
    "\n",
    "print(f\"\\n‚úÖ GraphRAG Benefits:\")\n",
    "print(\"  ‚Ä¢ Shows entity relationships and connections\")\n",
    "print(\"  ‚Ä¢ Reveals patterns across multiple documents\") \n",
    "print(\"  ‚Ä¢ Provides network context and supporting evidence\")\n",
    "print(\"  ‚Ä¢ Enables multi-hop reasoning\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: Graph Statistics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìä GRAPH STATISTICS\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total entities: {len(graph.nodes)}\")\n",
    "print(f\"Total relationships: {len(graph.edges)}\")\n",
    "print(f\"Graph density: {nx.density(graph):.3f}\")\n",
    "print(f\"Connected components: {nx.number_connected_components(graph)}\")\n",
    "\n",
    "# Most connected entities\n",
    "degree_centrality = nx.degree_centrality(graph)\n",
    "top_central = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(f\"\\nMost connected entities:\")\n",
    "for node, centrality in top_central:\n",
    "    name = graph.nodes[node]['name']\n",
    "    node_type = graph.nodes[node]['type']\n",
    "    print(f\"  ‚Ä¢ {name} ({node_type}): {centrality:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Try your own queries by calling: query_graph('your question here')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berserk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
