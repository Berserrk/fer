{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH TRAVERSAL ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFS: [1, 2, 3, 4, 5]\n",
      "DFS: [1, 2, 4, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "# BREAD FIRST SEARCH\n",
    "\n",
    "# Create a simple graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 5)])\n",
    "'''\n",
    "  1\n",
    " / \\\n",
    "2   3\n",
    "|   |\n",
    "4   5\n",
    "'''\n",
    "\n",
    "# BFS from node 1\n",
    "# BFS visits nodes level by level.\n",
    "# It starts at the root (node 1), then visits all neighbors before going deeper\n",
    "bfs_nodes = list(nx.bfs_tree(G, source=1))\n",
    "print(\"BFS:\", bfs_nodes)\n",
    "\n",
    "# DEPTH FIRST SEARCH\n",
    "# DFS from node 1\n",
    "# DFS goes as deep as possible along each branch before backtracking.\n",
    "# It starts at node 1 and explores one neighbor fully before moving to another.\n",
    "dfs_nodes = list(nx.dfs_tree(G, source=1))\n",
    "print(\"DFS:\", dfs_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINK PREDICTION ALGORITHMS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gnodes ['Alice', 'Bob', 'Claire', 'Dennis', 'Eva', 'Frank']\n",
      "pairs [('Alice', 'Dennis'), ('Alice', 'Eva'), ('Alice', 'Frank')]\n",
      "scores <generator object _apply_prediction.<locals>.<genexpr> at 0x1105e2240>\n",
      "Friend suggestions for Alice:\n",
      "  ‚Ä¢ Dennis  (shared friends = 2.2)\n",
      "  ‚Ä¢ Eva  (shared friends = 1.4)\n",
      "  ‚Ä¢ Frank  (shared friends = 0.3999999999999999)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1Ô∏è Build a toy social graph -----------------------------------\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([\n",
    "    (\"Alice\", \"Bob\"), (\"Alice\", \"Claire\"), (\"Bob\", \"Dennis\"),\n",
    "    (\"Claire\", \"Dennis\"), (\"Claire\", \"Eva\"), (\"Dennis\", \"Frank\"),\n",
    "    (\"Eva\", \"Frank\"),  # no edge yet between Alice and Dennis, etc.\n",
    "])\n",
    "\n",
    "# 2Ô∏è Pick the target user for recommendations -------------------\n",
    "target = \"Alice\"\n",
    "print(\"Gnodes\",G.nodes)\n",
    "# Pairs to score: (target, other) where no edge exists yet\n",
    "pairs = [(target, v) for v in G.nodes\n",
    "         if v != target and not G.has_edge(target, v)]\n",
    "print(\"pairs\",pairs)\n",
    "# 3Ô∏èCompute common-neighbor scores -----------------------------\n",
    "## takes every node pair you listed in pairs, counts how many friends they have in common inside graph G, and gives you a generator that yields (u, v, count) for each pair.\n",
    "\n",
    "scores = nx.common_neighbor_centrality(G, pairs)\n",
    "print(\"scores\",scores)\n",
    "# 44 Rank and show the top suggestions --------------------------\n",
    "top_k = sorted(scores, key=lambda x: x[2], reverse=True)[:3]\n",
    "# \n",
    "print(f\"Friend suggestions for {target}:\")\n",
    "for u, v, score in top_k:\n",
    "    print(f\"  ‚Ä¢ {v}  (shared friends = {score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAMIC Adar-Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eve    ‚Üî Prof Hub  AA = 2.885\n",
      "Alice  ‚Üî Bob     AA = 2.353\n",
      "Dana   ‚Üî Prof Hub  AA = 1.443\n",
      "Alice  ‚Üî Charlie  AA = 0.910\n",
      "Bob    ‚Üî Charlie  AA = 0.910\n",
      "Alice  ‚Üî Dana    AA = 0.000\n",
      "Bob    ‚Üî Dana    AA = 0.000\n",
      "Charlie ‚Üî Eve     AA = 0.000\n",
      "Dana   ‚Üî Eve     AA = 0.000\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 1Ô∏è  Build a tiny co-authorship graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([\n",
    "    (\"Alice\", \"Prof Hub\"), (\"Bob\", \"Prof Hub\"),         # hub collaborator\n",
    "    (\"Alice\", \"Eve\"), (\"Bob\", \"Eve\"),                   # niche collaborator\n",
    "    (\"Charlie\", \"Prof Hub\"), (\"Charlie\", \"Dana\")        # another cluster\n",
    "])\n",
    "\n",
    "# 2Ô∏è  Candidate pairs with no existing edge\n",
    "pairs = [(u, v) for u in G for v in G\n",
    "         if u < v and not G.has_edge(u, v)]\n",
    "\n",
    "# 3Ô∏è  Compute Adamic‚ÄìAdar\n",
    "aa = nx.adamic_adar_index(G, pairs)\n",
    "\n",
    "# 4Ô∏è  Show ranked suggestions\n",
    "for u, v, score in sorted(aa, key=lambda t: t[2], reverse=True):\n",
    "    print(f\"{u:6} ‚Üî {v:6}  AA = {score:.3f}\")\n",
    "\n",
    "'''\n",
    "\n",
    "Eve    ‚Üî Prof Hub  AA = 2.885 is the strongest candidate\n",
    "Although ‚ÄúProf Hub‚Äù has many connections, Eve is low-degree and therefore rare in the network; their shared neighbours are mostly Eve‚Äôs other collaborators, so the pair still scores highly.\n",
    " If you want to suppress hubs even further, try Resource Allocation or apply a post-filter that discards any pair involving nodes above a chosen degree threshold.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resource Allocation Index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top RA scores (higher ‚áí stronger recommendation):\n",
      "Eve     ‚Üî Prof Hub   RA = 1.000\n",
      "Alice   ‚Üî Bob        RA = 0.833\n",
      "Dana    ‚Üî Prof Hub   RA = 0.500\n",
      "Alice   ‚Üî Charlie    RA = 0.333\n",
      "Bob     ‚Üî Charlie    RA = 0.333\n",
      "Alice   ‚Üî Dana       RA = 0.000\n",
      "Bob     ‚Üî Dana       RA = 0.000\n",
      "Charlie ‚Üî Eve        RA = 0.000\n",
      "Dana    ‚Üî Eve        RA = 0.000\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 1Ô∏è‚É£  Build a toy co-authorship graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([\n",
    "    (\"Alice\",   \"Prof Hub\"),\n",
    "    (\"Bob\",     \"Prof Hub\"),\n",
    "    (\"Charlie\", \"Prof Hub\"),\n",
    "    (\"Alice\",   \"Eve\"),\n",
    "    (\"Bob\",     \"Eve\"),\n",
    "    (\"Charlie\", \"Dana\"),\n",
    "])\n",
    "\n",
    "# 2Ô∏è‚É£  Candidate pairs = no existing edge\n",
    "pairs = [(u, v) for u in G for v in G\n",
    "         if u < v and not G.has_edge(u, v)]\n",
    "\n",
    "# 3Ô∏è‚É£  Compute Resource Allocation Index\n",
    "ra = nx.resource_allocation_index(G, pairs)\n",
    "\n",
    "# 4Ô∏è‚É£  Rank and display top suggestions\n",
    "print(\"Top RA scores (higher ‚áí stronger recommendation):\")\n",
    "for u, v, score in sorted(ra, key=lambda t: t[2], reverse=True):\n",
    "    print(f\"{u:7} ‚Üî {v:9}  RA = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preferential attachement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs ranked by Preferential Attachment (highest first)\n",
      "Eve     ‚Üî Prof Hub   PA = 6\n",
      "Alice   ‚Üî Bob        PA = 4\n",
      "Alice   ‚Üî Charlie    PA = 4\n",
      "Bob     ‚Üî Charlie    PA = 4\n",
      "Charlie ‚Üî Eve        PA = 4\n",
      "Dana    ‚Üî Prof Hub   PA = 3\n",
      "Alice   ‚Üî Dana       PA = 2\n",
      "Bob     ‚Üî Dana       PA = 2\n",
      "Dana    ‚Üî Eve        PA = 2\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# -- same graph as before ------------------------------------\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([\n",
    "    (\"Alice\",   \"Prof Hub\"),\n",
    "    (\"Bob\",     \"Prof Hub\"),\n",
    "    (\"Charlie\", \"Prof Hub\"),\n",
    "    (\"Alice\",   \"Eve\"),\n",
    "    (\"Bob\",     \"Eve\"),\n",
    "    (\"Charlie\", \"Dana\")\n",
    "])\n",
    "\n",
    "# candidate pairs without an existing edge\n",
    "pairs = [(u, v) for u in G for v in G\n",
    "         if u < v and not G.has_edge(u, v)]\n",
    "\n",
    "# preferential-attachment generator\n",
    "pa = nx.preferential_attachment(G, pairs)   # yields (u, v, score)\n",
    "\n",
    "# sort & show\n",
    "print(\"Pairs ranked by Preferential Attachment (highest first)\")\n",
    "for u, v, score in sorted(pa, key=lambda t: t[2], reverse=True):\n",
    "    print(f\"{u:<7} ‚Üî {v:<9}  PA = {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SALTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "# --- build the same graph -----------------------------------\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([\n",
    "    (\"Alice\",   \"Prof Hub\"),\n",
    "    (\"Bob\",     \"Prof Hub\"),\n",
    "    (\"Charlie\", \"Prof Hub\"),\n",
    "    (\"Alice\",   \"Eve\"),\n",
    "    (\"Bob\",     \"Eve\"),\n",
    "    (\"Charlie\", \"Dana\")\n",
    "])\n",
    "\n",
    "# candidate pairs with no current edge\n",
    "pairs = [(u, v) for u in G for v in G\n",
    "         if u < v and not G.has_edge(u, v)]\n",
    "\n",
    "# --- Salton / Cosine index generator ------------------------\n",
    "def salton_index(G, ebunch):\n",
    "    for u, v in ebunch:\n",
    "        cn = len(set(G[u]) & set(G[v]))\n",
    "        denom = math.sqrt(G.degree(u) * G.degree(v))\n",
    "        yield (u, v, 0 if denom == 0 else cn / denom)\n",
    "\n",
    "# rank and display\n",
    "print(\"Pairs ranked by Salton / Cosine (highest first)\")\n",
    "for u, v, score in sorted(salton_index(G, pairs),\n",
    "                          key=lambda t: t[2], reverse=True):\n",
    "    print(f\"{u:<7} ‚Üî {v:<9}  Salton = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Salton / cosine index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 1Ô∏è‚É£  Toy membership data  -----------------------------------\n",
    "subs = {\n",
    "    \"r/Python\":   {\"alice\", \"bob\", \"claire\", \"dennis\"},\n",
    "    \"r/DataSci\":  {\"alice\", \"bob\", \"eva\", \"frank\"},\n",
    "    \"r/AI\":       {\"alice\", \"bob\", \"claire\", \"eva\", \"frank\", \"gina\"},\n",
    "    \"r/Funny\":    {\"alice\", \"bob\", \"claire\", \"dennis\", \"eva\",\n",
    "                   \"frank\", \"gina\", \"henry\", \"ida\", \"john\"},\n",
    "}\n",
    "\n",
    "# 2Ô∏è‚É£  Project down to a graph whose edge weight is SD --------\n",
    "G = nx.Graph()\n",
    "for a, users_a in subs.items():\n",
    "    for b, users_b in subs.items():\n",
    "        if a < b:                          # one direction only\n",
    "            overlap = len(users_a & users_b)\n",
    "            sd = 2 * overlap / (len(users_a) + len(users_b))\n",
    "            if sd > 0:                     # keep only pairs with some overlap\n",
    "                G.add_edge(a, b, weight=sd)\n",
    "\n",
    "# 3Ô∏è‚É£  Recommend similar communities for r/Python -------------\n",
    "target = \"r/Python\"\n",
    "recommend = sorted(\n",
    "    ((nbr, G[target][nbr][\"weight\"]) for nbr in G.neighbors(target)),\n",
    "    key=lambda t: t[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"Suggested subreddits for r/Python\")\n",
    "for sub, score in recommend:\n",
    "    print(f\"  ‚Ä¢ {sub:<10}  (Sorensen‚ÄìDice = {score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üíº Compliance Project: Suspicious Transaction Prediction\n",
    "\n",
    "##### üó∫Ô∏è **Context**\n",
    "- **Graph**: Transaction network\n",
    "  - **Nodes** = Bank accounts or customers\n",
    "  - **Edges** = Money transfers\n",
    "\n",
    "##### üéØ **Goal**\n",
    "Predict **suspicious or hidden links** (potential illicit transactions).\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚ö° **Why not only classic ML?**\n",
    "- Flat tables miss **graph structure** (e.g., indirect paths).\n",
    "- Need to capture hidden connections and network behavior.\n",
    "\n",
    "---\n",
    "\n",
    "##### üîó **Graph approach: node2vec**\n",
    "\n",
    "###### ‚úÖ **What it does**\n",
    "1. Runs random walks to explore graph neighborhoods.\n",
    "2. Learns a **vector (embedding)** for each account.\n",
    "3. Similar graph contexts ‚Üí similar vectors.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚öñÔ∏è **How to use embeddings**\n",
    "\n",
    "###### ‚ûï **Combine with features**\n",
    "- Transaction amounts\n",
    "- KYC scores\n",
    "- Transfer frequency\n",
    "\n",
    "###### ‚öôÔ∏è **Train ML model**\n",
    "- Input: [tabular features + embeddings]\n",
    "- Target: suspicious or normal label\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ **Benefits**\n",
    "- Finds hidden risky accounts with no direct links.\n",
    "- Highlights possible collusion paths.\n",
    "- Helps compliance teams proactively investigate.\n",
    "\n",
    "---\n",
    "\n",
    "##### üí¨ **Summary**\n",
    "> *We build a transaction graph, generate node2vec embeddings to capture hidden relationships, and combine them with classical features to train a model for suspicious link prediction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Banking usecase : node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting node2vec\n",
      "  Using cached node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
      "Collecting gensim<5.0.0,>=4.3.0 (from node2vec)\n",
      "  Using cached gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting joblib<2.0.0,>=1.4.0 (from node2vec)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /Users/firaterman/Documents/fer/research/.venv/lib/python3.12/site-packages (from node2vec) (3.5)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /Users/firaterman/Documents/fer/research/.venv/lib/python3.12/site-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/firaterman/Documents/fer/research/.venv/lib/python3.12/site-packages (from node2vec) (4.67.1)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.0->node2vec)\n",
      "  Using cached scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim<5.0.0,>=4.3.0->node2vec)\n",
      "  Downloading smart_open-7.3.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Using cached node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
      "Using cached gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl (24.0 MB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "Downloading smart_open-7.3.0-py3-none-any.whl (61 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, scipy, joblib, smart-open, gensim, node2vec\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6/6\u001b[0m [node2vec]4/6\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.3.3 joblib-1.5.1 node2vec-0.5.0 scipy-1.13.1 smart-open-7.3.0 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "! pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/firaterman/Documents/fer/research/.venv/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/firaterman/Documents/fer/research/.venv/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/firaterman/Documents/fer/research/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-macosx_12_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.7.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Transactions table:\n",
      "  from_account to_account  amount\n",
      "0            A          B     500\n",
      "1            A          C     200\n",
      "2            B          D     700\n",
      "3            C          D     300 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 16400.02it/s]\n",
      "Generating walks (CPU: 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 54050.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Embeddings table:\n",
      "  account     emb_1     emb_2     emb_3     emb_4\n",
      "0       A -0.113415  0.163851 -0.121504 -0.045400\n",
      "1       B -0.232574 -0.177920  0.161472  0.224325\n",
      "2       C -0.125386 -0.094084  0.184513 -0.038337\n",
      "3       D -0.013406  0.005911  0.127584  0.225232 \n",
      "\n",
      "üìÑ Features table before embeddings:\n",
      "  account  avg_amount_out  num_transfers  risk_score\n",
      "0       A           350.0            2.0         0.2\n",
      "1       B           700.0            1.0         0.3\n",
      "2       C           300.0            1.0         0.3\n",
      "3       D             0.0            0.0         0.9 \n",
      "\n",
      "üßæ Full table with features + embeddings:\n",
      "  account  avg_amount_out  num_transfers  risk_score     emb_1     emb_2  \\\n",
      "0       A           350.0            2.0         0.2 -0.113415  0.163851   \n",
      "1       B           700.0            1.0         0.3 -0.232574 -0.177920   \n",
      "2       C           300.0            1.0         0.3 -0.125386 -0.094084   \n",
      "3       D             0.0            0.0         0.9 -0.013406  0.005911   \n",
      "\n",
      "      emb_3     emb_4  \n",
      "0 -0.121504 -0.045400  \n",
      "1  0.161472  0.224325  \n",
      "2  0.184513 -0.038337  \n",
      "3  0.127584  0.225232   \n",
      "\n",
      "üè∑Ô∏è Table with labels:\n",
      "  account  avg_amount_out  num_transfers  risk_score     emb_1     emb_2  \\\n",
      "0       A           350.0            2.0         0.2 -0.113415  0.163851   \n",
      "1       B           700.0            1.0         0.3 -0.232574 -0.177920   \n",
      "2       C           300.0            1.0         0.3 -0.125386 -0.094084   \n",
      "3       D             0.0            0.0         0.9 -0.013406  0.005911   \n",
      "\n",
      "      emb_3     emb_4  label  \n",
      "0 -0.121504 -0.045400      0  \n",
      "1  0.161472  0.224325      0  \n",
      "2  0.184513 -0.038337      0  \n",
      "3  0.127584  0.225232      1   \n",
      "\n",
      "‚úÖ Final results with suspicion scores:\n",
      "  account  suspicion_score\n",
      "0       A             0.08\n",
      "1       B             0.06\n",
      "2       C             0.02\n",
      "3       D             0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 1: Create transactions data\n",
    "# ---------------------------------\n",
    "df = pd.DataFrame({\n",
    "    'from_account': [\"A\", \"A\", \"B\", \"C\"],\n",
    "    'to_account':   [\"B\", \"C\", \"D\", \"D\"],\n",
    "    'amount': [500, 200, 700, 300]\n",
    "})\n",
    "\n",
    "print(\"üîé Transactions table:\")\n",
    "print(df, '\\n')\n",
    "\n",
    "# Collect all unique accounts\n",
    "all_accounts = pd.DataFrame({'account': pd.unique(df[['from_account', 'to_account']].values.ravel())})\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 2: Build transaction graph\n",
    "# ---------------------------------\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(zip(df['from_account'], df['to_account']))\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 3: Generate node2vec embeddings\n",
    "# ---------------------------------\n",
    "'''\n",
    "‚úîÔ∏è num_walks controls how many ‚Äúsentences‚Äù each node contributes.\t‚Ä¢\tThe number of walks you start from each node.If you set 10, each node generates 10 random walk sequences.\n",
    "‚úîÔ∏è walk_length controls how long each ‚Äúsentence‚Äù is.\n",
    "‚úîÔ∏è dimensions controls how much ‚Äúspace‚Äù each node has to encode its context.\n",
    "\n",
    "Example: num_walks = 2, walk_length = 3 \n",
    "Walk 1: A ‚Üí B ‚Üí D ‚Üí C\n",
    "Walk 2: A ‚Üí C ‚Üí D ‚Üí B\n",
    "if more walk length than node then go back \n",
    "\t‚Ä¢\tLow num_walks (e.g., 2): less context variety, fewer samples.\n",
    "\t‚Ä¢\tHigh num_walks (e.g., 10 or 20): more robust learning, better node context representation.\n",
    "'''\n",
    "node2vec = Node2Vec(G, dimensions=4, walk_length=5, num_walks=10, workers=1, seed=42)\n",
    "model = node2vec.fit(window=3, min_count=1)\n",
    "\n",
    "embeddings = pd.DataFrame(\n",
    "    [model.wv[str(node)] for node in G.nodes()],\n",
    "    index=G.nodes()\n",
    ")\n",
    "embeddings.columns = [f'emb_{i+1}' for i in range(embeddings.shape[1])]\n",
    "embeddings.reset_index(inplace=True)\n",
    "embeddings.rename(columns={'index': 'account'}, inplace=True)\n",
    "\n",
    "print(\"üß© Embeddings table:\")\n",
    "print(embeddings, '\\n')\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 4: Compute features\n",
    "# ---------------------------------\n",
    "avg_amount = df.groupby('from_account')['amount'].mean().reset_index()\n",
    "avg_amount.rename(columns={'from_account': 'account', 'amount': 'avg_amount_out'}, inplace=True)\n",
    "\n",
    "num_tx = df.groupby('from_account').size().reset_index(name='num_transfers')\n",
    "num_tx.rename(columns={'from_account': 'account'}, inplace=True)\n",
    "\n",
    "# Merge to all accounts to avoid dropping receivers\n",
    "features_df = all_accounts.merge(avg_amount, on='account', how='left')\n",
    "features_df = features_df.merge(num_tx, on='account', how='left')\n",
    "\n",
    "# Fill missing values (accounts with only incoming transfers)\n",
    "features_df['avg_amount_out'] = features_df['avg_amount_out'].fillna(0)\n",
    "features_df['num_transfers'] = features_df['num_transfers'].fillna(0)\n",
    "\n",
    "# Dummy risk score\n",
    "features_df['risk_score'] = [0.2, 0.3, 0.3, 0.9]  # Example for A, B, C, D\n",
    "\n",
    "print(\"üìÑ Features table before embeddings:\")\n",
    "print(features_df, '\\n')\n",
    "\n",
    "# Merge embeddings\n",
    "full_df = features_df.merge(embeddings, on='account', how='left')\n",
    "\n",
    "print(\"üßæ Full table with features + embeddings:\")\n",
    "print(full_df, '\\n')\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 5: Add labels\n",
    "# ---------------------------------\n",
    "labels = pd.DataFrame({\n",
    "    'account': ['A', 'B', 'C', 'D'],\n",
    "    'label': [0, 0, 0, 1]  # D marked as suspicious\n",
    "})\n",
    "final_df = full_df.merge(labels, on='account', how='left')\n",
    "\n",
    "print(\"üè∑Ô∏è Table with labels:\")\n",
    "print(final_df, '\\n')\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 6: Prepare for ML\n",
    "# ---------------------------------\n",
    "X = final_df.drop(columns=['account', 'label'])\n",
    "y = final_df['label']\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict suspicion scores\n",
    "pred_probs = model.predict_proba(X)[:, 1]\n",
    "final_df['suspicion_score'] = pred_probs\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 7: Output final results\n",
    "# ---------------------------------\n",
    "print(\"‚úÖ Final results with suspicion scores:\")\n",
    "print(final_df[['account', 'suspicion_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
