- original document is a pdf, a full text court opinion, stored as plain text or PDFs in legal databases.
- Human-annotated posture labels : manually by expert: and inter-annotator agreement was measured using Kappa.
- Sections created by rule-based parsing or NLP segmentation: nlp or rule-based techniques used (regex patterns, heuristics, document parsers) in order to: 
    - identify section headings, split text into parafraphs, capture each section's paragraphs as lists.
    	‚Ä¢	Regex or heuristic-based text segmentation: to find section titles and divide paragraphs.
	‚Ä¢	Possibly sentence or paragraph tokenization using NLP libraries (e.g., spaCy, NLTK).

- Packaged into structured JSON for easy downstream ML use.
- Not originally generated by LLMs, but LLMs can assist downstream (e.g., summarization, feature extraction).



üí° Data quality and reliability
	‚Ä¢	Labels are high-quality because they‚Äôre human-annotated ‚Üí good training signal.
	‚Ä¢	However, some label noise still exists (hence the use of Kappa to report agreement).


‚Ä¢	Good candidates for legal-domain pretrained models (e.g., LegalBERT).


üü† Modeling perspective
	‚Ä¢	You are building a multi-class (or potentially multi-label) classification model mapping text to procedural posture.
	‚Ä¢	Document structure (sections, paragraphs) can be incorporated as hierarchical input.







‚úÖ What does ‚Äúmanually annotated‚Äù mean?
	‚Ä¢	It means humans, not algorithms or models, did this labeling.
	‚Ä¢	Each document was carefully reviewed to ensure the correct procedural posture was assigned.

‚∏ª

‚úÖ Why is this done?
	‚Ä¢	To create a gold standard dataset for machine learning.
	‚Ä¢	Labels tell a future model: ‚ÄúWhen you see this type of text, it means this procedural posture.‚Äù

‚∏ª

‚úÖ How do we know the labels are reliable?
	‚Ä¢	Because multiple annotators label some documents.
	‚Ä¢	Their agreement is measured using Kappa, which checks how consistently humans assign the same label (adjusting for chance).

‚∏ª

‚≠ê Summary

Humans (legal experts) read court decisions and assign a procedural posture label, describing how the case reached this court. This is manual work to ensure high-quality, accurate labels, and their consistency is checked using Kappa.


üü† Practical takeaways for modeling
	‚Ä¢	High Kappa labels (e.g., > 0.8): You can trust these as strong signals; your model should learn them well.
	‚Ä¢	Medium Kappa labels (e.g., ~0.6‚Äì0.8): Expect some noise; consider using robust loss functions or incorporating uncertainty.
	‚Ä¢	Low Kappa labels: May require extra strategies (e.g., review of borderline cases, label smoothing).

The Kappa table tells you how reliable each label is, as measured by how much human annotators agreed on it. High values (near 1) mean almost perfect agreement (clean data). Lower values signal potential ambiguity and help you understand possible model performance limits.

‚∏ª


üí° Why is Kappa reported?
	‚Ä¢	To quantify agreement among the annotators before they finalize one label.
	‚Ä¢	High Kappa means annotators mostly gave the same label ‚Üí confident final label.
	‚Ä¢	Low Kappa means annotators disagreed more ‚Üí more subjective or ambiguous cases.

‚∏ª

‚ùì Why aren‚Äôt annotator votes in your JSON?

The dataset was cleaned and finalized to make it easy for modeling.
	‚Ä¢	You only get the final ‚Äúground truth‚Äù label (the single posture label).
	‚Ä¢	The individual annotator votes and intermediate steps are not included.



‚öñÔ∏è Why does this matter for you?

When you train a model:
	‚Ä¢	You only use the final single label ("postures": ["On Appeal"]).
	‚Ä¢	You can rely on Kappa values to understand how reliable this label is, but you don‚Äôt see the raw votes.