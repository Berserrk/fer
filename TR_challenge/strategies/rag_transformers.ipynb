{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 331936506 characters\n",
      "\n",
      "Successfully parsed 18000 JSON objects\n",
      "Total documents for training (with postures): 17077\n",
      "Number of unique postures (classes): 224\n",
      "Shape of label matrix (documents x classes): (17077, 224)\n",
      "\n",
      "Train set size: 11962\n",
      "Validation set size: 2558\n",
      "Test set size: 2557\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, jaccard_score\n",
    "\n",
    "# --- 1. Load and Parse Data ---\n",
    "\n",
    "\n",
    "# Read the txt file\n",
    "with open('../data/TRDataChallenge2023.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(f\"File size: {len(content)} characters\")\n",
    "\n",
    "# Parse JSON dictionaries from the file\n",
    "data = []\n",
    "lines = content.strip().split('\\n')\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip():\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing line {i+1}: {e}\")\n",
    "            print(f\"Line content: {line[:100]}...\")\n",
    "\n",
    "print(f\"\\nSuccessfully parsed {len(data)} JSON objects\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data) # In your case, df = pd.DataFrame(data)\n",
    "\n",
    "# --- 2. Feature Extraction: Combine headtext and paragraphs ---\n",
    "def combine_sections_text(sections):\n",
    "    full_text = []\n",
    "    for section in sections:\n",
    "        # Prioritize headtext, then paragraphs. Join paragraphs with spaces.\n",
    "        section_text = []\n",
    "        if section.get('headtext'):\n",
    "            section_text.append(section['headtext'])\n",
    "        if section.get('paragraphs'):\n",
    "            section_text.extend(section['paragraphs'])\n",
    "        \n",
    "        # Join paragraphs within a section with a space, then sections with a newline\n",
    "        full_text.append(\" \".join(section_text))\n",
    "    return \"\\n\\n\".join(full_text)\n",
    "\n",
    "df['full_text'] = df['sections'].apply(combine_sections_text)\n",
    "\n",
    "# --- 3. Handle Documents with 0 Postures ---\n",
    "# Documents with empty 'postures' lists cannot be used for supervised training.\n",
    "# Filter them out before label binarization and splitting.\n",
    "df_trainable = df[df['postures'].apply(lambda x: len(x) > 0)].copy()\n",
    "\n",
    "# --- 4. Label Binarization (Multi-Hot Encoding) ---\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels_encoded = mlb.fit_transform(df_trainable['postures'])\n",
    "labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_, index=df_trainable.index)\n",
    "\n",
    "# Define NUM_CLASSES here\n",
    "NUM_CLASSES = len(mlb.classes_) # Add this line\n",
    "\n",
    "# Merge texts with their encoded labels\n",
    "X = df_trainable['full_text'].values\n",
    "y = labels_df.values\n",
    "\n",
    "print(f\"Total documents for training (with postures): {len(X)}\")\n",
    "print(f\"Number of unique postures (classes): {len(mlb.classes_)}\")\n",
    "print(f\"Shape of label matrix (documents x classes): {y.shape}\")\n",
    "\n",
    "# --- 5. Train-Validation-Test Split with Iterative Stratification ---\n",
    "# Iterative stratification is crucial for multi-label datasets to preserve label distributions.\n",
    "# This ensures that each split (train, val, test) contains a representative distribution of labels,\n",
    "# especially important for rare labels.\n",
    "\n",
    "# First, split into train and temp (validation + test)\n",
    "X_train, y_train, X_temp, y_temp = iterative_train_test_split(X.reshape(-1, 1), y, test_size=0.3) # 70% train, 30% temp\n",
    "X_train = X_train.flatten() # Flatten X_train back to 1D array\n",
    "\n",
    "# Then, split temp into validation and test\n",
    "X_val, y_val, X_test, y_test = iterative_train_test_split(X_temp.reshape(-1, 1), y_temp, test_size=0.5) # 15% val, 15% test\n",
    "X_val = X_val.flatten()\n",
    "X_test = X_test.flatten()\n",
    "\n",
    "print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# --- Common Evaluation Function ---\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None, threshold=0.5, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluates multi-label classification model performance.\n",
    "    y_pred should be binary predictions (0 or 1).\n",
    "    y_pred_proba should be probabilities (if available) for thresholding.\n",
    "    \"\"\"\n",
    "    if y_pred_proba is not None:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy (Exact Match Ratio)\": accuracy_score(y_true, y_pred),\n",
    "        \"Hamming Loss\": hamming_loss(y_true, y_pred),\n",
    "        \"Jaccard Score (Micro)\": jaccard_score(y_true, y_pred, average='micro'),\n",
    "        \"Jaccard Score (Macro)\": jaccard_score(y_true, y_pred, average='macro'),\n",
    "        \"F1 Score (Micro)\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"F1 Score (Macro)\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"F1 Score (Weighted)\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"Precision (Micro)\": precision_score(y_true, y_pred, average='micro'),\n",
    "        \"Recall (Micro)\": recall_score(y_true, y_pred, average='micro'),\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "    # Optional: Per-class metrics (useful for understanding imbalance)\n",
    "    if class_names and y_true.shape[1] < 50: # Avoid printing too many classes\n",
    "        print(\"\\n--- Per-Class F1 Score ---\")\n",
    "        f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            print(f\"  {class_name}: {f1_per_class[i]:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transformer-based Model: DistilBERT ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cbce59b0cb4960bfd2c4a8967fd856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd9c7cd927c48df84df4a780493b4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175897e4b45d40dd83e89e96200967a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'NUM_CLASSES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m test_dataset_hf\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# --- Model Initialization ---\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# For multi-label classification, set problem_type=\"multi_label_classification\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# This automatically configures the final layer with sigmoid activation and BCE loss.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m model_hf \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, num_labels\u001b[38;5;241m=\u001b[39m\u001b[43mNUM_CLASSES\u001b[49m, problem_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# --- Custom Compute Metrics Function ---\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# The Trainer needs a function to compute metrics during evaluation.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# We'll use our `evaluate_model` function for consistency.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_CLASSES' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import evaluate # Hugging Face evaluate library\n",
    "\n",
    "print(\"\\n--- Transformer-based Model: DistilBERT ---\")\n",
    "\n",
    "# Model name (DistilBERT is a good lightweight option for demonstration)\n",
    "# For long documents, consider 'allenai/longformer-base-4096' or 'google/bigbird-roberta-base'\n",
    "MODEL_NAME = \"distilbert/distilbert-base-uncased\" # or 'bert-base-uncased', 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- Tokenization Function ---\n",
    "# max_length is the crucial parameter for handling document length.\n",
    "# For standard BERT/DistilBERT, max_length is usually 512.\n",
    "# For Longformer, it can be 4096.\n",
    "# If documents are longer than max_length, 'truncation=True' will cut them off.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512) # Adjust max_length\n",
    "\n",
    "# Prepare the data for Hugging Face datasets format\n",
    "train_df = pd.DataFrame({'text': X_train.tolist(), 'labels': y_train.tolist()})\n",
    "val_df = pd.DataFrame({'text': X_val.tolist(), 'labels': y_val.tolist()})\n",
    "test_df = pd.DataFrame({'text': X_test.tolist(), 'labels': y_test.tolist()})\n",
    "\n",
    "train_dataset_hf = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
    "val_dataset_hf = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)\n",
    "test_dataset_hf = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# Ensure labels are in the correct format (list of floats for multi-label)\n",
    "train_dataset_hf.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset_hf.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset_hf.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# --- Model Initialization ---\n",
    "# For multi-label classification, set problem_type=\"multi_label_classification\"\n",
    "# This automatically configures the final layer with sigmoid activation and BCE loss.\n",
    "model_hf = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_CLASSES, problem_type=\"multi_label_classification\")\n",
    "\n",
    "# --- Custom Compute Metrics Function ---\n",
    "# The Trainer needs a function to compute metrics during evaluation.\n",
    "# We'll use our `evaluate_model` function for consistency.\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.from_numpy(logits)).numpy() # Apply sigmoid to logits\n",
    "    \n",
    "    # Calculate pos_weight for F1 score, etc., as it's typically for loss, but\n",
    "    # some metrics libraries might use it if averaged. Here, we pass it to our func\n",
    "    # Note: Hugging Face Trainer doesn't directly use pos_weight for metrics,\n",
    "    # but BCEWithLogitsLoss will use it for training loss.\n",
    "    \n",
    "    # Threshold predictions to binary (0 or 1)\n",
    "    threshold = 0.5 # Can be tuned\n",
    "    y_pred_binary = (predictions >= threshold).astype(int)\n",
    "\n",
    "    # Use the evaluate_model function\n",
    "    metrics_results = evaluate_model(labels, y_pred_binary, y_pred_proba=predictions)\n",
    "    \n",
    "    # Trainer expects a dictionary of metric_name: value\n",
    "    # We'll return micro F1, Hamming Loss, and Micro Jaccard for simplicity for the Trainer.\n",
    "    return {\n",
    "        \"f1_micro\": metrics_results[\"F1 Score (Micro)\"],\n",
    "        \"hamming_loss\": metrics_results[\"Hamming Loss\"],\n",
    "        \"jaccard_micro\": metrics_results[\"Jaccard Score (Micro)\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Training Arguments ---\n",
    "# Set up training arguments. Adjust per your computational resources.\n",
    "# For demonstration, small values.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3, # Reduce for faster demo, increase for performance\n",
    "    per_device_train_batch_size=8, # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, # Loads the best model based on eval_loss\n",
    "    metric_for_best_model=\"loss\", # Or 'f1_micro' if you define it in compute_metrics\n",
    "    greater_is_better=False, # For loss, smaller is better\n",
    "    learning_rate=2e-5, # Common for fine-tuning Transformers\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if GPU is available\n",
    ")\n",
    "\n",
    "# --- Trainer Initialization and Training ---\n",
    "trainer = Trainer(\n",
    "    model=model_hf,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_hf,\n",
    "    eval_dataset=val_dataset_hf,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer, # Pass tokenizer for padding/truncation during training\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# --- Evaluation on Test Set ---\n",
    "print(\"\\nResults for DistilBERT on Test Set:\")\n",
    "predictions_output = trainer.predict(test_dataset_hf)\n",
    "y_pred_hf_proba = torch.sigmoid(torch.from_numpy(predictions_output.predictions)).numpy()\n",
    "y_pred_hf = (y_pred_hf_proba >= 0.5).astype(int)\n",
    "\n",
    "evaluate_model(y_test, y_pred_hf, y_pred_proba=y_pred_hf_proba, class_names=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berserk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
