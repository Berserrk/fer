{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 331936506 characters\n"
     ]
    }
   ],
   "source": [
    "# Read the txt file\n",
    "with open('data/TRDataChallenge2023.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(f\"File size: {len(content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 18000 JSON objects\n"
     ]
    }
   ],
   "source": [
    "# Parse JSON dictionaries from the text file\n",
    "data = []\n",
    "lines = content.strip().split('\\n')\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip():\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing line {i+1}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Successfully parsed {len(data)} JSON objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1 : Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA STRUCTURE ANALYSIS\n",
      "Keys in the JSON objects:\n",
      "- documentId\n",
      "- postures\n",
      "- sections\n"
     ]
    }
   ],
   "source": [
    "# 2. Structure analysis\n",
    "print(\"DATA STRUCTURE ANALYSIS\")\n",
    "\n",
    "if data:\n",
    "    # to get an idea of the data:\n",
    "        # print(\"Sample JSON object:\")\n",
    "        # print(json.dumps(data[0], indent=2))\n",
    "    print(\"Keys in the JSON objects:\")\n",
    "    all_keys = set()\n",
    "    for obj in data:\n",
    "        if isinstance(obj, dict):\n",
    "            all_keys.update(obj.keys())\n",
    "    \n",
    "    for key in sorted(all_keys):\n",
    "        print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME CONVERSION\n",
      "DataFrame shape: (18000, 3)\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   documentId  18000 non-null  object\n",
      " 1   postures    18000 non-null  object\n",
      " 2   sections    18000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 422.0+ KB\n",
      "\n",
      "First few rows:\n",
      "                          documentId  \\\n",
      "0  Ib4e590e0a55f11e8a5d58a2c8dcb28b5   \n",
      "1  Ib06ab4d056a011e98c7a8e995225dbf9   \n",
      "2  Iaa3e3390b93111e9ba33b03ae9101fb2   \n",
      "3  I0d4dffc381b711e280719c3f0e80bdd0   \n",
      "4  I82c7ef10d6d111e8aec5b23c3317c9c0   \n",
      "\n",
      "                                            postures  \\\n",
      "0                                        [On Appeal]   \n",
      "1  [Appellate Review, Sentencing or Penalty Phase...   \n",
      "2          [Motion to Compel Arbitration, On Appeal]   \n",
      "3     [On Appeal, Review of Administrative Decision]   \n",
      "4                                        [On Appeal]   \n",
      "\n",
      "                                            sections  \n",
      "0  [{'headtext': '', 'paragraphs': ['Plaintiff Dw...  \n",
      "1  [{'headtext': '', 'paragraphs': ['After pleadi...  \n",
      "2  [{'headtext': '', 'paragraphs': ['Frederick Gr...  \n",
      "3  [{'headtext': '', 'paragraphs': ['Appeal from ...  \n",
      "4  [{'headtext': '', 'paragraphs': ['Order, Supre...  \n"
     ]
    }
   ],
   "source": [
    "# 3. Convert to DataFrame for easier analysis\n",
    "print(\"DATAFRAME CONVERSION\")\n",
    "if data and isinstance(data[0], dict):\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(\"\\nDataFrame info:\")\n",
    "    df.info()\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Data is not in a dictionary format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTIVE STATISTICS\n",
      "Missingvalues:\n",
      "documentId    0\n",
      "postures      0\n",
      "sections      0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "documentId    object\n",
      "postures      object\n",
      "sections      object\n",
      "dtype: object\n",
      "\n",
      "Dataset Overview:\n",
      "- Total documents: 18000\n",
      "- Unique document IDs: 18000\n",
      "\n",
      "Postures Analysis:\n",
      "Documents with empty postre lists: 923\n",
      "Total unique postures: 0\n",
      "Most commons postures:\n",
      "\n",
      "Sections Analysis:\n",
      "Average sections per document: 5.09\n",
      "Minsections: 1\n",
      "Max sections: 91\n",
      "\n",
      "Sample Section Structure:\n",
      "Sction keys: ['headtext', 'paragraphs']\n",
      "Paragrashs in first section: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"Missingvalues:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"- Total documents: {len(df)}\")\n",
    "    print(f\"- Unique document IDs: {df['documentId'].nunique()}\")\n",
    "    \n",
    "    # Analyze postures column\n",
    "    print(f\"\\nPostures Analysis:\")\n",
    "    all_postures = []\n",
    "    num_empty_postures = (df['postures'].apply(lambda x: isinstance(x, list) and len(x) == 0)).sum()\n",
    "    print(f\"Documents with empty postre lists: {num_empty_postures}\")\n",
    "            \n",
    "    posture_counts = Counter(all_postures)\n",
    "    print(f\"Total unique postures: {len(posture_counts)}\")\n",
    "    print(\"Most commons postures:\")\n",
    "    for posture, count in posture_counts.most_common(10):\n",
    "        print(f\"  {posture}: {count}\")\n",
    "    \n",
    "    # Analyze sections structure\n",
    "    print(f\"\\nSections Analysis:\")\n",
    "    section_counts = []\n",
    "    for sections in df['sections']:\n",
    "        if isinstance(sections, list):\n",
    "            section_counts.append(len(sections))\n",
    "    \n",
    "    if section_counts:\n",
    "        print(f\"Average sections per document: {np.mean(section_counts):.2f}\")\n",
    "        print(f\"Minsections: {min(section_counts)}\")\n",
    "        print(f\"Max sections: {max(section_counts)}\")\n",
    "        \n",
    "    # Sample section analysis\n",
    "    print(f\"\\nSample Section Structure:\")\n",
    "    if df['sections'].iloc[0] and isinstance(df['sections'].iloc[0], list):\n",
    "        sample_section = df['sections'].iloc[0][0]\n",
    "        if isinstance(sample_section, dict):\n",
    "            print(f\"Sction keys: {list(sample_section.keys())}\")\n",
    "            if 'paragraphs' in sample_section and isinstance(sample_section['paragraphs'], list):\n",
    "                print(f\"Paragrashs in first section: {len(sample_section['paragraphs'])}\")\n",
    "else:\n",
    "    print(\"DataFrame empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET COMPREHENSIVE COUNTS\n",
      "------------------------------\n",
      "Total Documents: 18,000\n",
      "Total Posture Instances: 27,659\n",
      "Unique Postures: 224\n",
      "Total Paragraphs: 542,169\n",
      "Average Postures per Document: 1.54\n",
      "Average Paragraphs per Document: 30.12\n",
      "Documents with Multiple Postures: 8,959 (49.8%)\n",
      "DOCUMENTS BY NUMBER OF POSTURES:\n",
      "Postures     Documents    Percentage  \n",
      "0 postures   923          5.1        %\n",
      "1 posture    8,118        45.1       %\n",
      "2 postures   7,604        42.2       %\n",
      "3 postures   1,129        6.3        %\n",
      "4 postures   190          1.1        %\n",
      "5 postures   32           0.2        %\n",
      "6 postures   2            0.0        %\n",
      "7 postures   2            0.0        %\n",
      "Single posture documents: 8,118 (45.1%)\n",
      "Multi-posture documents: 8,959 (49.8%)\n",
      "Maximum postures in one document: 7\n",
      "TOP 10 MOST COMMON POSTURES:\n",
      " 1. On Appeal: 9,197 (33.3%)\n",
      " 2. Appellate Review: 4,652 (16.8%)\n",
      " 3. Review of Administrative Decision: 2,773 (10.0%)\n",
      " 4. Motion to Dismiss: 1,679 (6.1%)\n",
      " 5. Sentencing or Penalty Phase Motion or Objection: 1,342 (4.9%)\n",
      " 6. Trial or Guilt Phase Motion or Objection: 1,097 (4.0%)\n",
      " 7. Motion for Attorney's Fees: 612 (2.2%)\n",
      " 8. Post-Trial Hearing Motion: 512 (1.9%)\n",
      " 9. Motion for Preliminary Injunction: 364 (1.3%)\n",
      "10. Motion to Dismiss for Lack of Subject Matter Jurisdiction: 343 (1.2%)\n",
      " Multi-posture documents: 8,959\n",
      "Single-posture  documents: 8,118\n",
      "DISTRIBUTION OF POSTURES IN MULTI-POSTURE DOCUMENTS:\n",
      "  2 postures: 7,604 documents (84.9%)\n",
      "  3 postures: 1,129 documents (12.6%)\n",
      "  4 postures: 190 documents (2.1%)\n",
      "  5 postures: 32 documents (0.4%)\n",
      "  6 postures: 2 documents (0.0%)\n",
      "  7 postures: 2 documents (0.0%)\n",
      "TOP 15 POSTURES MOST INVOLVED IN MULTI-POSTURE DOCUMENTS:  \n",
      "Rank Posture                                  Multi  Single  Total   Multi%  \n",
      "MOST COMMON POSTURE COMBINATIONS:\n",
      " 1. On Appeal + Review of Administrative Decision\n",
      "Count: 1,147 (12.8% of multi-posture docs)\n",
      "\n",
      " 2. Appellate Review + Sentencing or Penalty Phase Motion or Objection\n",
      "Count: 1,098 (12.3% of multi-posture docs)\n",
      "\n",
      " 3. Motion to Dismiss + On Appeal\n",
      "Count: 1,081 (12.1% of multi-posture docs)\n",
      "\n",
      " 4. Appellate Review + Trial or Guilt Phase Motion or Objection\n",
      "Count: 822 (9.2% of multi-posture docs)\n",
      "\n",
      " 5. Appellate Review + Post-Trial Hearing Motion\n",
      "Count: 345 (3.9% of multi-posture docs)\n",
      "\n",
      " 6. Motion for Attorney's Fees + On Appeal\n",
      "Count: 258 (2.9% of multi-posture docs)\n",
      "\n",
      " 7. On Appeal + Petition to Terminate Parental Rights\n",
      "Count: 196 (2.2% of multi-posture docs)\n",
      "\n",
      " 8. Motion to Compel Arbitration + On Appeal\n",
      "Count: 144 (1.6% of multi-posture docs)\n",
      "\n",
      " 9. Motion to Dismiss for Lack of Subject Matter Jurisdiction + On Appeal\n",
      "Count: 141 (1.6% of multi-posture docs)\n",
      "\n",
      "10. Motion for Preliminary Injunction + On Appeal\n",
      "Count: 124 (1.4% of multi-posture docs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Comprehensive counts\n",
    "print(\"DATASET COMPREHENSIVE COUNTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Count total documents\n",
    "total_documents = len(df)\n",
    "print(f\"Total Documents: {total_documents:,}\")\n",
    "\n",
    "# Count unique postures across all documents\n",
    "all_postures = []\n",
    "for posture_list in df['postures']:\n",
    "    if isinstance(posture_list, list):\n",
    "        all_postures.extend(posture_list)\n",
    "\n",
    "unique_postures = set(all_postures)\n",
    "total_posture_instances = len(all_postures)\n",
    "print(f\"Total Posture Instances: {total_posture_instances:,}\")\n",
    "print(f\"Unique Postures: {len(unique_postures):,}\")\n",
    "\n",
    "# Count total paragraphs across all documents\n",
    "total_paragraphs = 0\n",
    "for sections_list in df['sections']:\n",
    "    if isinstance(sections_list, list):\n",
    "        for section in sections_list:\n",
    "            if isinstance(section, dict) and 'paragraphs' in section:\n",
    "                if isinstance(section['paragraphs'], list):\n",
    "                    total_paragraphs += len(section['paragraphs'])\n",
    "\n",
    "print(f\"Total Paragraphs: {total_paragraphs:,}\")\n",
    "\n",
    "avg_postures_per_doc = total_posture_instances / total_documents\n",
    "avg_paragraphs_per_doc = total_paragraphs / total_documents\n",
    "\n",
    "print(f\"Average Postures per Document: {avg_postures_per_doc:.2f}\")\n",
    "print(f\"Average Paragraphs per Document: {avg_paragraphs_per_doc:.2f}\")\n",
    "\n",
    "# Documents with multiple postures\n",
    "multi_posture_docs_count = sum(1 for postures in df['postures'] if isinstance(postures, list) and len(postures) > 1)\n",
    "print(f\"Documents with Multiple Postures: {multi_posture_docs_count:,} ({(multi_posture_docs_count/total_documents)*100:.1f}%)\")\n",
    "\n",
    "# Detailed posture count distribution\n",
    "print(f\"DOCUMENTS BY NUMBER OF POSTURES:\")\n",
    "postures_per_document = []\n",
    "for postures in df['postures']:\n",
    "    if isinstance(postures, list):\n",
    "        postures_per_document.append(len(postures))\n",
    "    else:\n",
    "        postures_per_document.append(0)\n",
    "\n",
    "posture_distribution = Counter(postures_per_document)\n",
    "print(f\"{'Postures':<12} {'Documents':<12} {'Percentage':<12}\")\n",
    "\n",
    "\n",
    "for num_postures in sorted(posture_distribution.keys()):\n",
    "    count = posture_distribution[num_postures]\n",
    "    percentage = (count / total_documents) * 100\n",
    "    posture_label = f\"{num_postures} posture{'s' if num_postures != 1 else ''}\"\n",
    "    print(f\"{posture_label:<12} {count:<12,} {percentage:<11.1f}%\")\n",
    "\n",
    "single_posture_count = posture_distribution.get(1, 0)\n",
    "multi_posture_total = sum(count for postures, count in posture_distribution.items() if postures > 1)\n",
    "print(f\"Single posture documents: {single_posture_count:,} ({(single_posture_count/total_documents)*100:.1f}%)\")\n",
    "print(f\"Multi-posture documents: {multi_posture_total:,} ({(multi_posture_total/total_documents)*100:.1f}%)\")\n",
    "print(f\"Maximum postures in one document: {max(postures_per_document)}\")\n",
    "\n",
    "# Most common postures (top 10)\n",
    "posture_counts = Counter(all_postures)\n",
    "print(f\"TOP 10 MOST COMMON POSTURES:\")\n",
    "for i, (posture, count) in enumerate(posture_counts.most_common(10), 1):\n",
    "    percentage = (count / total_posture_instances) * 100\n",
    "    print(f\"{i:2d}. {posture}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "\n",
    "# Find documents with multiple postures\n",
    "multi_posture_docs = []\n",
    "single_posture_docs = []\n",
    "\n",
    "for idx, postures in enumerate(df['postures']):\n",
    "    if isinstance(postures, list):\n",
    "        if len(postures) > 1:\n",
    "            multi_posture_docs.append({\n",
    "                'index': idx,\n",
    "                'documentId': df.iloc[idx]['documentId'],\n",
    "                'postures': postures,\n",
    "                'num_postures': len(postures)\n",
    "            })\n",
    "        elif len(postures) == 1:\n",
    "            single_posture_docs.append({\n",
    "                'index': idx,\n",
    "                'postures': postures\n",
    "            })\n",
    "\n",
    "print(f\" Multi-posture documents: {len(multi_posture_docs):,}\")\n",
    "print(f\"Single-posture  documents: {len(single_posture_docs):,}\")\n",
    "\n",
    "\n",
    "\n",
    "# Distribution of number of postures per document\n",
    "posture_count_distribution = Counter([doc['num_postures'] for doc in multi_posture_docs])\n",
    "print(f\"DISTRIBUTION OF POSTURES IN MULTI-POSTURE DOCUMENTS:\")\n",
    "for num_postures in sorted(posture_count_distribution.keys()):\n",
    "    count = posture_count_distribution[num_postures]\n",
    "    percentage = (count / len(multi_posture_docs)) * 100\n",
    "    print(f\"  {num_postures} postures: {count:,} documents ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze which postures appear in multi-posture vs single-posture documents\n",
    "postures_in_multi = []\n",
    "postures_in_single = []\n",
    "\n",
    "for doc in multi_posture_docs:\n",
    "    postures_in_multi.extend(doc['postures'])\n",
    "\n",
    "for doc in single_posture_docs:\n",
    "    postures_in_single.extend(doc['postures'])\n",
    "\n",
    "# Count frequencies\n",
    "multi_posture_counts = Counter(postures_in_multi)\n",
    "single_posture_counts = Counter(postures_in_single)\n",
    "\n",
    "# All unique postures\n",
    "all_unique_postures = set(postures_in_multi + postures_in_single)\n",
    "\n",
    "# Calculate statistics for each posture\n",
    "posture_stats = {}\n",
    "for posture in all_unique_postures:\n",
    "    multi_freq = multi_posture_counts.get(posture, 0)\n",
    "    single_freq = single_posture_counts.get(posture, 0)\n",
    "    total_freq = multi_freq + single_freq\n",
    "    \n",
    "    multi_percentage = (multi_freq / total_freq) * 100 if total_freq > 0 else 0\n",
    "    \n",
    "    posture_stats[posture] = {\n",
    "        'multi_freq': multi_freq,\n",
    "        'single_freq': single_freq,\n",
    "        'total_freq': total_freq,\n",
    "        'multi_percentage': multi_percentage\n",
    "    }\n",
    "\n",
    "# Sort by multi-posture percentage (descending)\n",
    "sorted_by_multi = sorted(posture_stats.items(), key=lambda x: x[1]['multi_percentage'], reverse=True)\n",
    "\n",
    "print(f\"TOP 15 POSTURES MOST INVOLVED IN MULTI-POSTURE DOCUMENTS:  \")\n",
    "print(f\"{'Rank':<4} {'Posture':<40} {'Multi':<6} {'Single':<7} {'Total':<7} {'Multi%':<8}\")\n",
    "\n",
    "\n",
    "\n",
    "# Most common posture combinations in multi-posture documents\n",
    "print(f\"MOST COMMON POSTURE COMBINATIONS:\")\n",
    "combinations = [tuple(sorted(doc['postures'])) for doc in multi_posture_docs]\n",
    "combination_counts = Counter(combinations)\n",
    "\n",
    "for i, (combo, count) in enumerate(combination_counts.most_common(10), 1):\n",
    "    percentage = (count / len(multi_posture_docs)) * 100\n",
    "    print(f\"{i:2d}. {' + '.join(combo)}\")\n",
    "    print(f\"Count: {count:,} ({percentage:.1f}% of multi-posture docs)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 331936506 characters\n",
      "\n",
      "Successfully parsed 18000 JSON objects\n",
      "\n",
      "Example processed text:\n",
      "[PARA] Plaintiff Dwight Watson (“Husband”) appeals from the trial court’s equitable distribution order entered 28 February 2017. On appeal, plaintiff contends that the trial court erred in its classification, valuation, and distribution of the parties’ property and in granting defendant Gertha  Watson (“Wife”) an unequal distribution of martial property. Because the trial court’s findings of fact do not support its conclusions of law and because the distributional factors found by the trial cour\n"
     ]
    }
   ],
   "source": [
    "# Create the additional field in order to concatenate the text present in multiple paragraphers.\n",
    "# Add some separators to facilitate tokenization and embedding\n",
    "print(f\"File size: {len(content)} characters\")\n",
    "\n",
    "data = []\n",
    "lines = content.strip().split('\\n')\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip():\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "\n",
    "            # combine text from sections\n",
    "            sections = json_obj.get(\"sections\", [])\n",
    "            text_parts = []\n",
    "\n",
    "            for sec in sections:\n",
    "                headtext = sec.get(\"headtext\", \"\")\n",
    "                if headtext:\n",
    "                    text_parts.append(\"[HEAD] \" + headtext)\n",
    "                paragraphs = sec.get(\"paragraphs\", [])\n",
    "                for para in paragraphs:\n",
    "                    text_parts.append(\"[PARA] \" + para)\n",
    "\n",
    "            # Join sections with [SEP]\n",
    "            full_text = \" [SEP] \".join(text_parts)\n",
    "\n",
    "            # Save new keys to the object\n",
    "            json_obj[\"combined_text\"] = full_text\n",
    "\n",
    "            # Append to data list\n",
    "            data.append(json_obj)\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing line {i+1}: {e}\")\n",
    "            print(f\"Line content: {line[:100]}...\")\n",
    "\n",
    "print(f\"\\nSuccessfully parsed {len(data)} JSON objects\")\n",
    "\n",
    "# Example: print first processed document\n",
    "print(\"\\nExample processed text:\")\n",
    "print(data[0][\"combined_text\"][:500])  # Print first 100 characters to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          documentId  \\\n",
      "0  Ib4e590e0a55f11e8a5d58a2c8dcb28b5   \n",
      "1  Ib06ab4d056a011e98c7a8e995225dbf9   \n",
      "2  Iaa3e3390b93111e9ba33b03ae9101fb2   \n",
      "3  I0d4dffc381b711e280719c3f0e80bdd0   \n",
      "4  I82c7ef10d6d111e8aec5b23c3317c9c0   \n",
      "\n",
      "                                              labels  \\\n",
      "0                                        [On Appeal]   \n",
      "1  [Appellate Review, Sentencing or Penalty Phase...   \n",
      "2          [Motion to Compel Arbitration, On Appeal]   \n",
      "3     [On Appeal, Review of Administrative Decision]   \n",
      "4                                        [On Appeal]   \n",
      "\n",
      "                                                text  \n",
      "0  [PARA] Plaintiff Dwight Watson (“Husband”) app...  \n",
      "1  [PARA] After pleading guilty, William Jerome H...  \n",
      "2  [PARA] Frederick Greene, the plaintiff below, ...  \n",
      "3  [PARA] Appeal from an amended judgment of the ...  \n",
      "4  [PARA] Order, Supreme Court, New York County (...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"documentId\": d[\"documentId\"],\n",
    "        \"labels\": d[\"postures\"],\n",
    "        \"text\": d[\"combined_text\"]\n",
    "    }\n",
    "    for d in data\n",
    "])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('On Appeal', 9197), ('Appellate Review', 4652), ('Review of Administrative Decision', 2773), ('Motion to Dismiss', 1679), ('Sentencing or Penalty Phase Motion or Objection', 1342), ('Trial or Guilt Phase Motion or Objection', 1097), (\"Motion for Attorney's Fees\", 612), ('Post-Trial Hearing Motion', 512), ('Motion for Preliminary Injunction', 364), ('Motion to Dismiss for Lack of Subject Matter Jurisdiction', 343), ('Motion to Compel Arbitration', 255), ('Motion for New Trial', 226), ('Petition to Terminate Parental Rights', 219), ('Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict', 212), ('Motion for Reconsideration', 206), ('Motion to Dismiss for Lack of Personal Jurisdiction', 204), ('Motion for Costs', 168), ('Juvenile Delinquency Proceeding', 146), ('Motion for Default Judgment/Order of Default', 143), ('Motion to Dismiss for Lack of Standing', 137)]\n",
      "top 5 labels :['On Appeal', 'Appellate Review', 'Review of Administrative Decision', 'Motion to Dismiss', 'Sentencing or Penalty Phase Motion or Objection']\n",
      "Remaining documents after filtering: 15661\n"
     ]
    }
   ],
   "source": [
    "# TAKE TOPk and keep only labels from the topk\n",
    "#  Exclude documents that end up with no remaining labels\n",
    "from collections import Counter\n",
    "\n",
    "# data is a list of dicts, each with a 'postures' field\n",
    "label_counter = Counter()\n",
    "for entry in data:\n",
    "    label_counter.update(entry[\"postures\"])\n",
    "# Print most common labels\n",
    "print(label_counter.most_common(20))\n",
    "\n",
    "k = 5  \n",
    "top_k_labels = [label for label, _ in label_counter.most_common(k)]\n",
    "print(f\"top {5} labels :{top_k_labels}\")\n",
    "filtered_data = []\n",
    "for entry in data:\n",
    "    # Keep only labels that are in top k\n",
    "    new_labels = [label for label in entry[\"postures\"] if label in top_k_labels]\n",
    "\n",
    "    if new_labels:\n",
    "        # Update labels\n",
    "        entry[\"postures\"] = new_labels\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "print(f\"Remaining documents after filtering: {len(filtered_data)}\")\n",
    "\n",
    "# next steps: take care also of the kappa score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 10962\n",
      "val set size: 2349\n",
      "Test set size: 2350\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: train vs (val + test)\n",
    "train_data, temp_data = train_test_split(\n",
    "    filtered_data, \n",
    "    test_size=0.3,  # 30% goes to temp (val + test)\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: val vs test (half-half from temp)\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, \n",
    "    test_size=0.5,  # Split remaining 30% equally\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check sizes\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"val set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created subset with 500 samples.\n",
      "Created subset with 1000 samples.\n",
      "Created subset with 2000 samples.\n",
      "Created subset with 5000 samples.\n",
      "Created subset with 10000 samples.\n",
      "[PARA] Proceeding pursuant to CPLR article 78 (transferred to this Court by order of the Supreme Court, entered in Albany County) to review a determination of the Department of Health which found decedent ineligible for certain Medicaid benefits. [SEP] [PARA] In September 2003, Paul Hettinger (hereinafter decedent), a widower, executed a durable general power of attorney appointing Sharon Williams, an individual identified in the record as his cousin-in-law, as his attorney-in-fact.   The power \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "random.seed(42)\n",
    "# Define desired sample sizes\n",
    "sample_sizes = [500,1000, 2000, 5000, 10000]\n",
    "# Save all subsets\n",
    "train_subsets = {}\n",
    "\n",
    "for size in sample_sizes:\n",
    "    # Check if wehave enough samples\n",
    "    if len(train_data) >= size:\n",
    "        subset = random.sample(train_data, size)\n",
    "        train_subsets[size] = subset\n",
    "        print(f\"Created subset with {size} samples.\")\n",
    "    else:\n",
    "        print(f\"Not enough data to create subset with {size} samples (only {len(train_data)} avlable).\")\n",
    "\n",
    "# Example: look at first document text in 1k subset\n",
    "print(train_subsets[500][0][\"combined_text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilRoBERTa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare label binarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "label_list = top_k_labels\n",
    "# Create binarizer\n",
    "mlb = MultiLabelBinarizer(classes=label_list)\n",
    "mlb.fit([label_list])\n",
    "\n",
    "\n",
    "# Use the previously created 500-sample subset\n",
    "subset_500 = train_subsets[500]\n",
    "\n",
    "# Texts and labels from the subset\n",
    "train_texts = [entry[\"combined_text\"] for entry in subset_500]\n",
    "train_labels_raw = [entry[\"postures\"] for entry in subset_500]\n",
    "\n",
    "\n",
    "val_texts = [entry[\"combined_text\"] for entry in val_data]\n",
    "val_labels_raw = [entry[\"postures\"] for entry in val_data]\n",
    "\n",
    "test_texts = [entry[\"combined_text\"] for entry in test_data]\n",
    "test_labels_raw = [entry[\"postures\"] for entry in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_labels = mlb.transform(train_labels_raw)\n",
    "val_labels = mlb.transform(val_labels_raw)\n",
    "test_labels = mlb.transform(test_labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tojkenizer\n",
    "\n",
    "# tokenize text\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LegalMLCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        item[\"labels\"] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "train_dataset = LegalMLCDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = LegalMLCDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = LegalMLCDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model and modify for multi-label\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(label_list)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilroberta-base\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# M1 confiuration device\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/firaterman/Documents/fer/berserk3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,      # reduce it because of laptop limittions\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate on dev set each epoch\n",
    "    save_strategy=\"epoch\",              # Save checkpoint each epoch\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # Use lowest validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    probs = 1 / (1 + np.exp(-logits))  # sigmoid\n",
    "    preds = (probs >= 0.5).astype(int) # threshold\n",
    "\n",
    "    metrics = {\n",
    "        'micro/f1': f1_score(labels, preds, average='micro', zero_division=0),\n",
    "        'macro/f1': f1_score(labels, preds, average='macro', zero_division=0),\n",
    "        'micro/precision': precision_score(labels, preds, average='micro', zero_division=0),\n",
    "        'macro/precision': precision_score(labels, preds, average='macro', zero_division=0),\n",
    "        'micro/recall': recall_score(labels, preds, average='micro', zero_division=0),\n",
    "        'macro/recall': recall_score(labels, preds, average='macro', zero_division=0),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### option A: subset of 500 documents and top5 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 06:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro/f1</th>\n",
       "      <th>Macro/f1</th>\n",
       "      <th>Micro/precision</th>\n",
       "      <th>Macro/precision</th>\n",
       "      <th>Micro/recall</th>\n",
       "      <th>Macro/recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.307804</td>\n",
       "      <td>0.728411</td>\n",
       "      <td>0.355044</td>\n",
       "      <td>0.827155</td>\n",
       "      <td>0.334795</td>\n",
       "      <td>0.650729</td>\n",
       "      <td>0.377927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.250385</td>\n",
       "      <td>0.801531</td>\n",
       "      <td>0.504835</td>\n",
       "      <td>0.866430</td>\n",
       "      <td>0.533214</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>0.495168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.242956</td>\n",
       "      <td>0.816088</td>\n",
       "      <td>0.523792</td>\n",
       "      <td>0.885363</td>\n",
       "      <td>0.533953</td>\n",
       "      <td>0.756867</td>\n",
       "      <td>0.519173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.2781613210042318, metrics={'train_runtime': 410.2974, 'train_samples_per_second': 3.656, 'train_steps_per_second': 1.828, 'total_flos': 198711728640000.0, 'train_loss': 0.2781613210042318, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2164878249168396, 'eval_micro/f1': 0.8339106654512306, 'eval_macro/f1': 0.5349275550375603, 'eval_micro/precision': 0.9000393545848091, 'eval_macro/precision': 0.5353950156860148, 'eval_micro/recall': 0.7768342391304348, 'eval_macro/recall': 0.536347816782792, 'eval_runtime': 75.6638, 'eval_samples_per_second': 31.058, 'eval_steps_per_second': 15.529, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### option B: subset of 1000 documents and top5 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previously created 1000-sample subset\n",
    "subset_1000 = train_subsets[1000]\n",
    "\n",
    "# Texts and labels from the subset\n",
    "train_texts = [entry[\"combined_text\"] for entry in subset_1000]\n",
    "train_labels_raw = [entry[\"postures\"] for entry in subset_1000]\n",
    "\n",
    "\n",
    "val_texts = [entry[\"combined_text\"] for entry in val_data]\n",
    "val_labels_raw = [entry[\"postures\"] for entry in val_data]\n",
    "\n",
    "test_texts = [entry[\"combined_text\"] for entry in test_data]\n",
    "test_labels_raw = [entry[\"postures\"] for entry in test_data]\n",
    "\n",
    "\n",
    "\n",
    "train_labels = mlb.transform(train_labels_raw)\n",
    "val_labels = mlb.transform(val_labels_raw)\n",
    "test_labels = mlb.transform(test_labels_raw)\n",
    "\n",
    "\n",
    "train_dataset = LegalMLCDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = LegalMLCDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = LegalMLCDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 09:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro/f1</th>\n",
       "      <th>Macro/f1</th>\n",
       "      <th>Micro/precision</th>\n",
       "      <th>Macro/precision</th>\n",
       "      <th>Micro/recall</th>\n",
       "      <th>Macro/recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>0.210849</td>\n",
       "      <td>0.844989</td>\n",
       "      <td>0.730060</td>\n",
       "      <td>0.848019</td>\n",
       "      <td>0.782337</td>\n",
       "      <td>0.841980</td>\n",
       "      <td>0.740142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.205860</td>\n",
       "      <td>0.860527</td>\n",
       "      <td>0.685403</td>\n",
       "      <td>0.898818</td>\n",
       "      <td>0.836328</td>\n",
       "      <td>0.825365</td>\n",
       "      <td>0.663118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.200388</td>\n",
       "      <td>0.876796</td>\n",
       "      <td>0.801184</td>\n",
       "      <td>0.873989</td>\n",
       "      <td>0.802925</td>\n",
       "      <td>0.879620</td>\n",
       "      <td>0.800582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17552222311496735, 'eval_micro/f1': 0.8896388795140061, 'eval_macro/f1': 0.8136859754679941, 'eval_micro/precision': 0.8839704896042925, 'eval_macro/precision': 0.8080036994610944, 'eval_micro/recall': 0.8953804347826086, 'eval_macro/recall': 0.8196197054883976, 'eval_runtime': 75.2187, 'eval_samples_per_second': 31.242, 'eval_steps_per_second': 15.621, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### option C: subset of 500 documents and top10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 labels :['On Appeal', 'Appellate Review', 'Review of Administrative Decision', 'Motion to Dismiss', 'Sentencing or Penalty Phase Motion or Objection', 'Trial or Guilt Phase Motion or Objection', \"Motion for Attorney's Fees\", 'Post-Trial Hearing Motion', 'Motion for Preliminary Injunction', 'Motion to Dismiss for Lack of Subject Matter Jurisdiction']\n",
      "Remaining documents after filtering: 16140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 09:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro/f1</th>\n",
       "      <th>Macro/f1</th>\n",
       "      <th>Micro/precision</th>\n",
       "      <th>Macro/precision</th>\n",
       "      <th>Micro/recall</th>\n",
       "      <th>Macro/recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>0.213335</td>\n",
       "      <td>0.871021</td>\n",
       "      <td>0.803833</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.774558</td>\n",
       "      <td>0.890810</td>\n",
       "      <td>0.838396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.258268</td>\n",
       "      <td>0.872913</td>\n",
       "      <td>0.773823</td>\n",
       "      <td>0.877097</td>\n",
       "      <td>0.813184</td>\n",
       "      <td>0.868769</td>\n",
       "      <td>0.752101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.251022</td>\n",
       "      <td>0.880777</td>\n",
       "      <td>0.802876</td>\n",
       "      <td>0.869997</td>\n",
       "      <td>0.796360</td>\n",
       "      <td>0.891828</td>\n",
       "      <td>0.810407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19714035093784332, 'eval_micro/f1': 0.8770940454470061, 'eval_macro/f1': 0.8097453084242956, 'eval_micro/precision': 0.8570502431118314, 'eval_macro/precision': 0.7800796262236082, 'eval_micro/recall': 0.8980978260869565, 'eval_macro/recall': 0.846107740694551, 'eval_runtime': 75.5588, 'eval_samples_per_second': 31.102, 'eval_steps_per_second': 15.551, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "top_k_labels = [label for label, _ in label_counter.most_common(k)]\n",
    "print(f\"top {10} labels :{top_k_labels}\")\n",
    "filtered_data = []\n",
    "for entry in data:\n",
    "    # Keep only labels that are in top k\n",
    "    new_labels = [label for label in entry[\"postures\"] if label in top_k_labels]\n",
    "\n",
    "    if new_labels:\n",
    "        # Update labels\n",
    "        entry[\"postures\"] = new_labels\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "print(f\"Remaining documents after filtering: {len(filtered_data)}\")\n",
    "\n",
    "\n",
    "# Use the previously created 1000-sample subset\n",
    "subset_500 = train_subsets[500]\n",
    "\n",
    "# Texts and labels from the subset\n",
    "train_texts = [entry[\"combined_text\"] for entry in subset_1000]\n",
    "train_labels_raw = [entry[\"postures\"] for entry in subset_1000]\n",
    "\n",
    "\n",
    "val_texts = [entry[\"combined_text\"] for entry in val_data]\n",
    "val_labels_raw = [entry[\"postures\"] for entry in val_data]\n",
    "\n",
    "test_texts = [entry[\"combined_text\"] for entry in test_data]\n",
    "test_labels_raw = [entry[\"postures\"] for entry in test_data]\n",
    "\n",
    "\n",
    "\n",
    "train_labels = mlb.transform(train_labels_raw)\n",
    "val_labels = mlb.transform(val_labels_raw)\n",
    "test_labels = mlb.transform(test_labels_raw)\n",
    "\n",
    "\n",
    "train_dataset = LegalMLCDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = LegalMLCDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = LegalMLCDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berserk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
