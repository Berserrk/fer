{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_p_list = {\n",
    "    \")\":\"(\",\n",
    "    \"]\":\"[\",\n",
    "    \"}\":\"{\"\n",
    "}\n",
    "stack = []\n",
    "\n",
    "def valid_parentheses(s):\n",
    "    for p in s:\n",
    "        if p in valid_p_list.values():\n",
    "            stack.append(p)\n",
    "        elif stack and stack[-1]==valid_p_list[p]:\n",
    "            stack.pop()\n",
    "    return stack == []\n",
    "    \n",
    "\n",
    "s = \"()[]{\"\n",
    "ys= \"()[]\"\n",
    "valid_parentheses(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/firaterman/Documents/fer/berserk3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification, pipeline\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple, Set\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from docx import Document\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import spacy\n",
    "\n",
    "class BertNERProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize models and NLP tools\"\"\"\n",
    "        # BERT NER model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "        self.label_list = [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "        \n",
    "        # Sentence transformer for semantic similarity\n",
    "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # SpaCy for additional NLP tasks\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def process_docx(self, file_path: str) -> Dict[str, List[Dict[str, List[str]]]]:\n",
    "        \"\"\"Process DOCX file and extract grouped entities\"\"\"\n",
    "        doc = Document(file_path)\n",
    "        full_text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        chunks = self._split_into_chunks(full_text)\n",
    "        \n",
    "        # Extract initial entities\n",
    "        raw_entities = {\n",
    "            'PERSON': [],\n",
    "            'ORGANIZATION': [],\n",
    "            'LOCATION': [],\n",
    "            'MISCELLANEOUS': []\n",
    "        }\n",
    "        \n",
    "        # Process chunks and extract entities\n",
    "        for chunk in chunks:\n",
    "            entities = self._extract_entities(chunk)\n",
    "            for entity_type, entities_list in entities.items():\n",
    "                raw_entities[entity_type].extend(entities_list)\n",
    "        \n",
    "        # Group similar entities\n",
    "        grouped_entities = self._group_entities(raw_entities)\n",
    "        \n",
    "        return grouped_entities\n",
    "\n",
    "    def _group_entities(self, raw_entities: Dict[str, List[Dict[str, str]]]) -> Dict[str, List[Dict[str, List[str]]]]:\n",
    "        \"\"\"Group similar entities together using multiple similarity measures\"\"\"\n",
    "        grouped_results = {}\n",
    "        \n",
    "        for entity_type, entities in raw_entities.items():\n",
    "            if not entities:\n",
    "                grouped_results[entity_type] = []\n",
    "                continue\n",
    "                \n",
    "            # Extract unique entity texts\n",
    "            unique_entities = list({e['text'] for e in entities})\n",
    "            \n",
    "            if len(unique_entities) == 0:\n",
    "                grouped_results[entity_type] = []\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity matrix using multiple measures\n",
    "            embeddings = self.semantic_model.encode(unique_entities)\n",
    "            \n",
    "            # Perform clustering\n",
    "            clusters = self._cluster_entities(embeddings, unique_entities)\n",
    "            \n",
    "            # Post-process clusters with rule-based refinements\n",
    "            refined_clusters = self._refine_clusters(clusters, entity_type)\n",
    "            \n",
    "            grouped_results[entity_type] = refined_clusters\n",
    "            \n",
    "        return grouped_results\n",
    "\n",
    "    def _cluster_entities(self, embeddings: np.ndarray, entities: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Cluster entities using DBSCAN\"\"\"\n",
    "        # Perform DBSCAN clustering\n",
    "        clustering = DBSCAN(eps=0.3, min_samples=1, metric='cosine').fit(embeddings)\n",
    "        \n",
    "        # Group entities by cluster\n",
    "        clusters = {}\n",
    "        for idx, label in enumerate(clustering.labels_):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(entities[idx])\n",
    "            \n",
    "        return list(clusters.values())\n",
    "\n",
    "    def _refine_clusters(self, clusters: List[List[str]], entity_type: str) -> List[Dict[str, List[str]]]:\n",
    "        \"\"\"Apply rule-based refinements to clusters\"\"\"\n",
    "        refined_clusters = []\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            main_entity = self._find_main_entity(cluster, entity_type)\n",
    "            variations = [e for e in cluster if e != main_entity]\n",
    "            \n",
    "            # Apply type-specific rules\n",
    "            if entity_type == 'PERSON':\n",
    "                variations = self._refine_person_cluster(main_entity, variations)\n",
    "            elif entity_type == 'ORGANIZATION':\n",
    "                variations = self._refine_org_cluster(main_entity, variations)\n",
    "                \n",
    "            refined_clusters.append({\n",
    "                'main': main_entity,\n",
    "                'variations': variations\n",
    "            })\n",
    "            \n",
    "        return refined_clusters\n",
    "\n",
    "    def _find_main_entity(self, cluster: List[str], entity_type: str) -> str:\n",
    "        \"\"\"Determine the main entity name from a cluster\"\"\"\n",
    "        if entity_type == 'PERSON':\n",
    "            # Prefer full names\n",
    "            full_names = [name for name in cluster if len(name.split()) > 1]\n",
    "            if full_names:\n",
    "                return max(full_names, key=len)\n",
    "        \n",
    "        # Default to longest name\n",
    "        return max(cluster, key=len)\n",
    "\n",
    "    def _refine_person_cluster(self, main_entity: str, variations: List[str]) -> List[str]:\n",
    "        \"\"\"Apply person-specific refinement rules\"\"\"\n",
    "        main_doc = self.nlp(main_entity)\n",
    "        refined_variations = set(variations)\n",
    "        \n",
    "        # Extract main name components\n",
    "        main_names = set()\n",
    "        for token in main_doc:\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                main_names.add(token.text.lower())\n",
    "        \n",
    "        # Filter variations\n",
    "        for var in variations:\n",
    "            var_doc = self.nlp(var)\n",
    "            var_names = set()\n",
    "            for token in var_doc:\n",
    "                if token.pos_ == \"PROPN\":\n",
    "                    var_names.add(token.text.lower())\n",
    "            \n",
    "            # Remove if no name overlap\n",
    "            if not (main_names & var_names):\n",
    "                refined_variations.discard(var)\n",
    "                \n",
    "        return list(refined_variations)\n",
    "\n",
    "    def _refine_org_cluster(self, main_entity: str, variations: List[str]) -> List[str]:\n",
    "        \"\"\"Apply organization-specific refinement rules\"\"\"\n",
    "        main_tokens = set(self.nlp(main_entity.lower()))\n",
    "        refined_variations = set()\n",
    "        \n",
    "        for var in variations:\n",
    "            var_tokens = set(self.nlp(var.lower()))\n",
    "            # Keep if significant token overlap\n",
    "            if len(main_tokens & var_tokens) / len(main_tokens) > 0.3:\n",
    "                refined_variations.add(var)\n",
    "                \n",
    "        return list(refined_variations)\n",
    "\n",
    "    def _extract_entities(self, text: str) -> Dict[str, List[Dict[str, str]]]:\n",
    "        \"\"\"Extract named entities from text chunk\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        labels = [self.label_list[p] for p in predictions[0]]\n",
    "        \n",
    "        entities = {\n",
    "            'PERSON': [],\n",
    "            'ORGANIZATION': [],\n",
    "            'LOCATION': [],\n",
    "            'MISCELLANEOUS': []\n",
    "        }\n",
    "        \n",
    "        current_entity = {'type': None, 'text': ''}\n",
    "        \n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity['text']:\n",
    "                    self._add_entity(entities, current_entity)\n",
    "                current_entity = {\n",
    "                    'type': label[2:],\n",
    "                    'text': token.replace('##', '')\n",
    "                }\n",
    "            elif label.startswith('I-') and current_entity['text']:\n",
    "                current_entity['text'] += token.replace('##', '')\n",
    "            elif label == 'O':\n",
    "                if current_entity['text']:\n",
    "                    self._add_entity(entities, current_entity)\n",
    "                current_entity = {'type': None, 'text': ''}\n",
    "        \n",
    "        if current_entity['text']:\n",
    "            self._add_entity(entities, current_entity)\n",
    "            \n",
    "        return entities\n",
    "\n",
    "    def _split_into_chunks(self, text: str, max_length: int = 400) -> List[str]:\n",
    "        \"\"\"Split text into processable chunks\"\"\"\n",
    "        sentences = re.split('([.!?])', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) < max_length:\n",
    "                current_chunk += sentence\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def _add_entity(self, entities: Dict[str, List[Dict[str, str]]], \n",
    "                   entity: Dict[str, str]) -> None:\n",
    "        \"\"\"Add entity to appropriate category\"\"\"\n",
    "        if not entity['type']:\n",
    "            return\n",
    "            \n",
    "        entity_text = entity['text'].strip()\n",
    "        if not entity_text:\n",
    "            return\n",
    "            \n",
    "        entity_type = entity['type']\n",
    "        entity_dict = {'text': entity_text, 'type': entity_type}\n",
    "        \n",
    "        if entity_type == 'PER':\n",
    "            entities['PERSON'].append(entity_dict)\n",
    "        elif entity_type == 'ORG':\n",
    "            entities['ORGANIZATION'].append(entity_dict)\n",
    "        elif entity_type == 'LOC':\n",
    "            entities['LOCATION'].append(entity_dict)\n",
    "        elif entity_type == 'MISC':\n",
    "            entities['MISCELLANEOUS'].append(entity_dict)\n",
    "\n",
    "def process_document(file_path: str) -> None:\n",
    "    \"\"\"Process document and print grouped entities\"\"\"\n",
    "    processor = BertNERProcessor()\n",
    "    try:\n",
    "        entities = processor.process_docx(file_path)\n",
    "        \n",
    "        print(\"\\nExtracted and Grouped Named Entities:\")\n",
    "        print(\"-----------------------------------\")\n",
    "        \n",
    "        for category, clusters in entities.items():\n",
    "            if clusters:\n",
    "                print(f\"\\n{category}:\")\n",
    "                for idx, cluster in enumerate(clusters, 1):\n",
    "                    print(f\"\\nGroup {idx}:\")\n",
    "                    print(f\"Main: {cluster['main']}\")\n",
    "                    if cluster['variations']:\n",
    "                        print(\"Variations:\")\n",
    "                        for var in cluster['variations']:\n",
    "                            print(f\"- {var}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"path/to/your/document.docx\"\n",
    "    process_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =1 \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL \n",
    "pays = \"FRANCE1234\"\n",
    "F R A N C E 1 2 3 4\n",
    "1 2 3 4 5 6 7 8 9 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON \n",
    "pays = \"FRANCE1234\"\n",
    "F R A N C E 1 2 3 4\n",
    "0 1 2 3 4 5 6 7 8 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phillipe\n",
      "Mohammed\n",
      "les\n"
     ]
    }
   ],
   "source": [
    "# python \n",
    "eleves_list = [\"phillipe\", \"Mohammed\", \"les\"]\n",
    "\n",
    "for eleve in eleves_list:\n",
    "    print(eleve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP1: voici la liste des eleves: ['phillipe', 'Mohammed'] se termine ici.\n",
      "hello world\n",
      "les\n"
     ]
    }
   ],
   "source": [
    "eleves_list = [\"phillipe\", \"Mohammed\"]\n",
    "print(\"STEP1: voici la liste des eleves:\", eleves_list, \"se termine ici.\")\n",
    "print(\"hello world\")\n",
    "print(\"les\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for numero in range(1,10):\n",
    "    print(numero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age           City  Salary Department  Performance  Hire_Date\n",
      "0    Alice   25       New York   50000         HR          4.5 2022-01-01\n",
      "1      Bob   30  San Francisco   75000         IT          4.2 2022-01-02\n",
      "2  Charlie   35        Chicago   60000    Finance          4.7 2022-01-03\n",
      "3    David   28         Boston   65000  Marketing          3.9 2022-01-04\n",
      "4      Eve   22        Seattle   45000      Sales          4.1 2022-01-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 22],\n",
    "    'City': ['New York', 'San Francisco', 'Chicago', 'Boston', 'Seattle'],\n",
    "    'Salary': [50000, 75000, 60000, 65000, 45000],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'Marketing', 'Sales'],\n",
    "    'Performance': [4.5, 4.2, 4.7, 3.9, 4.1]\n",
    "})\n",
    "\n",
    "# Optionally, add a date column\n",
    "df['Hire_Date'] = pd.date_range(start='2022-01-01', periods=5)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "list_a = [1,3,4]\n",
    "\n",
    "if list_a[1] != 5 and list_a[0]!=8:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Name                         Alice\n",
      "Age                             25\n",
      "City                      New York\n",
      "Salary                       50000\n",
      "Department                      HR\n",
      "Performance                    4.5\n",
      "Hire_Date      2022-01-01 00:00:00\n",
      "Name: 0, dtype: object\n",
      "1 Name                           Bob\n",
      "Age                             30\n",
      "City                 San Francisco\n",
      "Salary                       75000\n",
      "Department                      IT\n",
      "Performance                    4.2\n",
      "Hire_Date      2022-01-02 00:00:00\n",
      "Name: 1, dtype: object\n",
      "2 Name                       Charlie\n",
      "Age                             35\n",
      "City                       Chicago\n",
      "Salary                       60000\n",
      "Department                 Finance\n",
      "Performance                    4.7\n",
      "Hire_Date      2022-01-03 00:00:00\n",
      "Name: 2, dtype: object\n",
      "3 Name                         David\n",
      "Age                             28\n",
      "City                        Boston\n",
      "Salary                       65000\n",
      "Department               Marketing\n",
      "Performance                    3.9\n",
      "Hire_Date      2022-01-04 00:00:00\n",
      "Name: 3, dtype: object\n",
      "4 Name                           Eve\n",
      "Age                             22\n",
      "City                       Seattle\n",
      "Salary                       45000\n",
      "Department                   Sales\n",
      "Performance                    4.1\n",
      "Hire_Date      2022-01-05 00:00:00\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories_article = [\"col1\", \"col2\", \"col3\"]\n",
    "# Create the activities table if it doesn't exist using BOOLEAN\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS activities (\n",
    "    entity TEXT,\n",
    "    activities BOOLEAN,\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query)\n",
    "\n",
    "# Create the table_query_db if it doesn't exist\n",
    "create_table_query_db = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS table_query_db (\n",
    "    entity TEXT,\n",
    "    activities BOOLEAN,\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query_db)\n",
    "\n",
    "insert_data = []\n",
    "for row in dr.iterrows():\n",
    "    step_data = row[1]\n",
    "    entity = step_data[\"entity\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    summary = step_data[\"summary\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    activities = tuple(bool(row[\"activity\"]) for activity in categories_article if activity is \"no label\") # Convert activities to boolean\n",
    "    current_timestamp = datetime.now().isoformat() # Get current timestamp in ISO format\n",
    "    comments = row[\"comments\"].replace(\"'\", \"''\")\n",
    "    flagged = bool(row[\"flagged\"])\n",
    "\n",
    "    data_row = (entity, activities, current_timestamp, comments, flagged)\n",
    "\n",
    "    # Check if the last entry for this entity is different\n",
    "    query = f\"SELECT * FROM table_query_db WHERE entity = %s ORDER BY timestamp DESC LIMIT 1\"\n",
    "    last_entry = conn.execute(query, (entity,)).fetchone()\n",
    "\n",
    "    # Check if there is a change in the activities\n",
    "    if last_entry:\n",
    "        if last_entry[1] != activities: # Exclude entity and timestamp for comparison\n",
    "            insert_data.append(data_row)\n",
    "    else:\n",
    "        insert_data.append(data_row) # This\n",
    "\n",
    "if insert_data:\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO {table_name} VALUES ({', '.join(['?'] * (len(categories_article) + 4))})\n",
    "    \"\"\"\n",
    "    conn.executemany(query,insert_data)\n",
    "    conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the activities table if it doesn't exist using BOOLEAN\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS activities (\n",
    "    entity TEXT,\n",
    "    {\",\".join([f'\"{activity}\" BOOLEAN' for activity in categories_article if activity != 'no label'])},\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query)\n",
    "# Create the table_query_db if it doesn't exist\n",
    "create_table_query_db = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS table_query_db (\n",
    "    entity TEXT,\n",
    "    {\",\".join([f'\"{activity}\" BOOLEAN' for activity in categories_article if activity != 'no label'])},\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query_db)\n",
    "insert_data = []\n",
    "for row in dr.iterrows():\n",
    "    step_data = row[1]\n",
    "    entity = step_data[\"entity\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    summary = step_data[\"summary\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    activities = tuple(bool(row[\"activity\"]) for activity in categories_article if activity is \"no label\") # Convert activities to boolean\n",
    "    current_timestamp = datetime.now().isoformat() # Get current timestamp in ISO format\n",
    "    comments = row[\"comments\"].replace(\"'\", \"''\")\n",
    "    flagged = bool(row[\"flagged\"])\n",
    "    data_row = (entity, activities, current_timestamp, comments, flagged)\n",
    "    # Check if the last entry for this entity is different\n",
    "    query = f\"SELECT * FROM table_query_db WHERE entity = %s ORDER BY timestamp DESC LIMIT 1\"\n",
    "    last_entry = conn.execute(query, (entity,)).fetchone()\n",
    "    # Check if there is a change in the activities\n",
    "    if last_entry:\n",
    "        if last_entry[1] != activities: # Exclude entity and timestamp for comparison\n",
    "            insert_data.append(data_row)\n",
    "    else:\n",
    "        insert_data.append(data_row) # This is a new entry for an entity\n",
    "\n",
    "if insert_data:\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO {table_name} VALUES ({', '.join(['?'] * (len(categories_article) + 4))})\n",
    "    \"\"\"\n",
    "    conn.executemany(query,insert_data)\n",
    "    conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387209.73000000004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range: \n",
    "    print(i)\n",
    "    def(in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in ./berserk3/lib/python3.12/site-packages (2024.11.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:\n",
      "Hello, Wörld! 123. Grüße\n",
      "\n",
      "Processed String:\n",
      "Hello  Wörld  123  Grüße\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# Step 1: Define the input string\n",
    "input_string = \"Hello, Wörld! 123. Grüße\"\n",
    "\n",
    "print(\"Original String:\")\n",
    "print(input_string)\n",
    "\n",
    "# Step 2: Replace non-alphanumeric characters with whitespace\n",
    "# Use the Unicode property \\p{L} to match any kind of letter and \\p{N} to match any kind of number\n",
    "output_string = re.sub(r'[^\\p{L}\\p{N}]', ' ', input_string)\n",
    "\n",
    "print(\"\\nProcessed String:\")\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:\n",
      "$2.8M\n",
      "\n",
      "Processed String:\n",
      " 2.8M\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Define the input string\n",
    "input_string = \"$2.8M\"\n",
    "\n",
    "print(\"Original String:\")\n",
    "print(input_string)\n",
    "\n",
    "# Step 2: Replace non-alphanumeric characters except dots, commas, and dollar signs with whitespace\n",
    "output_string = re.sub(r'[^a-zA-Z0-9.,]', ' ', input_string)\n",
    "\n",
    "print(\"\\nProcessed String:\")\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "   # Pattern includes Latin alphabet extensions used in European languages\n",
    "   pattern = r'[^a-zA-ZàáâäãåąčćęèéêëėįìíîïłńòóôöõøùúûüųūÿýżźñçčšžÀÁÂÄÃÅĄĆČĖĘÈÉÊËÌÍÎÏĮŁŃÒÓÔÖÕØÙÚÛÜŲŪŸÝŻŹÑßÇČŠŽ0-9.,]'\n",
    "   return re.sub(pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berserk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
