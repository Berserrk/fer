{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid_p_list = {\n",
    "    \")\":\"(\",\n",
    "    \"]\":\"[\",\n",
    "    \"}\":\"{\"\n",
    "}\n",
    "stack = []\n",
    "\n",
    "def valid_parentheses(s):\n",
    "    for p in s:\n",
    "        if p in valid_p_list.values():\n",
    "            stack.append(p)\n",
    "        elif stack and stack[-1]==valid_p_list[p]:\n",
    "            stack.pop()\n",
    "    return stack == []\n",
    "    \n",
    "\n",
    "    s = \"()[]{\"\n",
    "    ys= \"()[]\"\n",
    "    valid_parentheses(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdifflib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SequenceMatcher\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DBSCAN\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from docx import Document\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import spacy\n",
    "\n",
    "class BertNERProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize models and NLP tools\"\"\"\n",
    "        # BERT NER model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "        self.label_list = [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "        \n",
    "        # Sentence transformer for semantic similarity\n",
    "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # SpaCy for additional NLP tasks\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def process_docx(self, file_path: str) -> Dict[str, List[Dict[str, List[str]]]]:\n",
    "        \"\"\"Process DOCX file and extract grouped entities\"\"\"\n",
    "        doc = Document(file_path)\n",
    "        full_text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        chunks = self._split_into_chunks(full_text)\n",
    "        \n",
    "        # Extract initial entities\n",
    "        raw_entities = {\n",
    "            'PERSON': [],\n",
    "            'ORGANIZATION': [],\n",
    "            'LOCATION': [],\n",
    "            'MISCELLANEOUS': []\n",
    "        }\n",
    "        \n",
    "        # Process chunks and extract entities\n",
    "        for chunk in chunks:\n",
    "            entities = self._extract_entities(chunk)\n",
    "            for entity_type, entities_list in entities.items():\n",
    "                raw_entities[entity_type].extend(entities_list)\n",
    "        \n",
    "        # Group similar entities\n",
    "        grouped_entities = self._group_entities(raw_entities)\n",
    "        \n",
    "        return grouped_entities\n",
    "\n",
    "    def _group_entities(self, raw_entities: Dict[str, List[Dict[str, str]]]) -> Dict[str, List[Dict[str, List[str]]]]:\n",
    "        \"\"\"Group similar entities together using multiple similarity measures\"\"\"\n",
    "        grouped_results = {}\n",
    "        \n",
    "        for entity_type, entities in raw_entities.items():\n",
    "            if not entities:\n",
    "                grouped_results[entity_type] = []\n",
    "                continue\n",
    "                \n",
    "            # Extract unique entity texts\n",
    "            unique_entities = list({e['text'] for e in entities})\n",
    "            \n",
    "            if len(unique_entities) == 0:\n",
    "                grouped_results[entity_type] = []\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity matrix using multiple measures\n",
    "            embeddings = self.semantic_model.encode(unique_entities)\n",
    "            \n",
    "            # Perform clustering\n",
    "            clusters = self._cluster_entities(embeddings, unique_entities)\n",
    "            \n",
    "            # Post-process clusters with rule-based refinements\n",
    "            refined_clusters = self._refine_clusters(clusters, entity_type)\n",
    "            \n",
    "            grouped_results[entity_type] = refined_clusters\n",
    "            \n",
    "        return grouped_results\n",
    "\n",
    "    def _cluster_entities(self, embeddings: np.ndarray, entities: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Cluster entities using DBSCAN\"\"\"\n",
    "        # Perform DBSCAN clustering\n",
    "        clustering = DBSCAN(eps=0.3, min_samples=1, metric='cosine').fit(embeddings)\n",
    "        \n",
    "        # Group entities by cluster\n",
    "        clusters = {}\n",
    "        for idx, label in enumerate(clustering.labels_):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(entities[idx])\n",
    "            \n",
    "        return list(clusters.values())\n",
    "\n",
    "    def _refine_clusters(self, clusters: List[List[str]], entity_type: str) -> List[Dict[str, List[str]]]:\n",
    "        \"\"\"Apply rule-based refinements to clusters\"\"\"\n",
    "        refined_clusters = []\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            main_entity = self._find_main_entity(cluster, entity_type)\n",
    "            variations = [e for e in cluster if e != main_entity]\n",
    "            \n",
    "            # Apply type-specific rules\n",
    "            if entity_type == 'PERSON':\n",
    "                variations = self._refine_person_cluster(main_entity, variations)\n",
    "            elif entity_type == 'ORGANIZATION':\n",
    "                variations = self._refine_org_cluster(main_entity, variations)\n",
    "                \n",
    "            refined_clusters.append({\n",
    "                'main': main_entity,\n",
    "                'variations': variations\n",
    "            })\n",
    "            \n",
    "        return refined_clusters\n",
    "\n",
    "    def _find_main_entity(self, cluster: List[str], entity_type: str) -> str:\n",
    "        \"\"\"Determine the main entity name from a cluster\"\"\"\n",
    "        if entity_type == 'PERSON':\n",
    "            # Prefer full names\n",
    "            full_names = [name for name in cluster if len(name.split()) > 1]\n",
    "            if full_names:\n",
    "                return max(full_names, key=len)\n",
    "        \n",
    "        # Default to longest name\n",
    "        return max(cluster, key=len)\n",
    "\n",
    "    def _refine_person_cluster(self, main_entity: str, variations: List[str]) -> List[str]:\n",
    "        \"\"\"Apply person-specific refinement rules\"\"\"\n",
    "        main_doc = self.nlp(main_entity)\n",
    "        refined_variations = set(variations)\n",
    "        \n",
    "        # Extract main name components\n",
    "        main_names = set()\n",
    "        for token in main_doc:\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                main_names.add(token.text.lower())\n",
    "        \n",
    "        # Filter variations\n",
    "        for var in variations:\n",
    "            var_doc = self.nlp(var)\n",
    "            var_names = set()\n",
    "            for token in var_doc:\n",
    "                if token.pos_ == \"PROPN\":\n",
    "                    var_names.add(token.text.lower())\n",
    "            \n",
    "            # Remove if no name overlap\n",
    "            if not (main_names & var_names):\n",
    "                refined_variations.discard(var)\n",
    "                \n",
    "        return list(refined_variations)\n",
    "\n",
    "    def _refine_org_cluster(self, main_entity: str, variations: List[str]) -> List[str]:\n",
    "        \"\"\"Apply organization-specific refinement rules\"\"\"\n",
    "        main_tokens = set(self.nlp(main_entity.lower()))\n",
    "        refined_variations = set()\n",
    "        \n",
    "        for var in variations:\n",
    "            var_tokens = set(self.nlp(var.lower()))\n",
    "            # Keep if significant token overlap\n",
    "            if len(main_tokens & var_tokens) / len(main_tokens) > 0.3:\n",
    "                refined_variations.add(var)\n",
    "                \n",
    "        return list(refined_variations)\n",
    "\n",
    "    def _extract_entities(self, text: str) -> Dict[str, List[Dict[str, str]]]:\n",
    "        \"\"\"Extract named entities from text chunk\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        labels = [self.label_list[p] for p in predictions[0]]\n",
    "        \n",
    "        entities = {\n",
    "            'PERSON': [],\n",
    "            'ORGANIZATION': [],\n",
    "            'LOCATION': [],\n",
    "            'MISCELLANEOUS': []\n",
    "        }\n",
    "        \n",
    "        current_entity = {'type': None, 'text': ''}\n",
    "        \n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity['text']:\n",
    "                    self._add_entity(entities, current_entity)\n",
    "                current_entity = {\n",
    "                    'type': label[2:],\n",
    "                    'text': token.replace('##', '')\n",
    "                }\n",
    "            elif label.startswith('I-') and current_entity['text']:\n",
    "                current_entity['text'] += token.replace('##', '')\n",
    "            elif label == 'O':\n",
    "                if current_entity['text']:\n",
    "                    self._add_entity(entities, current_entity)\n",
    "                current_entity = {'type': None, 'text': ''}\n",
    "        \n",
    "        if current_entity['text']:\n",
    "            self._add_entity(entities, current_entity)\n",
    "            \n",
    "        return entities\n",
    "\n",
    "    def _split_into_chunks(self, text: str, max_length: int = 400) -> List[str]:\n",
    "        \"\"\"Split text into processable chunks\"\"\"\n",
    "        sentences = re.split('([.!?])', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) < max_length:\n",
    "                current_chunk += sentence\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def _add_entity(self, entities: Dict[str, List[Dict[str, str]]], \n",
    "                   entity: Dict[str, str]) -> None:\n",
    "        \"\"\"Add entity to appropriate category\"\"\"\n",
    "        if not entity['type']:\n",
    "            return\n",
    "            \n",
    "        entity_text = entity['text'].strip()\n",
    "        if not entity_text:\n",
    "            return\n",
    "            \n",
    "        entity_type = entity['type']\n",
    "        entity_dict = {'text': entity_text, 'type': entity_type}\n",
    "        \n",
    "        if entity_type == 'PER':\n",
    "            entities['PERSON'].append(entity_dict)\n",
    "        elif entity_type == 'ORG':\n",
    "            entities['ORGANIZATION'].append(entity_dict)\n",
    "        elif entity_type == 'LOC':\n",
    "            entities['LOCATION'].append(entity_dict)\n",
    "        elif entity_type == 'MISC':\n",
    "            entities['MISCELLANEOUS'].append(entity_dict)\n",
    "\n",
    "def process_document(file_path: str) -> None:\n",
    "    \"\"\"Process document and print grouped entities\"\"\"\n",
    "    processor = BertNERProcessor()\n",
    "    try:\n",
    "        entities = processor.process_docx(file_path)\n",
    "        \n",
    "        print(\"\\nExtracted and Grouped Named Entities:\")\n",
    "        print(\"-----------------------------------\")\n",
    "        \n",
    "        for category, clusters in entities.items():\n",
    "            if clusters:\n",
    "                print(f\"\\n{category}:\")\n",
    "                for idx, cluster in enumerate(clusters, 1):\n",
    "                    print(f\"\\nGroup {idx}:\")\n",
    "                    print(f\"Main: {cluster['main']}\")\n",
    "                    if cluster['variations']:\n",
    "                        print(\"Variations:\")\n",
    "                        for var in cluster['variations']:\n",
    "                            print(f\"- {var}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"path/to/your/document.docx\"\n",
    "    process_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =1 \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL \n",
    "pays = \"FRANCE1234\"\n",
    "F R A N C E 1 2 3 4\n",
    "1 2 3 4 5 6 7 8 9 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON \n",
    "pays = \"FRANCE1234\"\n",
    "F R A N C E 1 2 3 4\n",
    "0 1 2 3 4 5 6 7 8 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phillipe\n",
      "Mohammed\n",
      "les\n"
     ]
    }
   ],
   "source": [
    "# python \n",
    "eleves_list = [\"phillipe\", \"Mohammed\", \"les\"]\n",
    "\n",
    "for eleve in eleves_list:\n",
    "    print(eleve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP1: voici la liste des eleves: ['phillipe', 'Mohammed'] se termine ici.\n",
      "hello world\n",
      "les\n"
     ]
    }
   ],
   "source": [
    "eleves_list = [\"phillipe\", \"Mohammed\"]\n",
    "print(\"STEP1: voici la liste des eleves:\", eleves_list, \"se termine ici.\")\n",
    "print(\"hello world\")\n",
    "print(\"les\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for numero in range(1,10):\n",
    "    print(numero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age           City  Salary Department  Performance  Hire_Date\n",
      "0    Alice   25       New York   50000         HR          4.5 2022-01-01\n",
      "1      Bob   30  San Francisco   75000         IT          4.2 2022-01-02\n",
      "2  Charlie   35        Chicago   60000    Finance          4.7 2022-01-03\n",
      "3    David   28         Boston   65000  Marketing          3.9 2022-01-04\n",
      "4      Eve   22        Seattle   45000      Sales          4.1 2022-01-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 22],\n",
    "    'City': ['New York', 'San Francisco', 'Chicago', 'Boston', 'Seattle'],\n",
    "    'Salary': [50000, 75000, 60000, 65000, 45000],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'Marketing', 'Sales'],\n",
    "    'Performance': [4.5, 4.2, 4.7, 3.9, 4.1]\n",
    "})\n",
    "\n",
    "# Optionally, add a date column\n",
    "df['Hire_Date'] = pd.date_range(start='2022-01-01', periods=5)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "list_a = [1,3,4]\n",
    "\n",
    "if list_a[1] != 5 and list_a[0]!=8:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Name                         Alice\n",
      "Age                             25\n",
      "City                      New York\n",
      "Salary                       50000\n",
      "Department                      HR\n",
      "Performance                    4.5\n",
      "Hire_Date      2022-01-01 00:00:00\n",
      "Name: 0, dtype: object\n",
      "1 Name                           Bob\n",
      "Age                             30\n",
      "City                 San Francisco\n",
      "Salary                       75000\n",
      "Department                      IT\n",
      "Performance                    4.2\n",
      "Hire_Date      2022-01-02 00:00:00\n",
      "Name: 1, dtype: object\n",
      "2 Name                       Charlie\n",
      "Age                             35\n",
      "City                       Chicago\n",
      "Salary                       60000\n",
      "Department                 Finance\n",
      "Performance                    4.7\n",
      "Hire_Date      2022-01-03 00:00:00\n",
      "Name: 2, dtype: object\n",
      "3 Name                         David\n",
      "Age                             28\n",
      "City                        Boston\n",
      "Salary                       65000\n",
      "Department               Marketing\n",
      "Performance                    3.9\n",
      "Hire_Date      2022-01-04 00:00:00\n",
      "Name: 3, dtype: object\n",
      "4 Name                           Eve\n",
      "Age                             22\n",
      "City                       Seattle\n",
      "Salary                       45000\n",
      "Department                   Sales\n",
      "Performance                    4.1\n",
      "Hire_Date      2022-01-05 00:00:00\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories_article = [\"col1\", \"col2\", \"col3\"]\n",
    "# Create the activities table if it doesn't exist using BOOLEAN\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS activities (\n",
    "    entity TEXT,\n",
    "    activities BOOLEAN,\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query)\n",
    "\n",
    "# Create the table_query_db if it doesn't exist\n",
    "create_table_query_db = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS table_query_db (\n",
    "    entity TEXT,\n",
    "    activities BOOLEAN,\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query_db)\n",
    "\n",
    "insert_data = []\n",
    "for row in dr.iterrows():\n",
    "    step_data = row[1]\n",
    "    entity = step_data[\"entity\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    summary = step_data[\"summary\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    activities = tuple(bool(row[\"activity\"]) for activity in categories_article if activity is \"no label\") # Convert activities to boolean\n",
    "    current_timestamp = datetime.now().isoformat() # Get current timestamp in ISO format\n",
    "    comments = row[\"comments\"].replace(\"'\", \"''\")\n",
    "    flagged = bool(row[\"flagged\"])\n",
    "\n",
    "    data_row = (entity, activities, current_timestamp, comments, flagged)\n",
    "\n",
    "    # Check if the last entry for this entity is different\n",
    "    query = f\"SELECT * FROM table_query_db WHERE entity = %s ORDER BY timestamp DESC LIMIT 1\"\n",
    "    last_entry = conn.execute(query, (entity,)).fetchone()\n",
    "\n",
    "    # Check if there is a change in the activities\n",
    "    if last_entry:\n",
    "        if last_entry[1] != activities: # Exclude entity and timestamp for comparison\n",
    "            insert_data.append(data_row)\n",
    "    else:\n",
    "        insert_data.append(data_row) # This\n",
    "\n",
    "if insert_data:\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO {table_name} VALUES ({', '.join(['?'] * (len(categories_article) + 4))})\n",
    "    \"\"\"\n",
    "    conn.executemany(query,insert_data)\n",
    "    conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the activities table if it doesn't exist using BOOLEAN\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS activities (\n",
    "    entity TEXT,\n",
    "    {\",\".join([f'\"{activity}\" BOOLEAN' for activity in categories_article if activity != 'no label'])},\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query)\n",
    "# Create the table_query_db if it doesn't exist\n",
    "create_table_query_db = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS table_query_db (\n",
    "    entity TEXT,\n",
    "    {\",\".join([f'\"{activity}\" BOOLEAN' for activity in categories_article if activity != 'no label'])},\n",
    "    timestamp TIMESTAMP,\n",
    "    comments TEXT,\n",
    "    flagged BOOLEAN,\n",
    "    PRIMARY KEY(entity, timestamp)\n",
    ")\n",
    "\"\"\"\n",
    "conn.execute(create_table_query_db)\n",
    "insert_data = []\n",
    "for row in dr.iterrows():\n",
    "    step_data = row[1]\n",
    "    entity = step_data[\"entity\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    summary = step_data[\"summary\"].replace(\"'\", \"''\") # Escape single quotes\n",
    "    activities = tuple(bool(row[\"activity\"]) for activity in categories_article if activity is \"no label\") # Convert activities to boolean\n",
    "    current_timestamp = datetime.now().isoformat() # Get current timestamp in ISO format\n",
    "    comments = row[\"comments\"].replace(\"'\", \"''\")\n",
    "    flagged = bool(row[\"flagged\"])\n",
    "    data_row = (entity, activities, current_timestamp, comments, flagged)\n",
    "    # Check if the last entry for this entity is different\n",
    "    query = f\"SELECT * FROM table_query_db WHERE entity = %s ORDER BY timestamp DESC LIMIT 1\"\n",
    "    last_entry = conn.execute(query, (entity,)).fetchone()\n",
    "    # Check if there is a change in the activities\n",
    "    if last_entry:\n",
    "        if last_entry[1] != activities: # Exclude entity and timestamp for comparison\n",
    "            insert_data.append(data_row)\n",
    "    else:\n",
    "        insert_data.append(data_row) # This is a new entry for an entity\n",
    "\n",
    "if insert_data:\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO {table_name} VALUES ({', '.join(['?'] * (len(categories_article) + 4))})\n",
    "    \"\"\"\n",
    "    conn.executemany(query,insert_data)\n",
    "    conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387209.73000000004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range: \n",
    "    print(i)\n",
    "    def(in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in ./berserk3/lib/python3.12/site-packages (2024.11.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:\n",
      "Hello, Wörld! 123. Grüße\n",
      "\n",
      "Processed String:\n",
      "Hello  Wörld  123  Grüße\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# Step 1: Define the input string\n",
    "input_string = \"Hello, Wörld! 123. Grüße\"\n",
    "\n",
    "print(\"Original String:\")\n",
    "print(input_string)\n",
    "\n",
    "# Step 2: Replace non-alphanumeric characters with whitespace\n",
    "# Use the Unicode property \\p{L} to match any kind of letter and \\p{N} to match any kind of number\n",
    "output_string = re.sub(r'[^\\p{L}\\p{N}]', ' ', input_string)\n",
    "\n",
    "print(\"\\nProcessed String:\")\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:\n",
      "$2.8M\n",
      "\n",
      "Processed String:\n",
      " 2.8M\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Define the input string\n",
    "input_string = \"$2.8M\"\n",
    "\n",
    "print(\"Original String:\")\n",
    "print(input_string)\n",
    "\n",
    "# Step 2: Replace non-alphanumeric characters except dots, commas, and dollar signs with whitespace\n",
    "output_string = re.sub(r'[^a-zA-Z0-9.,]', ' ', input_string)\n",
    "\n",
    "print(\"\\nProcessed String:\")\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "   # Pattern includes Latin alphabet extensions used in European languages\n",
    "   pattern = r'[^a-zA-ZàáâäãåąčćęèéêëėįìíîïłńòóôöõøùúûüųūÿýżźñçčšžÀÁÂÄÃÅĄĆČĖĘÈÉÊËÌÍÎÏĮŁŃÒÓÔÖÕØÙÚÛÜŲŪŸÝŻŹÑßÇČŠŽ0-9.,]'\n",
    "   return re.sub(pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "import json\n",
    "\n",
    "def load_graph_data():\n",
    "    # Load your JSON files\n",
    "    try:\n",
    "        with open('nodes.json', 'r') as f:\n",
    "            nodes = json.load(f)\n",
    "        with open('links.json', 'r') as f:\n",
    "            links = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        # Default data if files not found\n",
    "        nodes = [\n",
    "            {\"id\": \"Person1\", \"type\": \"person\"},\n",
    "            {\"id\": \"Person2\", \"type\": \"person\"},\n",
    "            {\"id\": \"Person3\", \"type\": \"person\"},\n",
    "            {\"id\": \"CompanyA\", \"type\": \"company\"},\n",
    "            {\"id\": \"CompanyB\", \"type\": \"company\"}\n",
    "        ]\n",
    "        links = [\n",
    "            {\"source\": \"Person1\", \"target\": \"CompanyA\"},\n",
    "            {\"source\": \"Person2\", \"target\": \"CompanyA\"},\n",
    "            {\"source\": \"Person2\", \"target\": \"CompanyB\"},\n",
    "            {\"source\": \"Person3\", \"target\": \"CompanyB\"}\n",
    "        ]\n",
    "    return {\"nodes\": nodes, \"links\": links}\n",
    "\n",
    "def force_graph(graph_data):\n",
    "    html = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js\"></script>\n",
    "        <style>\n",
    "            .node-person { fill: #69b3a2; }\n",
    "            .node-company { fill: #404080; }\n",
    "            .link { stroke: #999; stroke-opacity: 0.6; }\n",
    "            .node-label { font-size: 12px; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div id=\"graph\"></div>\n",
    "        <script>\n",
    "            const data = \"\"\" + json.dumps(graph_data) + \"\"\";\n",
    "\n",
    "            const width = 1500;\n",
    "            const height = 800;\n",
    "            \n",
    "            const svg = d3.select(\"#graph\")\n",
    "                .append(\"svg\")\n",
    "                .attr(\"width\", width)\n",
    "                .attr(\"height\", height);\n",
    "\n",
    "            const simulation = d3.forceSimulation(data.nodes)\n",
    "                .force(\"link\", d3.forceLink(data.links).id(d => d.id))\n",
    "                .force(\"charge\", d3.forceManyBody().strength(-400))\n",
    "                .force(\"center\", d3.forceCenter(width / 2, height / 2))\n",
    "                .force(\"y\", d3.forceY(d => d.type === \"company\" ? height * 0.7 : height * 0.3).strength(1));\n",
    "\n",
    "            const link = svg.append(\"g\")\n",
    "                .selectAll(\"line\")\n",
    "                .data(data.links)\n",
    "                .join(\"line\")\n",
    "                .attr(\"class\", \"link\");\n",
    "\n",
    "            const node = svg.append(\"g\")\n",
    "                .selectAll(\"circle\")\n",
    "                .data(data.nodes)\n",
    "                .join(\"circle\")\n",
    "                .attr(\"class\", d => \"node-\" + d.type)\n",
    "                .attr(\"r\", d => d.type === \"company\" ? 25 : 20)\n",
    "                .call(drag(simulation));\n",
    "\n",
    "            const labels = svg.append(\"g\")\n",
    "                .selectAll(\"text\")\n",
    "                .data(data.nodes)\n",
    "                .join(\"text\")\n",
    "                .attr(\"class\", \"node-label\")\n",
    "                .text(d => d.id)\n",
    "                .attr(\"dx\", 0)\n",
    "                .attr(\"dy\", 25)\n",
    "                .attr(\"text-anchor\", \"middle\");\n",
    "\n",
    "            simulation.on(\"tick\", () => {\n",
    "                link\n",
    "                    .attr(\"x1\", d => d.source.x)\n",
    "                    .attr(\"y1\", d => d.source.y)\n",
    "                    .attr(\"x2\", d => d.target.x)\n",
    "                    .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "                node\n",
    "                    .attr(\"cx\", d => d.x)\n",
    "                    .attr(\"cy\", d => d.y);\n",
    "\n",
    "                labels\n",
    "                    .attr(\"x\", d => d.x)\n",
    "                    .attr(\"y\", d => d.y);\n",
    "            });\n",
    "\n",
    "            function drag(simulation) {\n",
    "                function dragstarted(event) {\n",
    "                    if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "                    event.subject.fx = event.subject.x;\n",
    "                    event.subject.fy = event.subject.y;\n",
    "                }\n",
    "                \n",
    "                function dragged(event) {\n",
    "                    event.subject.fx = event.x;\n",
    "                    event.subject.fy = event.y;\n",
    "                }\n",
    "                \n",
    "                function dragended(event) {\n",
    "                    if (!event.active) simulation.alphaTarget(0);\n",
    "                    // Nodes will stay where they are dragged\n",
    "                }\n",
    "                \n",
    "                return d3.drag()\n",
    "                    .on(\"start\", dragstarted)\n",
    "                    .on(\"drag\", dragged)\n",
    "                    .on(\"end\", dragended);\n",
    "            }\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    components.html(html, height=850)\n",
    "\n",
    "def main():\n",
    "    st.title(\"People-Company Network Graph\")\n",
    "    graph_data = load_graph_data()\n",
    "    force_graph(graph_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_dict(data_dict):\n",
    "   result = data_dict.copy()\n",
    "   \n",
    "   for entity, linked in data_dict.items():\n",
    "       for linked_entity in linked:\n",
    "           if linked_entity not in result:\n",
    "               result[linked_entity] = [e for e, l in data_dict.items() \n",
    "                                      if linked_entity in l]\n",
    "   \n",
    "   return result\n",
    "\n",
    "# Example\n",
    "data = {'entity1': ['entity2', 'entity3']}\n",
    "result = create_bidirectional_dict(data)\n",
    "result = create_bidirectional_dict(data)\n",
    "result = create_bidirectional_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    entity   linked_entities\n",
      "0  entity1  entity2, entity3\n",
      "1  entity2           entity1\n",
      "2  entity3           entity1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_bidirectional_df(data_dict):\n",
    "    rows = []\n",
    "    \n",
    "    # Add initial relationships\n",
    "    for entity, linked in data_dict.items():\n",
    "        rows.append({\n",
    "            'entity': entity,\n",
    "            'linked_entities': ', '.join(linked)\n",
    "        })\n",
    "        \n",
    "        # Add bidirectional relationships for each linked entity\n",
    "        for linked_entity in linked:\n",
    "            # Get all entities that this one is linked to\n",
    "            related = []\n",
    "            for e, l in data_dict.items():\n",
    "                if linked_entity in l:\n",
    "                    related.append(e)\n",
    "                if e == linked_entity:\n",
    "                    related.extend(l)\n",
    "            \n",
    "            rows.append({\n",
    "                'entity': linked_entity,\n",
    "                'linked_entities': ', '.join(set(related))\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows).drop_duplicates()\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    'entity1': ['entity2', 'entity3']\n",
    "}\n",
    "df = create_bidirectional_df(data)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    entity   linked_entities\n",
      "0  entity1  entity2, entity3\n",
      "1  entity2           entity1\n",
      "2  entity3           entity1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_bidirectional_df(data_dict):\n",
    "    rows = []\n",
    "    \n",
    "    # Add initial relationships\n",
    "    for entity, linked in data_dict.items():\n",
    "        rows.append({\n",
    "            'entity': entity,\n",
    "            'linked_entities': ', '.join(linked)\n",
    "        })\n",
    "        \n",
    "        # Add bidirectional relationships for each linked entity\n",
    "        for linked_entity in linked:\n",
    "            # Get all entities that this one is linked to\n",
    "            related = []\n",
    "            for e, l in data_dict.items():\n",
    "                if linked_entity in l:\n",
    "                    related.append(e)\n",
    "                if e == linked_entity:\n",
    "                    related.extend(l)\n",
    "            \n",
    "            rows.append({\n",
    "                'entity': linked_entity,\n",
    "                'linked_entities': ', '.join(set(related))\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows).drop_duplicates()\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    'entity1': ['entity2', 'entity3']\n",
    "}\n",
    "df = create_bidirectional_df(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    entity   linked_entities\n",
      "0  entity1  entity2, entity3\n",
      "1  entity2  entity3, entity1\n",
      "2  entity3  entity2, entity1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_bidirectional_df(data_dict):\n",
    "    rows = []\n",
    "    for entity, linked in data_dict.items():\n",
    "        rows.append({\n",
    "            'entity': entity, \n",
    "            'linked_entities': ', '.join(linked)\n",
    "        })\n",
    "        \n",
    "        # Add rows for linked entities\n",
    "        for linked_entity in linked:\n",
    "            other_entities = [e for e in linked if e != linked_entity]\n",
    "            if entity not in other_entities:\n",
    "                other_entities.append(entity)\n",
    "            rows.append({\n",
    "                'entity': linked_entity,\n",
    "                'linked_entities': ', '.join(other_entities)\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(rows).drop_duplicates()\n",
    "\n",
    "# Example\n",
    "data = {'entity1': ['entity2', 'entity3']}\n",
    "df = create_bidirectional_df(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create DataFrames\n",
    "left_df = pd.DataFrame({\n",
    "    'key': [1, 2, 3, 4, 5],\n",
    "    'value': ['A', 'B', 'C', 'D', 'E']\n",
    "})\n",
    "\n",
    "right_df = pd.DataFrame({\n",
    "    'key': [3, 4, 5, 6, 7],\n",
    "    'value': ['C', 'D', 'E', 'F', 'G']\n",
    "})\n",
    "\n",
    "print(\"Left DataFrame:\")\n",
    "print(left_df)\n",
    "\n",
    "print(\"\\nRight DataFrame:\")\n",
    "print(right_df)\n",
    "\n",
    "# Step 2: Perform the left anti join\n",
    "merged_df = left_df.merge(right_df, on=['key', 'value'], how='left', indicator=True)\n",
    "left_anti_join_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "print(\"\\nLeft Anti Join Result:\")\n",
    "print(left_anti_join_dfon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   id              list_column\n",
      "0   1  [apple, banana, cherry]\n",
      "1   2          [dog, elephant]\n",
      "2   3      [fish, goat, horse]\n",
      "\n",
      "DataFrame with String Column:\n",
      "   id              list_column        string_column\n",
      "0   1  [apple, banana, cherry]  apple banana cherry\n",
      "1   2          [dog, elephant]         dog elephant\n",
      "2   3      [fish, goat, horse]      fish goat horse\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create a DataFrame\n",
    "data = {\n",
    "    'id': [1, 2, 3],\n",
    "    'list_column': [['apple', 'banana', 'cherry'], ['dog', 'elephant'], ['fish', 'goat', 'horse']]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Step 2: Convert list column to string column\n",
    "df['string_column'] = df['list_column'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\"\\nDataFrame with String Column:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"source\": \"Person1\",\n",
    "        \"target\": \"CompanyA\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"Person2\",\n",
    "        \"target\": \"CompanyA\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Upload JSON file\", type=['json'])\n",
    "    \n",
    "    if uploaded_file:\n",
    "        data = json.load(uploaded_file)\n",
    "        \n",
    "        html_content = f\"\"\"\n",
    "            <html>\n",
    "                <head>\n",
    "                    <script src=\"https://unpkg.com/3d-force-graph\"></script>\n",
    "                </head>\n",
    "                <body>\n",
    "                    <div id=\"3d-graph\" style=\"width: 100%; height: 600px;\"></div>\n",
    "                    <script>\n",
    "                        const myDict = {json.dumps(data)};\n",
    "                        const graphData = {{\n",
    "                            nodes: [],\n",
    "                            links: []\n",
    "                        }};\n",
    "                        \n",
    "                        function processDict(dict, parent = null) {{\n",
    "                            Object.entries(dict).forEach(([key, value]) => {{\n",
    "                                if (!graphData.nodes.find(n => n.id === key)) {{\n",
    "                                    graphData.nodes.push({{id: key, name: key, group: 1}});\n",
    "                                }}\n",
    "                                \n",
    "                                if (parent) {{\n",
    "                                    graphData.links.push({{source: parent, target: key}});\n",
    "                                }}ca de\n",
    "                                \n",
    "                                if (typeof value === 'object' && value !== null) {{\n",
    "                                    processDict(value, key);\n",
    "                                }} else {{\n",
    "                                    if (!graphData.nodes.find(n => n.id === value)) {{\n",
    "                                        graphData.nodes.push({{id: value, name: value, group: 2}});\n",
    "                                    }}\n",
    "                                    graphData.links.push({{source: key, target: value}});\n",
    "                                }}\n",
    "                            }});\n",
    "                        }}\n",
    "                        \n",
    "                        processDict(myDict);\n",
    "                        \n",
    "                        const graph = ForceGraph3D()\n",
    "                            .graphData(graphData)\n",
    "                            .nodeLabel('name')\n",
    "                            .nodeColor(node => node.group === 1 ? '#ff4444' : '#4444ff')\n",
    "                            .linkWidth(1)\n",
    "                            .linkOpacity(0.8)\n",
    "                            .backgroundColor('#ffffff')\n",
    "                            (document.getElementById('3d-graph'));\n",
    "                    </script>\n",
    "                </body>\n",
    "            </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        components.html(html_content, height=600)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "import json\n",
    "\n",
    "# Configure the page to use wide mode\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "def main():\n",
    "    # Remove default padding\n",
    "    st.markdown(\"\"\"\n",
    "        <style>\n",
    "            .block-container {\n",
    "                padding-top: 1rem;\n",
    "                padding-bottom: 0rem;\n",
    "                padding-left: 1rem;\n",
    "                padding-right: 1rem;\n",
    "            }\n",
    "        </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Upload JSON file\", type=['json'])\n",
    "    \n",
    "    if uploaded_file:\n",
    "        data = json.load(uploaded_file)\n",
    "        \n",
    "        html_content = f\"\"\"\n",
    "            <html>\n",
    "                <head>\n",
    "                    <script src=\"https://unpkg.com/3d-force-graph\"></script>\n",
    "                    <style>\n",
    "                        #3d-graph {{\n",
    "                            width: 100vw !important;\n",
    "                            height: 100vh !important;\n",
    "                            position: fixed;\n",
    "                            left: 0;\n",
    "                            top: 0;\n",
    "                        }}\n",
    "                    </style>\n",
    "                </head>\n",
    "                <body>\n",
    "                    <div id=\"3d-graph\"></div>\n",
    "                    <script>\n",
    "                        const myDict = {json.dumps(data)};\n",
    "                        const graphData = {{\n",
    "                            nodes: [],\n",
    "                            links: []\n",
    "                        }};\n",
    "                        \n",
    "                        function processDict(dict, parent = null) {{\n",
    "                            Object.entries(dict).forEach(([key, value]) => {{\n",
    "                                if (!graphData.nodes.find(n => n.id === key)) {{\n",
    "                                    graphData.nodes.push({{id: key, name: key, group: 1}});\n",
    "                                }}\n",
    "                                \n",
    "                                if (parent) {{\n",
    "                                    graphData.links.push({{source: parent, target: key}});\n",
    "                                }}\n",
    "                                \n",
    "                                if (typeof value === 'object' && value !== null) {{\n",
    "                                    processDict(value, key);\n",
    "                                }} else {{\n",
    "                                    if (!graphData.nodes.find(n => n.id === value)) {{\n",
    "                                        graphData.nodes.push({{id: value, name: value, group: 2}});\n",
    "                                    }}\n",
    "                                    graphData.links.push({{source: key, target: value}});\n",
    "                                }}\n",
    "                            }});\n",
    "                        }}\n",
    "                        \n",
    "                        processDict(myDict);\n",
    "                        \n",
    "                        const graph = ForceGraph3D()\n",
    "                            .graphData(graphData)\n",
    "                            .nodeLabel('name')\n",
    "                            .nodeColor(node => node.group === 1 ? '#ff4444' : '#4444ff')\n",
    "                            .linkWidth(1)\n",
    "                            .linkOpacity(0.8)\n",
    "                            .backgroundColor('#ffffff')\n",
    "                            (document.getElementById('3d-graph'));\n",
    "                            \n",
    "                        // Adjust graph size on window resize\n",
    "                        window.addEventListener('resize', () => {\n",
    "                            graph.width(window.innerWidth)\n",
    "                                .height(window.innerHeight);\n",
    "                        });\n",
    "                    </script>\n",
    "                </body>\n",
    "            </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        components.html(html_content, height=800)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "define a new prompt because this one is keeping entity like \"officials\", \"senior officials\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity resolution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sample data...\n",
      "\n",
      "Input data:\n",
      "   id        name          address     phone\n",
      "0   1  John Smith      123 Main St  555-0123\n",
      "1   2   Jon Smith  123 Main Street   5550123\n",
      "2   3    Jane Doe      456 Oak Ave  555-4567\n",
      "3   4    J. Smith     123 Main St.  555-0123\n",
      "\n",
      "Finding matches...\n",
      "Creating blocking keys...\n",
      "\n",
      "Blocking keys created:\n",
      "  blocking_key        name\n",
      "0          joh  John Smith\n",
      "1          jon   Jon Smith\n",
      "2          jan    Jane Doe\n",
      "3          j s    J. Smith\n",
      "\n",
      "Grouping records...\n",
      "\n",
      "Comparing records...\n",
      "\n",
      "No matches found!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Simple text cleaning function\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def calculate_similarity(str1, str2):\n",
    "    \"\"\"Calculate string similarity using SequenceMatcher\"\"\"\n",
    "    # Clean both strings\n",
    "    str1_clean = clean_text(str1)\n",
    "    str2_clean = clean_text(str2)\n",
    "    return SequenceMatcher(None, str1_clean, str2_clean).ratio()\n",
    "\n",
    "def create_blocking_key(row, blocking_fields):\n",
    "    \"\"\"Create a simple blocking key from the first 3 letters of each blocking field\"\"\"\n",
    "    key_parts = []\n",
    "    for field in blocking_fields:\n",
    "        if field in row and pd.notna(row[field]):\n",
    "            # Get first 3 letters of cleaned value\n",
    "            cleaned_value = clean_text(str(row[field]))\n",
    "            key_parts.append(cleaned_value[:3])\n",
    "    return '_'.join(key_parts)\n",
    "\n",
    "def find_matches(df, blocking_fields, comparison_fields, threshold=0.7):\n",
    "    \"\"\"Main function to find matching entities\"\"\"\n",
    "    \n",
    "    # Step 1: Create blocking keys\n",
    "    print(\"Creating blocking keys...\")\n",
    "    df['blocking_key'] = df.apply(lambda x: create_blocking_key(x, blocking_fields), axis=1)\n",
    "    print(\"\\nBlocking keys created:\")\n",
    "    print(df[['blocking_key'] + blocking_fields])\n",
    "    \n",
    "    # Step 2: Group by blocking key\n",
    "    print(\"\\nGrouping records...\")\n",
    "    blocks = df.groupby('blocking_key')\n",
    "    \n",
    "    # Step 3: Compare records within blocks\n",
    "    matches = []\n",
    "    print(\"\\nComparing records...\")\n",
    "    \n",
    "    for block_key, block in blocks:\n",
    "        if len(block) > 1:  # Only process blocks with multiple records\n",
    "            print(f\"\\nProcessing block: {block_key} with {len(block)} records\")\n",
    "            # Compare all pairs in the block\n",
    "            for i in range(len(block)):\n",
    "                for j in range(i + 1, len(block)):\n",
    "                    record1 = block.iloc[i]\n",
    "                    record2 = block.iloc[j]\n",
    "                    \n",
    "                    # Calculate similarity for each field\n",
    "                    field_similarities = {}\n",
    "                    for field in comparison_fields:\n",
    "                        if pd.notna(record1[field]) and pd.notna(record2[field]):\n",
    "                            similarity = calculate_similarity(str(record1[field]), str(record2[field]))\n",
    "                            field_similarities[field] = similarity\n",
    "                    \n",
    "                    if field_similarities:\n",
    "                        avg_similarity = np.mean(list(field_similarities.values()))\n",
    "                        \n",
    "                        # Debug print\n",
    "                        print(f\"\\nComparing records {record1.name} and {record2.name}:\")\n",
    "                        print(f\"Field similarities: {field_similarities}\")\n",
    "                        print(f\"Average similarity: {avg_similarity:.2f}\")\n",
    "                        \n",
    "                        if avg_similarity >= threshold:\n",
    "                            matches.append({\n",
    "                                'record1_id': record1.name,\n",
    "                                'record2_id': record2.name,\n",
    "                                'similarity': avg_similarity,\n",
    "                                'blocking_key': record1['blocking_key'],\n",
    "                                'field_similarities': field_similarities\n",
    "                            })\n",
    "    \n",
    "    # Convert matches to DataFrame\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    return matches_df\n",
    "\n",
    "# Test the code with sample data\n",
    "print(\"Creating sample data...\")\n",
    "data = {\n",
    "    'id': range(1, 5),\n",
    "    'name': ['John Smith', 'Jon Smith', 'Jane Doe', 'J. Smith'],\n",
    "    'address': ['123 Main St', '123 Main Street', '456 Oak Ave', '123 Main St.'],\n",
    "    'phone': ['555-0123', '5550123', '555-4567', '555-0123']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nInput data:\")\n",
    "print(df)\n",
    "\n",
    "# Define fields for blocking and comparison\n",
    "blocking_fields = ['name']\n",
    "comparison_fields = ['name', 'address', 'phone']\n",
    "\n",
    "# Find matches with detailed output\n",
    "print(\"\\nFinding matches...\")\n",
    "matches = find_matches(df, blocking_fields, comparison_fields, threshold=0.7)\n",
    "\n",
    "# Display results\n",
    "if len(matches) > 0:\n",
    "    print(\"\\nMatches found:\")\n",
    "    for _, match in matches.iterrows():\n",
    "        print(f\"\\nMatch pair (similarity: {match['similarity']:.2f}):\")\n",
    "        print(\"Record 1:\")\n",
    "        print(df.loc[match['record1_id']])\n",
    "        print(\"\\nRecord 2:\")\n",
    "        print(df.loc[match['record2_id']])\n",
    "else:\n",
    "    print(\"\\nNo matches found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      "   id        name          address     phone\n",
      "0   1  John Smith      123 Main St  555-0123\n",
      "1   2   Jon Smith  123 Main Street   5550123\n",
      "2   3    Jane Doe      456 Oak Ave  555-4567\n",
      "3   4    J. Smith     123 Main St.  555-0123\n",
      "\n",
      "==================================================\n",
      "\n",
      "Matching Results:\n",
      "\n",
      "Match Found:\n",
      "Record 1: ID: 1, Name: John Smith\n",
      "Record 2: ID: 2, Name: Jon Smith\n",
      "Overall Similarity: 0.93\n",
      "Field-by-field similarities:\n",
      "  name: 0.95\n",
      "  address: 0.85\n",
      "  phone: 1.00\n",
      "\n",
      "Match Found:\n",
      "Record 1: ID: 1, Name: John Smith\n",
      "Record 2: ID: 4, Name: J. Smith\n",
      "Overall Similarity: 0.94\n",
      "Field-by-field similarities:\n",
      "  name: 0.82\n",
      "  address: 1.00\n",
      "  phone: 1.00\n",
      "\n",
      "Match Found:\n",
      "Record 1: ID: 2, Name: Jon Smith\n",
      "Record 2: ID: 4, Name: J. Smith\n",
      "Overall Similarity: 0.91\n",
      "Field-by-field similarities:\n",
      "  name: 0.88\n",
      "  address: 0.85\n",
      "  phone: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Similarities for entity and adress \n",
    "\n",
    "# Import required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Simple text cleaning function\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def calculate_similarity(str1, str2):\n",
    "    \"\"\"Calculate string similarity using SequenceMatcher\"\"\"\n",
    "    str1_clean = clean_text(str1)\n",
    "    str2_clean = clean_text(str2)\n",
    "    return SequenceMatcher(None, str1_clean, str2_clean).ratio()\n",
    "\n",
    "def find_matches(df, comparison_fields, threshold=0.7):\n",
    "    \"\"\"Simplified matching function that compares all pairs\"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    # Compare all pairs\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i + 1, len(df)):\n",
    "            record1 = df.iloc[i]\n",
    "            record2 = df.iloc[j]\n",
    "            \n",
    "            # Calculate similarities for each field\n",
    "            similarities = []\n",
    "            for field in comparison_fields:\n",
    "                if pd.notna(record1[field]) and pd.notna(record2[field]):\n",
    "                    sim = calculate_similarity(str(record1[field]), str(record2[field]))\n",
    "                    similarities.append(sim)\n",
    "                    \n",
    "            if similarities:\n",
    "                avg_similarity = np.mean(similarities)\n",
    "                \n",
    "                if avg_similarity >= threshold:\n",
    "                    match = {\n",
    "                        'Record 1': f\"ID: {record1['id']}, Name: {record1['name']}\",\n",
    "                        'Record 2': f\"ID: {record2['id']}, Name: {record2['name']}\",\n",
    "                        'Similarity': f\"{avg_similarity:.2f}\",\n",
    "                        'Matching Fields': {\n",
    "                            field: f\"{calculate_similarity(str(record1[field]), str(record2[field])):.2f}\"\n",
    "                            for field in comparison_fields\n",
    "                        }\n",
    "                    }\n",
    "                    matches.append(match)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'id': range(1, 5),\n",
    "    'name': ['John Smith', 'Jon Smith', 'Jane Doe', 'J. Smith'],\n",
    "    'address': ['123 Main St', '123 Main Street', '456 Oak Ave', '123 Main St.'],\n",
    "    'phone': ['555-0123', '5550123', '555-4567', '555-0123']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Input Data:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Find matches\n",
    "comparison_fields = ['name', 'address', 'phone']\n",
    "matches = find_matches(df, comparison_fields, threshold=0.7)\n",
    "\n",
    "# Display results\n",
    "print(\"Matching Results:\")\n",
    "for match in matches:\n",
    "    print(\"\\nMatch Found:\")\n",
    "    print(f\"Record 1: {match['Record 1']}\")\n",
    "    print(f\"Record 2: {match['Record 2']}\")\n",
    "    print(f\"Overall Similarity: {match['Similarity']}\")\n",
    "    print(\"Field-by-field similarities:\")\n",
    "    for field, sim in match['Matching Fields'].items():\n",
    "        print(f\"  {field}: {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jellyfish\n",
      "  Using cached jellyfish-1.1.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Using cached jellyfish-1.1.3-cp312-cp312-macosx_11_0_arm64.whl (311 kB)\n",
      "Installing collected packages: jellyfish\n",
      "Successfully installed jellyfish-1.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      "         name                                        description\n",
      "0  John Smith  A software engineer with 10 years of experienc...\n",
      "1   Jon Smith  Experienced software developer specializing in...\n",
      "2    Jane Doe  Marketing professional with expertise in digit...\n",
      "3    J. Smith  Senior software engineer with Python and Java ...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Matching Results:\n",
      "\n",
      "Match Found:\n",
      "Record 1: ID: 0, Name: John Smith\n",
      "Record 2: ID: 3, Name: J. Smith\n",
      "Overall Similarity: 0.66\n",
      "Field-by-field similarities:\n",
      "  name: 0.82\n",
      "  description: 0.41\n",
      "\n",
      "Match Found:\n",
      "Record 1: ID: 1, Name: Jon Smith\n",
      "Record 2: ID: 3, Name: J. Smith\n",
      "Overall Similarity: 0.65\n",
      "Field-by-field similarities:\n",
      "  name: 0.88\n",
      "  description: 0.30\n"
     ]
    }
   ],
   "source": [
    "# with name and entity description\n",
    "\n",
    "# Import required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing special characters and standardizing format\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def calculate_name_similarity(name1, name2):\n",
    "    \"\"\"Calculate similarity between names using SequenceMatcher\"\"\"\n",
    "    name1_clean = clean_text(name1)\n",
    "    name2_clean = clean_text(name2)\n",
    "    return SequenceMatcher(None, name1_clean, name2_clean).ratio()\n",
    "\n",
    "def calculate_description_similarity(desc1, desc2):\n",
    "    \"\"\"Calculate similarity between descriptions using TF-IDF and cosine similarity\"\"\"\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        token_pattern=r'\\b\\w+\\b',  # Match whole words only\n",
    "        min_df=1  # Include all terms since we're only comparing two documents\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the descriptions\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([clean_text(desc1), clean_text(desc2)])\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        return float(similarity)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def find_matches(df, name_threshold=0.8, desc_threshold=0.3, name_weight=0.6):\n",
    "    \"\"\"Find matching entities based on name and description similarity\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'name' and 'description' columns\n",
    "        name_threshold: Minimum similarity threshold for names\n",
    "        desc_threshold: Minimum similarity threshold for descriptions\n",
    "        name_weight: Weight given to name similarity (1 - name_weight for description)\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    # Compare all pairs\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i + 1, len(df)):\n",
    "            record1 = df.iloc[i]\n",
    "            record2 = df.iloc[j]\n",
    "            \n",
    "            # Calculate name similarity\n",
    "            name_sim = calculate_name_similarity(record1['name'], record2['name'])\n",
    "            \n",
    "            # Only proceed if names are similar enough\n",
    "            if name_sim >= name_threshold:\n",
    "                # Calculate description similarity\n",
    "                desc_sim = calculate_description_similarity(\n",
    "                    record1['description'], \n",
    "                    record2['description']\n",
    "                )\n",
    "                \n",
    "                # Calculate weighted average similarity\n",
    "                weighted_sim = (name_sim * name_weight + \n",
    "                              desc_sim * (1 - name_weight))\n",
    "                \n",
    "                # Check if description similarity meets threshold\n",
    "                if desc_sim >= desc_threshold:\n",
    "                    match = {\n",
    "                        'Record 1': f\"ID: {record1.name}, Name: {record1['name']}\",\n",
    "                        'Record 2': f\"ID: {record2.name}, Name: {record2['name']}\",\n",
    "                        'Overall Similarity': f\"{weighted_sim:.2f}\",\n",
    "                        'Similarities': {\n",
    "                            'name': f\"{name_sim:.2f}\",\n",
    "                            'description': f\"{desc_sim:.2f}\"\n",
    "                        }\n",
    "                    }\n",
    "                    matches.append(match)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example usage with sample data\n",
    "data = {\n",
    "    'name': [\n",
    "        'John Smith',\n",
    "        'Jon Smith',\n",
    "        'Jane Doe',\n",
    "        'J. Smith'\n",
    "    ],\n",
    "    'description': [\n",
    "        'A software engineer with 10 years of experience in Python and Java development',\n",
    "        'Experienced software developer specializing in Python and Java programming',\n",
    "        'Marketing professional with expertise in digital campaigns',\n",
    "        'Senior software engineer with Python and Java background'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Input Data:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Find matches\n",
    "matches = find_matches(df, name_threshold=0.7, desc_threshold=0.3, name_weight=0.6)\n",
    "\n",
    "# Display results\n",
    "print(\"Matching Results:\")\n",
    "for match in matches:\n",
    "    print(\"\\nMatch Found:\")\n",
    "    print(f\"Record 1: {match['Record 1']}\")\n",
    "    print(f\"Record 2: {match['Record 2']}\")\n",
    "    print(f\"Overall Similarity: {match['Overall Similarity']}\")\n",
    "    print(\"Field-by-field similarities:\")\n",
    "    for field, sim in match['Similarities'].items():\n",
    "        print(f\"  {field}: {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "class EntityMatcher:\n",
    "    def __init__(self, embedding_model='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the entity matcher with specified embedding model\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Model name for sentence-transformers or 'openai' for OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        if embedding_model != 'openai':\n",
    "            # Load local sentence-transformers model\n",
    "            self.model = SentenceTransformer(embedding_model)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing special characters and standardizing format\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def calculate_name_similarity(self, name1, name2):\n",
    "        \"\"\"Calculate similarity between names using SequenceMatcher\"\"\"\n",
    "        name1_clean = self.clean_text(name1)\n",
    "        name2_clean = self.clean_text(name2)\n",
    "        return SequenceMatcher(None, name1_clean, name2_clean).ratio()\n",
    "\n",
    "    async def get_openai_embedding(self, text):\n",
    "        \"\"\"Get embeddings using OpenAI's API\"\"\"\n",
    "        try:\n",
    "            response = await openai.Embedding.acreate(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=text\n",
    "            )\n",
    "            return response['data'][0]['embedding']\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting OpenAI embedding: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_local_embedding(self, text):\n",
    "        \"\"\"Get embeddings using sentence-transformers\"\"\"\n",
    "        try:\n",
    "            embedding = self.model.encode(text, convert_to_tensor=True)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting local embedding: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_description_similarity(self, desc1, desc2):\n",
    "        \"\"\"Calculate similarity between descriptions using embeddings\"\"\"\n",
    "        # Clean descriptions\n",
    "        desc1_clean = self.clean_text(desc1)\n",
    "        desc2_clean = self.clean_text(desc2)\n",
    "        \n",
    "        # Get embeddings\n",
    "        if self.embedding_model == 'openai':\n",
    "            # Use OpenAI embeddings (requires async)\n",
    "            emb1 = await self.get_openai_embedding(desc1_clean)\n",
    "            emb2 = await self.get_openai_embedding(desc2_clean)\n",
    "        else:\n",
    "            # Use local model embeddings\n",
    "            emb1 = self.get_local_embedding(desc1_clean)\n",
    "            emb2 = self.get_local_embedding(desc2_clean)\n",
    "        \n",
    "        if emb1 is None or emb2 is None:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        if isinstance(emb1, torch.Tensor):\n",
    "            emb1 = emb1.cpu().numpy()\n",
    "            emb2 = emb2.cpu().numpy()\n",
    "        \n",
    "        similarity = cosine_similarity(\n",
    "            emb1.reshape(1, -1),\n",
    "            emb2.reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return float(similarity)\n",
    "\n",
    "    def find_matches(self, df, name_threshold=0.8, desc_threshold=0.7, name_weight=0.4):\n",
    "        \"\"\"Find matching entities based on name and description similarity\"\"\"\n",
    "        matches = []\n",
    "        total_comparisons = len(df) * (len(df) - 1) // 2\n",
    "        \n",
    "        print(f\"Processing {total_comparisons} comparisons...\")\n",
    "        \n",
    "        # Get all description embeddings first for efficiency\n",
    "        descriptions = df['description'].apply(self.clean_text).tolist()\n",
    "        if self.embedding_model != 'openai':\n",
    "            all_embeddings = self.model.encode(descriptions, convert_to_tensor=True)\n",
    "        \n",
    "        # Compare all pairs\n",
    "        for i in range(len(df)):\n",
    "            for j in range(i + 1, len(df)):\n",
    "                record1 = df.iloc[i]\n",
    "                record2 = df.iloc[j]\n",
    "                \n",
    "                # Calculate name similarity\n",
    "                name_sim = self.calculate_name_similarity(record1['name'], record2['name'])\n",
    "                \n",
    "                # Only proceed if names are similar enough\n",
    "                if name_sim >= name_threshold:\n",
    "                    # Calculate description similarity using cached embeddings\n",
    "                    if self.embedding_model != 'openai':\n",
    "                        desc_sim = float(cosine_similarity(\n",
    "                            all_embeddings[i].reshape(1, -1),\n",
    "                            all_embeddings[j].reshape(1, -1)\n",
    "                        )[0][0])\n",
    "                    else:\n",
    "                        desc_sim = self.calculate_description_similarity(\n",
    "                            record1['description'],\n",
    "                            record2['description']\n",
    "                        )\n",
    "                    \n",
    "                    # Calculate weighted similarity\n",
    "                    weighted_sim = (name_sim * name_weight + \n",
    "                                  desc_sim * (1 - name_weight))\n",
    "                    \n",
    "                    # Check if description similarity meets threshold\n",
    "                    if desc_sim >= desc_threshold:\n",
    "                        match = {\n",
    "                            'Record 1': f\"ID: {record1.name}, Name: {record1['name']}\",\n",
    "                            'Record 2': f\"ID: {record2.name}, Name: {record2['name']}\",\n",
    "                            'Overall Similarity': f\"{weighted_sim:.2f}\",\n",
    "                            'Similarities': {\n",
    "                                'name': f\"{name_sim:.2f}\",\n",
    "                                'description': f\"{desc_sim:.2f}\"\n",
    "                            },\n",
    "                            'Description 1': record1['description'],\n",
    "                            'Description 2': record2['description']\n",
    "                        }\n",
    "                        matches.append(match)\n",
    "        \n",
    "        return matches\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Sample data\n",
    "    data = {\n",
    "        'name': [\n",
    "            'John Smith',\n",
    "            'Jon Smith',\n",
    "            'Jane Doe',\n",
    "            'J. Smith'\n",
    "        ],\n",
    "        'description': [\n",
    "            'A software engineer with 10 years of experience in Python and Java development',\n",
    "            'Experienced software developer specializing in Python and Java programming',\n",
    "            'Marketing professional with expertise in digital campaigns',\n",
    "            'Senior software engineer with Python and Java background'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Input Data:\")\n",
    "    print(df)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Initialize matcher with chosen model\n",
    "    matcher = EntityMatcher(embedding_model='all-MiniLM-L6-v2')  # or 'openai' for OpenAI embeddings\n",
    "    \n",
    "    # Find matches\n",
    "    matches = matcher.find_matches(\n",
    "        df,\n",
    "        name_threshold=0.7,\n",
    "        desc_threshold=0.7,\n",
    "        name_weight=0.4  # Give more weight to description similarity\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"Matching Results:\")\n",
    "    for match in matches:\n",
    "        print(\"\\nMatch Found:\")\n",
    "        print(f\"Record 1: {match['Record 1']}\")\n",
    "        print(f\"Record 2: {match['Record 2']}\")\n",
    "        print(f\"Overall Similarity: {match['Overall Similarity']}\")\n",
    "        print(\"Field-by-field similarities:\")\n",
    "        for field, sim in match['Similarities'].items():\n",
    "            print(f\"  {field}: {sim}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import openai\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class LLMEntityMatcher:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize the matcher with OpenAI API key\"\"\"\n",
    "        openai.api_key = api_key\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def calculate_name_similarity(self, name1: str, name2: str) -> float:\n",
    "        \"\"\"Calculate basic name similarity\"\"\"\n",
    "        name1_clean = self.clean_text(name1)\n",
    "        name2_clean = self.clean_text(name2)\n",
    "        return SequenceMatcher(None, name1_clean, name2_clean).ratio()\n",
    "\n",
    "    async def compare_descriptions(self, desc1: str, desc2: str) -> Dict:\n",
    "        \"\"\"Use LLM to directly compare two descriptions\"\"\"\n",
    "        prompt = f\"\"\"Compare these two entity descriptions and analyze their similarity:\n",
    "\n",
    "Description 1: {desc1}\n",
    "Description 2: {desc2}\n",
    "\n",
    "Provide a JSON response with:\n",
    "1. A similarity score between 0 and 1\n",
    "2. Key matching aspects\n",
    "3. Key differences\n",
    "4. Confidence in the comparison\n",
    "\n",
    "Format:\n",
    "{{\n",
    "    \"similarity_score\": float,\n",
    "    \"matching_aspects\": [str],\n",
    "    \"differences\": [str],\n",
    "    \"confidence\": float\n",
    "}}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = await openai.ChatCompletion.acreate(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in entity resolution and semantic analysis.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in LLM comparison: {e}\")\n",
    "            return {\n",
    "                \"similarity_score\": 0.0,\n",
    "                \"matching_aspects\": [],\n",
    "                \"differences\": [\"Error in comparison\"],\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "\n",
    "    async def find_matches(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        name_threshold: float = 0.7,\n",
    "        desc_threshold: float = 0.7,\n",
    "        name_weight: float = 0.3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Find matching entities using LLM comparison\"\"\"\n",
    "        matches = []\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            for j in range(i + 1, len(df)):\n",
    "                record1 = df.iloc[i]\n",
    "                record2 = df.iloc[j]\n",
    "                \n",
    "                # First check name similarity to filter obvious non-matches\n",
    "                name_sim = self.calculate_name_similarity(record1['name'], record2['name'])\n",
    "                \n",
    "                if name_sim >= name_threshold:\n",
    "                    # Get LLM comparison for descriptions\n",
    "                    llm_result = await self.compare_descriptions(\n",
    "                        record1['description'],\n",
    "                        record2['description']\n",
    "                    )\n",
    "                    \n",
    "                    desc_sim = llm_result['similarity_score']\n",
    "                    \n",
    "                    # Calculate weighted similarity\n",
    "                    weighted_sim = (name_sim * name_weight + \n",
    "                                  desc_sim * (1 - name_weight))\n",
    "                    \n",
    "                    if desc_sim >= desc_threshold:\n",
    "                        match = {\n",
    "                            'record1_id': record1.name,\n",
    "                            'record2_id': record2.name,\n",
    "                            'record1_name': record1['name'],\n",
    "                            'record2_name': record2['name'],\n",
    "                            'overall_similarity': weighted_sim,\n",
    "                            'name_similarity': name_sim,\n",
    "                            'description_similarity': desc_sim,\n",
    "                            'matching_aspects': llm_result['matching_aspects'],\n",
    "                            'differences': llm_result['differences'],\n",
    "                            'confidence': llm_result['confidence']\n",
    "                        }\n",
    "                        matches.append(match)\n",
    "        \n",
    "        return matches\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize matcher\n",
    "matcher = LLMEntityMatcher(api_key='your-api-key')\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'name': [\n",
    "        'John Smith',\n",
    "        'Jon Smith',\n",
    "        'Jane Doe',\n",
    "        'J. Smith'\n",
    "    ],\n",
    "    'description': [\n",
    "        'A software engineer with 10 years of experience in Python and Java development',\n",
    "        'Experienced software developer specializing in Python and Java programming',\n",
    "        'Marketing professional with expertise in digital campaigns',\n",
    "        'Senior software engineer with Python and Java background'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Find matches\n",
    "matches = await matcher.find_matches(\n",
    "    df,\n",
    "    name_threshold=0.7,\n",
    "    desc_threshold=0.7,\n",
    "    name_weight=0.3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for match in matches:\n",
    "    print(f\"\\nMatch Found (Overall Similarity: {match['overall_similarity']:.2f}):\")\n",
    "    print(f\"Record 1: {match['record1_name']}\")\n",
    "    print(f\"Record 2: {match['record2_name']}\")\n",
    "    print(\"\\nMatching Aspects:\")\n",
    "    for aspect in match['matching_aspects']:\n",
    "        print(f\"- {aspect}\")\n",
    "    print(\"\\nKey Differences:\")\n",
    "    for diff in match['differences']:\n",
    "        print(f\"- {diff}\")\n",
    "    print(f\"Confidence: {match['confidence']:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing special characters and standardizing format\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def prepare_dataframe(df):\n",
    "    \"\"\"Prepare DataFrame by adding cleaned versions while keeping originals\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_prepared = df.copy()\n",
    "    \n",
    "    # Add cleaned name while keeping original\n",
    "    df_prepared['name_cleaned'] = df_prepared['name'].apply(clean_text)\n",
    "    \n",
    "    # Add cleaned description while keeping original\n",
    "    df_prepared['description_cleaned'] = df_prepared['description'].apply(clean_text)\n",
    "    \n",
    "    return df_prepared\n",
    "\n",
    "def calculate_name_similarity(name1, name2):\n",
    "    \"\"\"Calculate similarity between names using SequenceMatcher\"\"\"\n",
    "    return SequenceMatcher(None, name1, name2).ratio()\n",
    "\n",
    "def calculate_description_similarity(desc1, desc2):\n",
    "    \"\"\"Calculate similarity between descriptions using TF-IDF and cosine similarity\"\"\"\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        token_pattern=r'\\b\\w+\\b',  # Match whole words only\n",
    "        min_df=1  # Include all terms since we're only comparing two documents\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the descriptions\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([desc1, desc2])\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        return float(similarity)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def find_matches(df, name_threshold=0.8, desc_threshold=0.3, name_weight=0.6):\n",
    "    \"\"\"Find matching entities based on name and description similarity\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'name' and 'description' columns\n",
    "        name_threshold: Minimum similarity threshold for names\n",
    "        desc_threshold: Minimum similarity threshold for descriptions\n",
    "        name_weight: Weight given to name similarity (1 - name_weight for description)\n",
    "    \"\"\"\n",
    "    # Prepare DataFrame with cleaned versions\n",
    "    df_prepared = prepare_dataframe(df)\n",
    "    matches = []\n",
    "    \n",
    "    # Compare all pairs\n",
    "    for i in range(len(df_prepared)):\n",
    "        for j in range(i + 1, len(df_prepared)):\n",
    "            record1 = df_prepared.iloc[i]\n",
    "            record2 = df_prepared.iloc[j]\n",
    "            \n",
    "            # Calculate name similarity using cleaned names\n",
    "            name_sim = calculate_name_similarity(\n",
    "                record1['name_cleaned'],\n",
    "                record2['name_cleaned']\n",
    "            )\n",
    "            \n",
    "            # Only proceed if names are similar enough\n",
    "            if name_sim >= name_threshold:\n",
    "                # Calculate description similarity using cleaned descriptions\n",
    "                desc_sim = calculate_description_similarity(\n",
    "                    record1['description_cleaned'],\n",
    "                    record2['description_cleaned']\n",
    "                )\n",
    "                \n",
    "                # Calculate weighted average similarity\n",
    "                weighted_sim = (name_sim * name_weight + \n",
    "                              desc_sim * (1 - name_weight))\n",
    "                \n",
    "                # Check if description similarity meets threshold\n",
    "                if desc_sim >= desc_threshold:\n",
    "                    match = {\n",
    "                        'Record 1': f\"ID: {record1.name}, Name: {record1['name']}\",  # Original name\n",
    "                        'Record 2': f\"ID: {record2.name}, Name: {record2['name']}\",  # Original name\n",
    "                        'Overall Similarity': f\"{weighted_sim:.2f}\",\n",
    "                        'Similarities': {\n",
    "                            'name': f\"{name_sim:.2f}\",\n",
    "                            'description': f\"{desc_sim:.2f}\"\n",
    "                        },\n",
    "                        'Original Names': {\n",
    "                            'name1': record1['name'],\n",
    "                            'name2': record2['name']\n",
    "                        },\n",
    "                        'Cleaned Names': {\n",
    "                            'name1': record1['name_cleaned'],\n",
    "                            'name2': record2['name_cleaned']\n",
    "                        }\n",
    "                    }\n",
    "                    matches.append(match)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example usage with sample data\n",
    "data = {\n",
    "    'name': [\n",
    "        'John Smith, PhD',\n",
    "        'Jon Smith Jr.',\n",
    "        'Jane Doe, MBA',\n",
    "        'J. Smith III'\n",
    "    ],\n",
    "    'description': [\n",
    "        'A software engineer with 10 years of experience in Python and Java development',\n",
    "        'Experienced software developer specializing in Python and Java programming',\n",
    "        'Marketing professional with expertise in digital campaigns',\n",
    "        'Senior software engineer with Python and Java background'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Input Data:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Find matches\n",
    "matches = find_matches(df, name_threshold=0.7, desc_threshold=0.3, name_weight=0.6)\n",
    "\n",
    "# Display results\n",
    "print(\"Matching Results:\")\n",
    "for match in matches:\n",
    "    print(\"\\nMatch Found:\")\n",
    "    print(f\"Record 1: {match['Record 1']}\")\n",
    "    print(f\"Record 2: {match['Record 2']}\")\n",
    "    print(f\"Overall Similarity: {match['Overall Similarity']}\")\n",
    "    print(\"Original Names:\")\n",
    "    print(f\"  Name 1: {match['Original Names']['name1']}\")\n",
    "    print(f\"  Name 2: {match['Original Names']['name2']}\")\n",
    "    print(\"Cleaned Names:\")\n",
    "    print(f\"  Name 1: {match['Cleaned Names']['name1']}\")\n",
    "    print(f\"  Name 2: {match['Cleaned Names']['name2']}\")\n",
    "    print(\"Field-by-field similarities:\")\n",
    "    for field, sim in match['Similarities'].items():\n",
    "        print(f\"  {field}: {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_connected_entries(dictionary):\n",
    "    \"\"\"\n",
    "    Merges dictionary entries that share common names.\n",
    "    \n",
    "    Args:\n",
    "        dictionary: A dictionary with keys mapping to lists of names\n",
    "        \n",
    "    Returns:\n",
    "        A new dictionary with connected entries merged\n",
    "    \"\"\"\n",
    "    # Map each name to all keys it appears in\n",
    "    name_to_keys = {}\n",
    "    for key, names in dictionary.items():\n",
    "        for name in names:\n",
    "            if name not in name_to_keys:\n",
    "                name_to_keys[name] = []\n",
    "            name_to_keys[name].append(key)\n",
    "    \n",
    "    # Track which keys have been processed\n",
    "    processed_keys = set()\n",
    "    result = {}\n",
    "    \n",
    "    # Process each key\n",
    "    for key in dictionary:\n",
    "        # Skip if already processed\n",
    "        if key in processed_keys:\n",
    "            continue\n",
    "        \n",
    "        # Start with current key and its names\n",
    "        connected_keys = [key]\n",
    "        connected_names = set(dictionary[key])\n",
    "        \n",
    "        # Find all connected keys through common names\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            \n",
    "            # For current set of names, find all related keys\n",
    "            for name in list(connected_names):\n",
    "                for related_key in name_to_keys[name]:\n",
    "                    if related_key not in connected_keys:\n",
    "                        connected_keys.append(related_key)\n",
    "                        # Add all names from this key\n",
    "                        for related_name in dictionary[related_key]:\n",
    "                            if related_name not in connected_names:\n",
    "                                connected_names.add(related_name)\n",
    "                                changed = True\n",
    "        \n",
    "        # Mark all these keys as processed\n",
    "        for k in connected_keys:\n",
    "            processed_keys.add(k)\n",
    "        \n",
    "        # Add to result using the first key\n",
    "        result[key] = list(connected_names)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    dic_a = {\n",
    "        \"match1\": [\"name1\", \"name2\"],\n",
    "        \"match2\": [\"name2\", \"name3\"],\n",
    "        \"match3\": [\"name5\", \"name4\"]\n",
    "    }\n",
    "    \n",
    "    new_dic_a = merge_connected_entries(dic_a)\n",
    "    print(\"Original dictionary:\", dic_a)\n",
    "    print(\"New dictionary:\", new_dic_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_one_only(pairs, flagged_entities):\n",
    "    one_only = []\n",
    "    for pair in pairs:\n",
    "        entity1, entity2 = pair\n",
    "        if (entity1 in flagged_entities) != (entity2 in flagged_entities):\n",
    "            one_only.append(pair)\n",
    "    return one_only\n",
    "\n",
    "# Example usage:\n",
    "pairs = [(\"A\", \"B\"), (\"C\", \"D\"), (\"E\", \"F\"), (\"G\", \"H\")]\n",
    "flagged_entities = [\"A\", \"C\", \"E\", \"H\"]\n",
    "\n",
    "result = filter_one_only(pairs, flagged_entities)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'B'), ('E', 'F'), ('G', 'H')]\n"
     ]
    }
   ],
   "source": [
    "pairs = [(\"A\", \"B\"), (\"C\", \"D\"), (\"E\", \"F\"), (\"G\", \"H\")]\n",
    "flagged_entities = {\"A\", \"E\", \"G\"}\n",
    "\n",
    "one_only = [pair for pair in pairs if (pair[0] in flagged_entities) ^ (pair[1] in flagged_entities)]\n",
    "\n",
    "print(one_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'B'), ('E', 'F'), ('G', 'H')]\n"
     ]
    }
   ],
   "source": [
    "pairs = [(\"B\", \"A\"), (\"C\", \"D\"), (\"E\", \"F\"), (\"G\", \"H\")]\n",
    "flagged_entities = {\"A\", \"E\", \"G\"}\n",
    "\n",
    "one_only = [\n",
    "    (pair[0], pair[1]) if pair[0] in flagged_entities else (pair[1], pair[0])\n",
    "    for pair in pairs\n",
    "    if (pair[0] in flagged_entities) ^ (pair[1] in flagged_entities)\n",
    "]\n",
    "\n",
    "print(one_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = {\n",
    "    \"desc1\":{\n",
    "        \"entities\": [\"entity1\", \"entity2\"],\n",
    "        \"relationships\": \"OWNER\"\n",
    "        \"reason\": \"the two entities are related\"\n",
    "    }\n",
    "    \"desc2\":{\n",
    "        \"entities\": [\"entity3\", \"entity4\"],\n",
    "        \"relationships\": \"OWNER\"\n",
    "        \"reason\": \"the two entities are related\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from st_link_analysis import st_link_analysis, NodeStyle, EdgeStyle\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "# Sample Data\n",
    "elements = {\n",
    "    \"nodes\": [\n",
    "        {\"data\": {\"id\": 1, \"label\": \"PERSON\", \"name\": \"Streamlit\"}},\n",
    "        {\"data\": {\"id\": 2, \"label\": \"PERSON\", \"name\": \"Hello\"}},\n",
    "        {\"data\": {\"id\": 3, \"label\": \"PERSON\", \"name\": \"World\"}},\n",
    "        {\"data\": {\"id\": 4, \"label\": \"POST\", \"content\": \"x\"}},\n",
    "        {\"data\": {\"id\": 5, \"label\": \"POST\", \"content\": \"y\"}},\n",
    "    ],\n",
    "    \"edges\": [\n",
    "        {\"data\": {\"id\": 6, \"label\": \"FOLLOWS\", \"source\": 1, \"target\": 2}},\n",
    "        {\"data\": {\"id\": 7, \"label\": \"FOLLOWS\", \"source\": 2, \"target\": 3}},\n",
    "        {\"data\": {\"id\": 8, \"label\": \"POSTED\", \"source\": 3, \"target\": 4}},\n",
    "        {\"data\": {\"id\": 9, \"label\": \"POSTED\", \"source\": 1, \"target\": 5}},\n",
    "        {\"data\": {\"id\": 10, \"label\": \"QUOTES\", \"source\": 5, \"target\": 4}},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Style node & edge groups\n",
    "node_styles = [\n",
    "    NodeStyle(\"PERSON\", \"#FF7F3E\", \"name\", \"person\"),\n",
    "    NodeStyle(\"POST\", \"#2A629A\", \"content\", \"description\"),\n",
    "]\n",
    "\n",
    "edge_styles = [\n",
    "    EdgeStyle(\"OWNER\", caption='label', directed=True),\n",
    "    EdgeStyle(\"SUPPLIER\", caption='label', directed=True),\n",
    "    EdgeStyle(\"ASSOCIATE\", caption='label', directed=True),\n",
    "    EdgeStyle(\"SHAREHOLDER\", caption='label', directed=True),\n",
    "    EdgeStyle(\"COMPETITOR\", caption='label', directed=True),\n",
    "    EdgeStyle(\"EMPLOYER\", caption='label', directed=True),\n",
    "    EdgeStyle(\"ADVISOR\", caption='label', directed=True),\n",
    "    EdgeStyle(\"CONTRIBUTOR\", caption='label', directed=True),\n",
    "    EdgeStyle(\"REPRESENTATIVE\", caption='label', directed=True),\n",
    "    EdgeStyle(\"BENEFICIARY\", caption='label', directed=True),\n",
    "    EdgeStyle(\"LICENSOR\", caption='label', directed=True),\n",
    "    EdgeStyle(\"VICTIM\", caption='label', directed=True),\n",
    "    EdgeStyle(\"WITNESS\", caption='label', directed=True),\n",
    "    EdgeStyle(\"OTHER TYPES\", caption='label', directed=True)\n",
    "]\n",
    "\n",
    "# Render the component\n",
    "st.markdown(\"### st-link-analysis: Example\")\n",
    "st_link_analysis(elements, \"cose\", node_styles, edge_styles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"nodes\": [\n",
      "        {\n",
      "            \"data\": {\n",
      "                \"id\": 1,\n",
      "                \"label\": \"ENTITY\",\n",
      "                \"name\": \"entity1\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"data\": {\n",
      "                \"id\": 2,\n",
      "                \"label\": \"ENTITY\",\n",
      "                \"name\": \"entity2\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"data\": {\n",
      "                \"id\": 3,\n",
      "                \"label\": \"ENTITY\",\n",
      "                \"name\": \"entity3\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"data\": {\n",
      "                \"id\": 4,\n",
      "                \"label\": \"ENTITY\",\n",
      "                \"name\": \"entity4\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"edges\": [\n",
      "        {\n",
      "            \"data\": {\n",
      "                \"id\": 1,\n",
      "                \"label\": \"OWNER\",\n",
      "                \"source\": 1,\n",
      "                \"target\": 2,\n",
      "                \"reason\": \"the two entities are related\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"data\": {\n",
      "                \"id\": 2,\n",
      "                \"label\": \"OWNER\",\n",
      "                \"source\": 3,\n",
      "                \"target\": 4,\n",
      "                \"reason\": \"the two entities are related\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Input dictionary\n",
    "test1 = {\n",
    "    \"desc1\": {\n",
    "        \"entities\": [\"entity1\", \"entity2\"],\n",
    "        \"relationships\": \"OWNER\",\n",
    "        \"reason\": \"the two entities are related\"\n",
    "    },\n",
    "    \"desc2\": {\n",
    "        \"entities\": [\"entity3\", \"entity4\"],\n",
    "        \"relationships\": \"OWNER\",\n",
    "        \"reason\": \"the two entities are related\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Assign unique IDs to entities\n",
    "entity_to_id = {}\n",
    "current_id = 1\n",
    "\n",
    "# Extract all unique entities and assign IDs\n",
    "for desc in test1.values():\n",
    "    for entity in desc[\"entities\"]:\n",
    "        if entity not in entity_to_id:\n",
    "            entity_to_id[entity] = current_id\n",
    "            current_id += 1\n",
    "\n",
    "# Create node list\n",
    "nodes = [{\"data\": {\"id\": id, \"label\": \"ENTITY\", \"name\": name}} for name, id in entity_to_id.items()]\n",
    "\n",
    "# Create edge list\n",
    "edges = [\n",
    "    {\n",
    "        \"data\": {\n",
    "            \"id\": idx + 1,\n",
    "            \"label\": desc[\"relationships\"],\n",
    "            \"source\": entity_to_id[desc[\"entities\"][0]],\n",
    "            \"target\": entity_to_id[desc[\"entities\"][1]],\n",
    "            \"reason\": desc[\"reason\"]\n",
    "        }\n",
    "    }\n",
    "    for idx, desc in enumerate(test1.values())\n",
    "]\n",
    "\n",
    "# Final structure\n",
    "elements = {\n",
    "    \"nodes\": nodes,\n",
    "    \"edges\": edges\n",
    "}\n",
    "\n",
    "# Print output\n",
    "import json\n",
    "print(json.dumps(elements, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed PDF saved as: output.pdf\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def compress_pdf(input_pdf, output_pdf):\n",
    "    gs_command = [\n",
    "        \"gs\", \"-sDEVICE=pdfwrite\", \"-dCompatibilityLevel=1.4\",\n",
    "        \"-dPDFSETTINGS=/screen\",  # Change to /ebook for better quality\n",
    "        \"-dNOPAUSE\", \"-dQUIET\", \"-dBATCH\",\n",
    "        f\"-sOutputFile={output_pdf}\", input_pdf\n",
    "    ]\n",
    "    subprocess.run(gs_command)\n",
    "    print(f\"Compressed PDF saved as: {output_pdf}\")\n",
    "\n",
    "compress_pdf(\"doc.pdf\", \"output.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1650711533.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"reason\": \"the two entities are related\"\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary\n",
    "test1 = {\n",
    "    \"desc1\": {\n",
    "        \"entities\": [\"entity1\", \"entity2\"],\n",
    "        \"relationships\": \"OWNER\",\n",
    "        \"reason\": \"the two entities are related\"\n",
    "    },\n",
    "    \"desc2\": {\n",
    "        \"entities\": [\"entity3\", \"entity4\"],\n",
    "        \"relationships\": \"OWNER\",\n",
    "        \"reason\": \"the two entities are related\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "data = []\n",
    "for key, value in test1.items():\n",
    "    entity, entity_connected = value[\"entities\"]  # Extract the two entities\n",
    "    relationships = value[\"relationships\"]  # Extract relationship type\n",
    "    data.append([entity, entity_connected, relationships])  # Append as a row\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"entity\", \"entity connected\", \"relationships\"])\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (Replace with your actual DataFrame)\n",
    "test1 = {\n",
    "    f\"desc{i}\": {\n",
    "        \"entities\": [f\"entity{i}\", f\"entity{i+1}\"],\n",
    "        \"relationships\": \"OWNER\",\n",
    "        \"reason\": \"the two entities are related\"\n",
    "    }\n",
    "    for i in range(1, 51)  # Creating 50 rows for testing\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "data = []\n",
    "for key, value in test1.items():\n",
    "    entity, entity_connected = value[\"entities\"]\n",
    "    relationships = value[\"relationships\"]\n",
    "    data.append([entity, entity_connected, relationships])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Entity\", \"Entity Connected\", \"Relationships\"])\n",
    "\n",
    "# **1️⃣ Show a preview (first 10 rows)**\n",
    "st.subheader(\"🔹 Data Preview (First 10 Rows)\")\n",
    "st.dataframe(df.head(10), height=300, width=600)\n",
    "\n",
    "# **2️⃣ Full table inside an expander**\n",
    "with st.expander(\"🔍 View Full Data (50 Rows)\"):\n",
    "    st.data_editor(df, height=500, use_container_width=True)\n",
    "\n",
    "# **3️⃣ Allow user to select number of rows displayed**\n",
    "num_rows = st.slider(\"Select number of rows to display:\", min_value=5, max_value=50, value=10, step=5)\n",
    "st.dataframe(df.head(num_rows), height=400, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my problem is pip dependency resolver does not currently take into account all the packages that are installed. this behaviour is the source of the following depedency conflits. azure cli 2.70 requires smver==2.13.0 but you have semver 3.04 which is incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install semver==2.13.0 --force-reinstall\n",
    "pip check\n",
    "\n",
    "pip uninstall azure-cli\n",
    "pip install azure-cli\n",
    "\n",
    "python -m venv myenv\n",
    "source myenv/bin/activate  # On macOS/Linux\n",
    "myenv\\Scripts\\activate  # On Windows\n",
    "pip install azure-cli\n",
    "\n",
    "\n",
    "pip dependency resolver does not currently take into account all the packages that are installed. flair 0.13.1 requires semver<4.0.0>=3.0.0 but you have semver 2.13.0 which is incomptabile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: nums = [2,7,11,15], target = 9\n",
    "Output: [0,1]\n",
    "Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 11, 15]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [2, 7,11,15]\n",
    "target=9\n",
    "seen=[] \n",
    "for i,v in nums:\n",
    "    if v in seen:\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the main values of those things \n",
    "    # try to f120*ind the best places available in this marke50\n",
    "\n",
    "100*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reformulate properly: as of today there are 5 hits to migrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformulate and enhance: I have a beeter vision of your question, No indeed in that case it won't generate any hits because we are not considering connected us domicile heirs. In that case, we should update the scenario to take into consideration connected relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.39819595"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chf_price=0.87557\n",
    "quantity=1\n",
    "stock_price=184.3350\n",
    "quantity*stock_price*chf_price\n",
    "\n",
    "what are the duffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.590909064049588"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAPL APPLE INC 265598 US0378331005 AAPL NASDAQ 1 COMMON\n",
    "AMZN AMAZON.COM INC 3691937 US0231351067 AMZN NASDAQ 1 COMMON\n",
    "CMG CHIPOTLE MEXICAN GRILL\n",
    "INC 37655664 US1696561059 CMG NYSE 1 COMMON\n",
    "GOOGL ALPHABET INC-CL A 208813719 US02079K3059 GOOGL NASDAQ 1 COMMON\n",
    "IYW ISHARES USTECHNOLOGY\n",
    "ETF 10158652 US4642877215 IYW ARCA 1 ETF\n",
    "MA MASTERCARD INC - A 38685693 US57636Q1040 MA NYSE 1 COMMON\n",
    "MSFT MICROSOFT CORP 272093 US5949181045 MSFT NASDAQ 1 COMMON\n",
    "NVDA NVIDIA CORP 4815747 US67066G1040 NVDA NASDAQ 1 COMMON\n",
    "SOXX ISHARES SEMICONDUCTOR\n",
    "ETF 12658194 US4642875235 NASDAQ 1 ETF\n",
    "TSLA TESLA INC 76792991 US88160R1014 NASDAQ 1 COMMON\n",
    "VOO VANGUARD S&P 500 ETF 136155102 US9229083632 VOO ARCA 1 ETF\n",
    "VT VANGUARD TOT WORLD STK\n",
    "ETF 52197301 US9220427424 VT ARCA 1 ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "03.09.2024\tUSA\t1 USD\t0.86072\n",
    "30.09.2024\tUSA\t1 USD\t0.84971\n",
    "23.10.2024\tUSA\t1 USD\t0.87557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623.14"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "303.91+157.84+161.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "795/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I have a meet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438.34000000000003"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "795-(198.82+157.84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[1, 2, 3]\n",
      "2\n",
      "6\n",
      "24\n",
      "[0, 2, 3]\n",
      "1\n",
      "3\n",
      "12\n",
      "[0, 1, 3]\n",
      "1\n",
      "2\n",
      "8\n",
      "[0, 1, 2]\n",
      "1\n",
      "2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "nums = [1,2,3,4]\n",
    "product_list = []\n",
    "len_nums = len(nums)\n",
    "summ = []\n",
    "print(len_nums)\n",
    "for i,v in enumerate(nums):\n",
    "    product = 1\n",
    "    i_valid = [num for num in range(len(nums)) if num != i]\n",
    "    print(i_valid)\n",
    "    for num in i_valid:\n",
    "        product *= nums[num]\n",
    "        print(product)\n",
    "    summ.append(product)\n",
    "return summ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 12, 8, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here’s the updated note with a concise question added about data storage:\n",
    "\n",
    "⸻\n",
    "\n",
    "Designing a scalable data architecture on Azure and Databricks starts with proper layering. Ingest data from sources like CRMs, apps, APIs, and files using Azure Data Factory for batch or Event Hubs for streaming. Use Databricks Autoloader for file-based incremental loads.\n",
    "\n",
    "Store everything in Azure Data Lake Storage Gen2 using a medallion architecture—Bronze for raw data, Silver for cleaned/enriched data, and Gold for business-ready aggregates.\n",
    "\n",
    "Process data with Azure Databricks using Spark notebooks. Apply business logic, schema enforcement, and use Delta Lake for versioning and updates.\n",
    "\n",
    "Model and analyze data using Databricks SQL or Power BI. Use the Lakehouse approach to unify analytics and AI.\n",
    "\n",
    "For ML, manage experiments and models with MLflow; optionally use Azure ML. Use Silver/Gold data layers for training features.\n",
    "\n",
    "Ensure governance with Microsoft Purview for data lineage and cataloging. Secure access with RBAC, POSIX ACLs, and Unity Catalog.\n",
    "\n",
    "Orchestrate workflows with Data Factory or Databricks Workflows. Integrate Git for CI/CD and automate pipelines.\n",
    "\n",
    "Key principles: ensure data quality with tools like Great Expectations, build modular pipelines, document lineage, and monitor using Azure Monitor and job alerts.\n",
    "\n",
    "⸻\n",
    "\n",
    "When meeting with teams using other architectures, ask:\n",
    "\t•\tWhat are your main data goals (e.g., reporting, ML)?\n",
    "\t•\tWhat platforms and tools do you use (cloud, storage, ETL)?\n",
    "\t•\tHow is your data stored—data lake, warehouse, or a hybrid? What formats do you use (e.g., Parquet, Delta, CSV)?\n",
    "\t•\tIs your setup batch, real-time, or both?\n",
    "\t•\tHow do you manage ingestion, modeling, and transformations?\n",
    "\t•\tWhat BI tools are used? Is data self-service?\n",
    "\t•\tHow do you handle ML workflows and deployment?\n",
    "\t•\tHow is data access, governance, and compliance managed?\n",
    "\t•\tCan our systems align or integrate? Are there blockers we can address together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [2,7,11,15]\n",
    "target = 9\n",
    "nums[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ok/ok/ok/ok\n",
      ".txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "name, ext = os.path.sploneitext(\"/ok/ok/ok/ok.txt\")\n",
    "print(name)\n",
    "print(ext)xsswz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3319 87.330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_ounce = 31.1035\n",
    "gram = 605\n",
    "chf_price = 87.280\n",
    "worth_ounce = gram / try_ounce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52804.4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram * chf_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy = 3504 + 4972 + 9993 + 9815 + 15133 + 2460 + 1324 + 440 + 87 + 174 + 86 + 25615 + 177 + 175 + 164 +442\n",
    "sold = 12805 + 8534 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy - sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berserk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
